{"StableSR_doc/README":{"slug":"StableSR_doc/README","filePath":"StableSR_doc/README.md","title":"README","links":[],"tags":[],"content":"StableSR_doc"},"StableSR_doc/ldm/models/diffusion/ddpm/DiffusionWrapper(pl.LightningModule)":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/DiffusionWrapper(pl.LightningModule)","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/DiffusionWrapper(pl.LightningModule).md","title":"DiffusionWrapper(pl.LightningModule)","links":[],"tags":[],"content":"class DiffusionWrapper(pl.LightningModule):\n    def __init__(self, diff_model_config, conditioning_key):\n        super().__init__()\n        self.diffusion_model = instantiate_from_config(diff_model_config) # [[instantiate_from_config]]\n        self.conditioning_key = conditioning_key\n        assert self.conditioning_key in [None, &#039;concat&#039;, &#039;crossattn&#039;, &#039;hybrid&#039;, &#039;adm&#039;]\n \n    def forward(self, x, t, c_concat: list = None, c_crossattn: list = None, struct_cond=None, seg_cond=None):\n        if self.conditioning_key is None: # [[#none无条件扩散unconditional|None：无条件扩散（Unconditional）]]\n            out = self.diffusion_model(x, t)\n        elif self.conditioning_key == &#039;concat&#039;: # [[#concat通道拼接channel-concat|concat：通道拼接（Channel Concat）]]\n            xc = torch.cat([x] + c_concat, dim=1) \n            out = self.diffusion_model(xc, t)\n        elif self.conditioning_key == &#039;crossattn&#039;: # [[#crossattn交叉注意力cross-attention|crossattn：交叉注意力（Cross-Attention）]]\n            cc = torch.cat(c_crossattn, 1)\n            if seg_cond is None:\n                out = self.diffusion_model(x, t, context=cc, struct_cond=struct_cond)\n            else:\n                out = self.diffusion_model(x, t, context=cc, struct_cond=struct_cond, seg_cond=seg_cond)\n        elif self.conditioning_key == &#039;hybrid&#039;: # [[#hybrid通道拼接--交叉注意力concat--cross-attn|hybrid：通道拼接 + 交叉注意力（Concat + Cross-Attn）]]\n            xc = torch.cat([x] + c_concat, dim=1)\n            cc = torch.cat(c_crossattn, 1)\n            out = self.diffusion_model(xc, t, context=cc)\n        elif self.conditioning_key == &#039;adm&#039;: # [[#adm标签注入classifier-free-guidance|adm：标签注入（Classifier-free Guidance）]]\n            cc = c_crossattn[0]\n            out = self.diffusion_model(x, t, y=cc)\n        else:\n            raise NotImplementedError()\n \n        return out\n \nDiffusionWrapper.forward() 中 conditioning_key 的五种模式解析\n在 StableSR 或 Latent Diffusion 中，conditioning_key 决定了条件信息如何注入到扩散模型（通常是 UNet，如 UNetModelDualConv2d）中。该参数影响的是 DiffusionWrapper 在 forward 阶段如何组织输入，并通过哪些通路将条件信息传递到扩散网络。\n\nNone：无条件扩散（Unconditional）\nout = self.diffusion_model(x, t)\n\n不使用任何条件信息；\n输入仅为带噪图像 x 和时间步 t；\n通常用于纯图像建模、初期训练或 unconditional generation。\n\n\nconcat：通道拼接（Channel Concat）\nxc = torch.cat([x] + c_concat, dim=1)\nout = self.diffusion_model(xc, t)\n\n条件以图像形式提供（如低清图、边缘图、小波子带），直接拼接到 x 上；\n通道维度变为 C + C_cond；\n是 SR3 使用的典型条件注入方法；\n是传统sr3的方法,简单高效，但灵活性较差。\n\n\ncrossattn：交叉注意力（Cross-Attention）\ncc = torch.cat(c_crossattn, 1)\nout = self.diffusion_model(x, t, context=cc, struct_cond=..., seg_cond=...)\n\n条件信息（如结构图、小波图、文本）通过 cond_stage_model 编码为 latent 向量；\n这些 latent 向量作为 context，传入 UNet 内部的 CrossAttention 模块；\n可选地支持结构条件 struct_cond 和语义条件 seg_cond；\n是现代条件扩散（如 Stable Diffusion）最常用策略，灵活且表达力强。\n\n\nhybrid：通道拼接 + 交叉注意力（Concat + Cross-Attn）\nxc = torch.cat([x] + c_concat, dim=1)\ncc = torch.cat(c_crossattn, 1)\nout = self.diffusion_model(xc, t, context=cc)\n\n同时使用 concat 和 cross-attention 两种通路注入条件；\n通道拼接传递低级信息（细节、边缘）；\ncross-attn 传递高级语义（结构 latent、小波 latent）；\n在 StableSR 中，通常两路条件信息都来源于同一个结构图，只是经过不同路径处理；\n平衡引导性与灵活性，是推荐模式之一。\n\n\nadm：标签注入（Classifier-free Guidance）\ncc = c_crossattn[0]\nout = self.diffusion_model(x, t, y=cc)\n\n用于类别或 token 作为标签条件（如 ImageNet class label）；\ny=cc 表示将条件作为类标签注入；\n需要扩散模型支持 y 输入（如通过 ConditionalBatchNorm、embedding 等）；\n常见于 ADM/DDPMv2 等分类条件生成场景。\n\n\n总结\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconditioning_key条件注入方式典型用途None无条件纯图像建模&#039;concat&#039;通道拼接SR3、结构图引导&#039;crossattn&#039;交叉注意力Stable Diffusion、结构/文本引导&#039;hybrid&#039;拼接 + 注意力StableSR、小波 + 结构联合引导&#039;adm&#039;标签输入分类条件生成（如 class-conditional DDPM）\n实际应用中，可根据条件类型和任务目标灵活选择或组合这些模式。"},"StableSR_doc/ldm/models/diffusion/ddpm/Module_info":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/Module_info","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/Module_info.md","title":"Module_info","links":["StableSR_doc/ldm/models/diffusion/ddpm/torch2img","StableSR_doc/ldm/models/diffusion/ddpm/cal_pca_components","StableSR_doc/ldm/models/diffusion/ddpm/visualize_fea","StableSR_doc/ldm/models/diffusion/ddpm/calc_mean_std","StableSR_doc/ldm/models/diffusion/ddpm/adaptive_instance_normalization","StableSR_doc/ldm/models/diffusion/ddpm/space_timesteps","StableSR_doc/ldm/models/diffusion/ddpm/disabled_train","StableSR_doc/ldm/models/diffusion/ddpm/uniform_on_device","StableSR_doc/ldm/models/diffusion/ddpm/class-DDPM/DDPM(pl.LightningModule)","LatentDiffusion(DDPM)","LatentDiffusionSRTextWT(DDPM)","LatentDiffusionSRTextWTFFHQ(LatentDiffusionSRTextWT)","StableSR_doc/ldm/models/diffusion/ddpm/DiffusionWrapper(pl.LightningModule)","Layout2ImgDiffusion(LatentDiffusion)"],"tags":[],"content":"\n\n                  \n                  UML 图解：ddpm.py文件结构 \n                  \n                \n\n\n\n‘ldm.models.diffusion.ddpm’ 中各个类的关系。\n\n\n\nFunctions\ntorch2img\ncal_pca_components\nvisualize_fea\ncalc_mean_std\nadaptive_instance_normalization\nspace_timesteps\ndisabled_train\nuniform_on_device\nClasses\nDDPM(pl.LightningModule)\nLatentDiffusion(DDPM)\nLatentDiffusionSRTextWT(DDPM)\nLatentDiffusionSRTextWTFFHQ(LatentDiffusionSRTextWT)\nDiffusionWrapper(pl.LightningModule)\nLayout2ImgDiffusion(LatentDiffusion)"},"StableSR_doc/ldm/models/diffusion/ddpm/adaptive_instance_normalization":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/adaptive_instance_normalization","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/adaptive_instance_normalization.md","title":"adaptive_instance_normalization","links":[],"tags":[],"content":""},"StableSR_doc/ldm/models/diffusion/ddpm/cal_pca_components":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/cal_pca_components","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/cal_pca_components.md","title":"cal_pca_components","links":[],"tags":[],"content":""},"StableSR_doc/ldm/models/diffusion/ddpm/calc_mean_std":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/calc_mean_std","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/calc_mean_std.md","title":"calc_mean_std","links":[],"tags":[],"content":""},"StableSR_doc/ldm/models/diffusion/ddpm/class-DDPM/DDPM(pl.LightningModule)":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/class-DDPM/DDPM(pl.LightningModule)","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/class DDPM/DDPM(pl.LightningModule).md","title":"DDPM(pl.LightningModule)","links":["register_schedule","DDPM","_get_rows_from_list","configure_optimizers","forward","get_input","get_loss","get_v","StableSR_doc/ldm/models/diffusion/ddpm/class-DDPM/init_from_ckpt","on_train_batch_end","p_losses","p_mean_variance","predict_start_from_noise","predict_start_from_z_and_v","q_mean_variance","q_posterior","q_sample","q_sample_respace","shared_step","training_step"],"tags":[],"content":"Class: DDPM\nInheritance Tree (MRO):\n\nDDPM\nLightningModule\nABC\nDeviceDtypeModuleMixin\nHyperparametersMixin\nGradInformation\nModelIO\nModelHooks\nDataHooks\nCheckpointHooks\nModule\nobject\n\nInstance Attributes (from self.xxx assignments):\n\nparameterization (defined in: [[init]])\ncond_stage_model (defined in: [[init]])\nclip_denoised (defined in: [[init]])\nlog_every_t (defined in: [[init]])\nfirst_stage_key (defined in: [[init]])\nimage_size (defined in: [[init]])\nchannels (defined in: [[init]])\nuse_positional_encodings (defined in: [[init]])\nmodel (defined in: [[init]])\nuse_ema (defined in: [[init]])\nuse_scheduler (defined in: [[init]])\nv_posterior (defined in: [[init]])\noriginal_elbo_weight (defined in: [[init]])\nl_simple_weight (defined in: [[init]])\nloss_type (defined in: [[init]])\nlearn_logvar (defined in: [[init]])\nlogvar (defined in: [[init, init]])\nmodel_ema (defined in: [[init]])\nscheduler_config (defined in: [[init]])\nmonitor (defined in: [[init]])\nnum_timesteps (defined in: register_schedule)\nlinear_start (defined in: register_schedule)\nlinear_end (defined in: register_schedule)\n\nProject-defined Methods:\n\n[[init]]  ←  DDPM\n_get_rows_from_list  ←  DDPM\nconfigure_optimizers  ←  DDPM\nforward  ←  DDPM\nget_input  ←  DDPM\nget_loss  ←  DDPM\nget_v  ←  DDPM\ninit_from_ckpt  ←  DDPM\non_train_batch_end  ←  DDPM\np_losses  ←  DDPM\np_mean_variance  ←  DDPM\npredict_start_from_noise  ←  DDPM\npredict_start_from_z_and_v  ←  DDPM\nq_mean_variance  ←  DDPM\nq_posterior  ←  DDPM\nq_sample  ←  DDPM\nq_sample_respace  ←  DDPM\nregister_schedule  ←  DDPM\nshared_step  ←  DDPM\ntraining_step  ←  DDPM\n\nProject-defined Attributes:"},"StableSR_doc/ldm/models/diffusion/ddpm/class-DDPM/__init__":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/class-DDPM/__init__","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/class DDPM/__init__.md","title":"__init__","links":["StableSR_doc/pytorch-lightning"],"tags":[],"content":"init 函数介绍\nclass DDPM(pl.LightningModule): # [[pytorch lightning#pllightningmoudule|pl.LightningMoudule]]\n    # classic DDPM with Gaussian diffusion, in image space\n    def __init__(self,\n                 unet_config,\n                 timesteps=1000,\n                 beta_schedule=&quot;linear&quot;,\n                 loss_type=&quot;l2&quot;,\n                 ckpt_path=None,\n                 ignore_keys=[],\n                 load_only_unet=False,\n                 monitor=&quot;val/loss&quot;,\n                 use_ema=True,\n                 first_stage_key=&quot;image&quot;,\n                 image_size=256,\n                 channels=3,\n                 log_every_t=100,\n                 clip_denoised=True,\n                 linear_start=1e-4,\n                 linear_end=2e-2,\n                 cosine_s=8e-3,\n                 given_betas=None,\n                 original_elbo_weight=0.,\n                 v_posterior=0.,  # weight for choosing posterior variance as sigma = (1-v) * beta_tilde + v * beta\n                 l_simple_weight=1.,\n                 conditioning_key=None,\n                 parameterization=&quot;eps&quot;,  # all assuming fixed variance schedules\n                 scheduler_config=None,\n                 use_positional_encodings=False,\n                 learn_logvar=False,\n                 logvar_init=0.,\n                 ):\n        super().__init__() \n        # [[#parameterization|parameterization]]\n        assert parameterization in [&quot;eps&quot;, &quot;x0&quot;, &quot;v&quot;], &#039;currently only supporting &quot;eps&quot; and &quot;x0&quot; and &quot;v&quot;&#039;\n        self.parameterization = parameterization\n        print(f&quot;{self.__class__.__name__}: Running in {self.parameterization}-prediction mode&quot;)\n        self.cond_stage_model = None\n        self.clip_denoised = clip_denoised\n        self.log_every_t = log_every_t\n        self.first_stage_key = first_stage_key\n        self.image_size = image_size  # try conv?\n        self.channels = channels\n        self.use_positional_encodings = use_positional_encodings\n        self.model = DiffusionWrapper(unet_config, conditioning_key) # [[DiffusionWrapper(pl.LightningModule)]]\n        count_params(self.model, verbose=True) # [[#line-45-to-57--ddpm__init__-中模型管理调度器与损失权重部分解析|line 45 to 57 🔧 `DDPM.__init__` 中模型管理、调度器与损失权重部分解析]]\n        self.use_ema = use_ema\n        if self.use_ema:\n            self.model_ema = LitEma(self.model)\n            print(f&quot;Keeping EMAs of {len(list(self.model_ema.buffers()))}.&quot;)\n \n        self.use_scheduler = scheduler_config is not None\n        if self.use_scheduler:\n            self.scheduler_config = scheduler_config\n \n        self.v_posterior = v_posterior\n        self.original_elbo_weight = original_elbo_weight\n        self.l_simple_weight = l_simple_weight\n \n        if monitor is not None: # [[#-pytorch-lightning-中的-monitor-参数简析|📈 PyTorch Lightning 中的 `monitor` 参数简析]]\n            self.monitor = monitor \n        if ckpt_path is not None: # 加载预训练模型\n            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys, only_model=load_only_unet) # [[init_from_ckpt]]\n\t\t# [[#line-64-to-end|line 64 to end]]\n\t\t# [[#1-注册调度表beta-schedule|1. 注册调度表（beta schedule）]]\n        self.register_schedule(given_betas=given_betas, beta_schedule=beta_schedule, timesteps=timesteps,\n                               linear_start=linear_start, linear_end=linear_end, cosine_s=cosine_s)\n\t\t# [[#2-设置损失函数类型|2. 设置损失函数类型]]\n        self.loss_type = loss_type\n\t\t# [[#logvar-在-ddpm-扩散模型中的作用与实现|logvar 在 DDPM 扩散模型中的作用与实现]]\n        self.learn_logvar = learn_logvar\n        self.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))\n        if self.learn_logvar:\n            self.logvar = nn.Parameter(self.logvar, requires_grad=True)\n \nparameterization\n\nparameterization: 扩散模型的预测目标，有三种：\n\n&quot;eps&quot;: 噪声预测（最常用）\n&quot;x0&quot;: 预测原始图像\n&quot;v&quot;: v-pred 方案，平衡两个极端\n\n\n断言限制只支持上述三种模式\n\nline 45 to 57 🔧 DDPM.__init__ 中模型管理、调度器与损失权重部分解析\n这部分代码负责扩散模型训练过程中的几个重要功能组件的配置，包括参数统计、EMA 平滑、调度器设置，以及损失项的加权策略。\n\n🔢 模型参数统计与打印\ncount_params(self.model, verbose=True)\n\n调用 count_params 打印模型参数数量；\nself.model 是之前创建的 DiffusionWrapper（包含 UNet 和条件控制）；\nverbose=True 表示输出详细层级参数统计，有助于模型调试与规模评估。\n\n\n🧮 EMA 模型配置（Exponential Moving Average）\nself.use_ema = use_ema\nif self.use_ema:\n    self.model_ema = LitEma(self.model)\n    print(f&quot;Keeping EMAs of {len(list(self.model_ema.buffers()))}.&quot;)\n\nuse_ema: 控制是否启用 EMA；\n如果启用，将创建 model_ema，用于在训练中对模型参数进行滑动平均；\nEMA 可在推理时提供更稳定的结果（尤其训练后期）；\nLitEma 是一个内部实现的 EMA 工具类（模仿 PyTorch EMA 实现）；\nbuffers() 提供了所有被 EMA 追踪的张量（通常是模型权重）。\n\nPS. EMA简介\n指数移动平均（Exponential Moving Average）也叫权重移动平均（Weighted Moving Average），是一种给予近期数据更高权重的平均方法。\n📈 训练调度器配置（如学习率调度）\nself.use_scheduler = scheduler_config is not None\nif self.use_scheduler:\n    self.scheduler_config = scheduler_config\n\n如果提供了 scheduler_config，则将其保存；\n后续在 configure_optimizers() 方法中会用到该配置；\n可用于定义如余弦退火（cosine annealing）、线性 warmup 等学习率调度策略。\n\n\n⚖️ 损失项权重设置\nself.v_posterior = v_posterior\nself.original_elbo_weight = original_elbo_weight\nself.l_simple_weight = l_simple_weight\n这三项参数控制扩散损失的组成：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n参数名含义v_posterior后验方差的加权控制项。用于设置预测方差的策略。具体计算形式为：σ² = (1 - v) * beta_tilde + v * beta，其中 v 就是这个参数。original_elbo_weight是否加入原始论文中的 ELBO loss 项（常为 0，代表不启用）l_simple_weightL2 或 L1 损失的主权重，用于稳定训练\n\n这部分最终将作用于 get_loss() 或 p_losses() 中的损失函数组合；\n对于不同任务（如图像重建 vs 生成），可以通过调整这些参数来平衡生成质量和保真度。\n\n\n✅ 小结\n这一部分为训练过程提供了必要的配置管理：\n\n模型参数统计有助于可视化规模；\nEMA 管理可提升训练稳定性；\n调度器设置为优化器行为提供了灵活性；\n损失项加权控制生成模型优化目标的侧重点。\n\n📈 PyTorch Lightning 中的 monitor 参数简析\nmonitor 是 PyTorch Lightning 中回调（Callback）机制的一部分，用于指定训练过程中要监控的指标名称，供如 ModelCheckpoint、EarlyStopping 等回调依据该指标执行相应逻辑（如保存模型、提前停止等）。\n详见pytorch_Lightning Callback 机制\n\n✅ 关键用途\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n组件用途ModelCheckpoint保存性能最好的模型EarlyStopping在验证指标停止提升时中止训练\n\n🧩 monitor 的工作流程\n\n\n模型内部记录指标：\nself.log(&quot;val/loss&quot;, val_loss, prog_bar=True)\n\n\n指定 monitor 的回调监听这个指标：\nModelCheckpoint(monitor=&quot;val/loss&quot;, mode=&quot;min&quot;)\n\n\nLightning 自动比较并触发保存 / 停止逻辑。\n\n\n\n⚙️ monitor 示例配置\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n \ncheckpoint = ModelCheckpoint(\n    monitor=&quot;val/psnr&quot;,   # 监听 PSNR 指标\n    mode=&quot;max&quot;,           # 指标越大越好\n    save_top_k=1,\n    filename=&quot;best-psnr&quot;\n)\nfrom pytorch_lightning.callbacks import EarlyStopping\n \nearly_stop = EarlyStopping(\n    monitor=&quot;val/loss&quot;,\n    mode=&quot;min&quot;,\n    patience=5\n)\n\n🧠 说明\n\nmonitor 是一个字符串，必须和 .log(...) 中记录的名字一致；\n不会自动创建指标值，只是引用已有指标；\n和 mode 一起决定何时触发操作（min → 越小越好，max → 越大越好）；\n在模型类中可用 self.monitor 传递给 callback 实现动态配置。\n\n\n✅ 总结\n\nmonitor 是 回调系统监听的指标名称；\n配合 .log(...) 使用；\n决定是否保存模型 / 提前停止；\n本身不计算指标，仅作为引用字段使用。\n\nline 64 to end\n这一部分主要完成了噪声调度与损失配置,具体来说,\n本部分代码完成了扩散过程中的beta调度表构建(即噪声调度)、损失函数类型设定和对数方差的初始化，是 DDPM 建模核心参数的关键配置步骤。\n\n1. 注册调度表（beta schedule）\nself.register_schedule(\n    given_betas=given_betas,\n    beta_schedule=beta_schedule,\n    timesteps=timesteps,\n    linear_start=linear_start,\n    linear_end=linear_end,\n    cosine_s=cosine_s\n)\n\n该函数根据 beta_schedule 的类型（如 “linear” 或 “cosine”）构建扩散过程中的时间步噪声系数表；\n通常会生成如下参数：\n\nbetas：每一步的噪声幅度；\nalphas, alphas_cumprod, sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod 等；\n\n\n这些系数会在后续 q_sample, q_posterior, predict_start_from_noise 等函数中使用；\n参数说明：\n\nlinear_start, linear_end: 用于线性 beta 调度起止值；\ncosine_s: 用于调整余弦调度的形状；\ngiven_betas: 若提供，优先使用用户自定义 beta 表。\n\n\n\n\n2. 设置损失函数类型\nself.loss_type = loss_type\n\n控制训练时使用哪种损失：\n\n&quot;l2&quot;（默认）：预测的噪声与真实噪声之间的均方误差；\n&quot;l1&quot;：预测残差的绝对值损失；\n也可能支持其他自定义损失类型，如 perceptual loss、hybrid loss 等；\n\n\n实际使用在 get_loss() 或 p_losses() 中处理。\n\n\n3. 初始化对数方差（log-variance）\nself.learn_logvar = learn_logvar\nself.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))\nif self.learn_logvar:\n    self.logvar = nn.Parameter(self.logvar, requires_grad=True)\n\nlogvar 是一个长度为 num_timesteps 的张量，表示每一扩散时间步的对数方差；\n如果启用 learn_logvar=True，则将其设为可学习参数（nn.Parameter），允许模型自动优化每一时间步的不确定性；\n如果不启用，logvar 就是一个固定的常量值张量；\n在 get_loss() 中会参与 KL 项或 likelihood 的权重调整。\n\nlogvar 在 DDPM 扩散模型中的作用与实现\nlogvar（对数方差）是扩散模型（如 DDPM）中用于控制训练损失权重和不确定性建模的一个重要变量。在 StableSR 和 DDPM 实现中，它是 diffusion 模型的一部分，而非 encoder 或 decoder 的组成部分。\n\n1. 背景：为何需要 logvar\n扩散模型训练时通常以预测噪声为目标，基本损失形式为(以L2损失为例)：\nL_t = \\left\\| \\varepsilon_{\\text{pred}} - \\varepsilon_{\\text{true}} \\right\\|^2\n为了增强灵活性、稳定性或逼近对数似然，一些变体引入了对数方差 logvar，使得损失函数变为：\nL_t = \\frac{1}{2} \\cdot \\exp(-\\text{logvar}_t) \\cdot \\left\\| \\varepsilon_{\\text{pred}} - \\varepsilon_{\\text{true}} \\right\\|^2 + \\frac{1}{2} \\cdot \\text{logvar}_t\n这相当于使用一个可变的时间步权重项，用于：\n\n控制每一时间步损失的相对重要性；\n模拟高斯似然中的分布不确定性；\n使得模型对某些时间步预测更加稳健。\n\n\n2. 初始化方式\nlogvar 通常被初始化为常数张量：\nself.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))\n\nlogvar_init: 对数方差的初始值（常为 0）；\nnum_timesteps: 扩散总步数（如 1000）；\n得到形状为 (T,) 的 logvar 张量，其中 T 是时间步数。\n\n若启用可学习方差：\nif self.learn_logvar:\n    self.logvar = nn.Parameter(self.logvar, requires_grad=True)\n则该张量在训练中会自动更新，每一时间步都有不同的可学习不确定性。\n\n3. 使用方式（训练时）\nlogvar 通常参与损失函数定义，在 get_loss() 或 p_losses() 中用作动态权重：\nloss = weighted_mse / torch.exp(self.logvar[t]) + self.logvar[t]\n或更复杂的：\nloss = loss_weight * loss_raw + offset * logvar[t]\n\n4. logvar总结\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n项目内容名称logvar（对数方差）类型Tensor / nn.Parameter维度(num_timesteps,)用途控制不同时间步的损失权重与不确定性建模初始化方法torch.full(size, fill_value)是否可训练由 learn_logvar 参数控制\nlogvar 是一个扩散过程中的权重调节器，尤其在加入 ELBO、VLB 等目标时尤为关键。\n\n总结\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n项目功能说明register_schedule()构造扩散过程中每一步的 beta 参数，用于控制加噪过程loss_type决定训练时的损失函数类型，如 L2 或 L1learn_logvar 和 logvar控制是否学习每个时间步的对数方差，以适配不同的不确定性建模策略\n这些设置构成了扩散模型训练阶段的核心数学基础。"},"StableSR_doc/ldm/models/diffusion/ddpm/class-DDPM/forward":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/class-DDPM/forward","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/class DDPM/forward.md","title":"forward","links":[],"tags":[],"content":""},"StableSR_doc/ldm/models/diffusion/ddpm/class-DDPM/init_from_ckpt":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/class-DDPM/init_from_ckpt","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/class DDPM/init_from_ckpt.md","title":"init_from_ckpt","links":[],"tags":[],"content":"    def init_from_ckpt(self, path, ignore_keys=list(), only_model=False):\n        sd = torch.load(path, map_location=&quot;cpu&quot;)\n        if &quot;state_dict&quot; in list(sd.keys()):\n            sd = sd[&quot;state_dict&quot;]\n        keys = list(sd.keys())\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(&quot;Deleting key {} from state_dict.&quot;.format(k))\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(&#039;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&#039;)\n        print(f&quot;Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys&quot;)\n        if len(missing) &gt; 0:\n            print(f&quot;Missing Keys: {missing}&quot;)\n        if len(unexpected) &gt; 0:\n            print(f&quot;Unexpected Keys: {unexpected}&quot;)\ninit_from_ckpt(path, ignore_keys=list(), only_model=False) 函数解析\n该函数用于从 checkpoint 文件中加载模型参数，可选择只加载 UNet（self.model），或加载整个 DDPM 模型本身。支持忽略部分参数键名，适用于微调、迁移学习等场景。\n\n函数签名\ndef init_from_ckpt(self, path, ignore_keys=list(), only_model=False):\n\npath: checkpoint 文件路径（通常为 .ckpt 或 .pth）\nignore_keys: 字符串列表，指定哪些 key 应从 state_dict 中排除（常用于忽略不兼容的模块）\nonly_model: 若为 True，则只加载 self.model 的参数，不加载整个 DDPM 类结构（适用于仅更新 UNet 时）\n\n\n1. 加载 checkpoint 字典\nsd = torch.load(path, map_location=&quot;cpu&quot;)\nif &quot;state_dict&quot; in list(sd.keys()):\n    sd = sd[&quot;state_dict&quot;]\n\n使用 torch.load 加载权重；\n有些 checkpoint 是通过 PyTorch Lightning 保存的，外层是个字典，内部的模型权重位于 state_dict 键下；\n这一步兼容这两种结构。\n\n\n2. 根据 ignore_keys 删除不需要的参数\nkeys = list(sd.keys())\nfor k in keys:\n    for ik in ignore_keys:\n        if k.startswith(ik):\n            print(&quot;Deleting key {} from state_dict.&quot;.format(k))\n            del sd[k]\n\n遍历所有参数名（key），如果以 ik 中任一字符串开头，则删除该 key；\n典型用途：跳过 cond_stage_model, model_ema, scheduler 等与当前任务无关的部分。\n\n\n3. 加载权重到模型中\nmissing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(sd, strict=False)\n\nstrict=False：允许 checkpoint 和当前模型结构不完全一致（否则会报错）；\nmissing：当前模型中存在而 checkpoint 中没有的 key；\nunexpected：checkpoint 中存在但当前模型没有的 key；\n根据 only_model 决定加载整个 DDPM 模型或仅加载其 self.model（通常是 UNet）。\n\n\n4. 打印加载结果\nprint(&#039;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&#039;)\nprint(f&quot;Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys&quot;)\nif len(missing) &gt; 0:\n    print(f&quot;Missing Keys: {missing}&quot;)\nif len(unexpected) &gt; 0:\n    print(f&quot;Unexpected Keys: {unexpected}&quot;)\n\n清晰地汇报恢复情况；\n若 missing/unexpected 过多，可能说明模型结构不匹配，需要调整配置或 ignore_keys。\n\n\n总结表格\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n参数作用path指定加载的 checkpoint 文件路径ignore_keys忽略掉具有特定前缀的参数键（如 model_ema）only_model是否只加载 self.model（通常为 UNet）strict=False宽松加载，不要求结构完全一致missing / unexpected分别记录缺失和多余的参数 key 名\n此函数广泛用于 StableSR/LDMS/LatentDiffusion 的预训练模型加载与微调流程中，推荐配合 YAML 配置与 callback 一起使用。"},"StableSR_doc/ldm/models/diffusion/ddpm/disabled_train":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/disabled_train","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/disabled_train.md","title":"disabled_train","links":[],"tags":[],"content":""},"StableSR_doc/ldm/models/diffusion/ddpm/space_timesteps":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/space_timesteps","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/space_timesteps.md","title":"space_timesteps","links":[],"tags":[],"content":""},"StableSR_doc/ldm/models/diffusion/ddpm/torch2img":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/torch2img","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/torch2img.md","title":"torch2img","links":[],"tags":[],"content":"def torch2img(input):\n    input_ = input[0]\n    input_ = input_.permute(1,2,0)\n    input_ = input_.data.cpu().numpy()\n    input_ = (input_ + 1.0) / 2\n    cv2.imwrite(&#039;./test.png&#039;, input_[:,:,::-1]*255.0)"},"StableSR_doc/ldm/models/diffusion/ddpm/uniform_on_device":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/uniform_on_device","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/uniform_on_device.md","title":"uniform_on_device","links":[],"tags":[],"content":""},"StableSR_doc/ldm/models/diffusion/ddpm/visualize_fea":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/visualize_fea","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/visualize_fea.md","title":"visualize_fea","links":[],"tags":[],"content":""},"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/Module_info":{"slug":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/Module_info","filePath":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/Module_info.md","title":"Module_info","links":["convert_module_to_f16","convert_module_to_f32","exists","cal_fea_cossim","count_flops_attn","AttentionPool2d","TimestepBlock","TimestepBlockDual","TimestepBlock3cond","TimestepEmbedSequential","Upsample","TransposedUpsample","Downsample","ResBlock","ResBlockDual","AttentionBlock","QKVAttentionLegacy","QKVAttention","UNetModel","StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/UNetModelDualcondV2","EncoderUNetModelWT"],"tags":[],"content":"ldm/modules/diffusionmodules/openaimodel.py\n\n\n                  \n                  UML图解：openaimodels.py \n                  \n                \n\n\n\n\n\n\nFunctions\n\nconvert_module_to_f16\nconvert_module_to_f32\nexists\ncal_fea_cossim\ncount_flops_attn\n\nClasses\n\nAttentionPool2d\nTimestepBlock\nTimestepBlockDual\nTimestepBlock3cond\nTimestepEmbedSequential\nUpsample\nTransposedUpsample\nDownsample\nResBlock\nResBlockDual\nAttentionBlock\nQKVAttentionLegacy\nQKVAttention\nUNetModel\nUNetModelDualcondV2\nEncoderUNetModelWT\n"},"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/UNetModelDualcondV2":{"slug":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/UNetModelDualcondV2","filePath":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class UNetModelDualcondV2/UNetModelDualcondV2.md","title":"UNetModelDualcondV2","links":["StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/attribute","StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/UNetModelDualcondV2","StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/convert_to_fp16","StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/convert_to_fp32","forward"],"tags":[],"content":"Class: UNetModelDualcondV2\nInheritance Tree (MRO):\n\nUNetModelDualcondV2\nnn.Module\nobject\n\nInstance Attributes (from self.xxx assignments):\n\nimage_size (defined in: init)\nin_channels (defined in: init)\nmodel_channels (defined in: init)\nout_channels (defined in: init)\nattention_resolutions (defined in: init)\ndropout (defined in: init)\nchannel_mult (defined in: init)\nconv_resample (defined in: init)\nnum_classes (defined in: init)\nuse_checkpoint (defined in: init)\ndtype (defined in: init)\nnum_heads (defined in: init)\nnum_head_channels (defined in: init)\nnum_heads_upsample (defined in: init)\npredict_codebook_ids (defined in: init)\ntime_embed (defined in: init)\ninput_blocks (defined in: init)\n_feature_size (defined in: init)\nmiddle_block (defined in: init)\noutput_blocks (defined in: init)\nout (defined in: init)\nnum_res_blocks (defined in: init, init)\nid_predictor (defined in: init)\nlabel_emb (defined in: init, init)\nattribute\n\nProject-defined Methods:\n\n[[init]]  ←  UNetModelDualcondV2\nconvert_to_fp16  ←  UNetModelDualcondV2\nconvert_to_fp32  ←  UNetModelDualcondV2\nforward  ←  UNetModelDualcondV2\n\nProject-defined Class Attributes:"},"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/__init__":{"slug":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/__init__","filePath":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class UNetModelDualcondV2/__init__.md","title":"__init__","links":[],"tags":[],"content":"init\n    def __init__(\n        self,\n        image_size,\n        in_channels,\n        model_channels,\n        out_channels,\n        num_res_blocks,\n        attention_resolutions,\n        dropout=0,\n        channel_mult=(1, 2, 4, 8),\n        conv_resample=True,\n        dims=2,\n        num_classes=None,\n        use_checkpoint=False,\n        use_fp16=False,\n        num_heads=-1,\n        num_head_channels=-1,\n        num_heads_upsample=-1,\n        use_scale_shift_norm=False,\n        resblock_updown=False,\n        use_new_attention_order=False,\n        use_spatial_transformer=False,    # custom transformer support\n        transformer_depth=1,              # custom transformer support\n        context_dim=None,                 # custom transformer support\n        n_embed=None,                     # custom support for prediction of discrete ids into codebook of first stage vq model\n        legacy=True,\n        disable_self_attentions=None,\n        num_attention_blocks=None,\n        disable_middle_self_attn=False,\n        use_linear_in_transformer=False,\n        semb_channels=None\n    ):\n        super().__init__()\n        if use_spatial_transformer:\n            assert context_dim is not None, &#039;Fool!! You forgot to include the dimension of your cross-attention conditioning...&#039;\n \n        if context_dim is not None:\n            assert use_spatial_transformer, &#039;Fool!! You forgot to use the spatial transformer for your cross-attention conditioning...&#039;\n            from omegaconf.listconfig import ListConfig\n            if type(context_dim) == ListConfig:\n                context_dim = list(context_dim)\n \n        if num_heads_upsample == -1:\n            num_heads_upsample = num_heads\n \n        if num_heads == -1:\n            assert num_head_channels != -1, &#039;Either num_heads or num_head_channels has to be set&#039;\n \n        if num_head_channels == -1:\n            assert num_heads != -1, &#039;Either num_heads or num_head_channels has to be set&#039;\n \n        self.image_size = image_size\n        self.in_channels = in_channels\n        self.model_channels = model_channels\n        self.out_channels = out_channels\n        if isinstance(num_res_blocks, int): # [[#residual-block|residual block]]\n            self.num_res_blocks = len(channel_mult) * [num_res_blocks]\n        else:\n            if len(num_res_blocks) != len(channel_mult):\n                raise ValueError(&quot;provide num_res_blocks either as an int (globally constant) or &quot;\n                                 &quot;as a list/tuple (per-level) with the same length as channel_mult&quot;)\n            self.num_res_blocks = num_res_blocks\n        if disable_self_attentions is not None: # [[#attention-block|attention block]]\n            # should be a list of booleans, indicating whether to disable self-attention in TransformerBlocks or not\n            assert len(disable_self_attentions) == len(channel_mult)\n        if num_attention_blocks is not None:\n            assert len(num_attention_blocks) == len(self.num_res_blocks)\n            assert all(map(lambda i: self.num_res_blocks[i] &gt;= num_attention_blocks[i], range(len(num_attention_blocks))))\n            print(f&quot;Constructor of UNetModel received num_attention_blocks={num_attention_blocks}. &quot;\n                  f&quot;This option has LESS priority than attention_resolutions {attention_resolutions}, &quot;\n                  f&quot;i.e., in cases where num_attention_blocks[i] &gt; 0 but 2**i not in attention_resolutions, &quot;\n                  f&quot;attention will still not be set.&quot;)\n\t\t# [[#unet-关键参数配置|UNet 关键参数配置]] \n        self.attention_resolutions = attention_resolutions\n        self.dropout = dropout\n        self.channel_mult = channel_mult\n        self.conv_resample = conv_resample\n        self.num_classes = num_classes\n        self.use_checkpoint = use_checkpoint\n        self.dtype = th.float16 if use_fp16 else th.float32\n        self.num_heads = num_heads\n        self.num_head_channels = num_head_channels\n        self.num_heads_upsample = num_heads_upsample\n        self.predict_codebook_ids = n_embed is not None\n\t\t# [[#time_embed|time_embed]]\n        time_embed_dim = model_channels * 4\n        self.time_embed = nn.Sequential(\n            linear(model_channels, time_embed_dim),\n            nn.SiLU(),\n            linear(time_embed_dim, time_embed_dim),\n        )\n\t\t# [[#类别条件class-conditional机制在-unet-中的实现|类别条件（Class-Conditional）机制在 UNet 中的实现]]\n        if self.num_classes is not None:\n            if isinstance(self.num_classes, int):\n                self.label_emb = nn.Embedding(num_classes, time_embed_dim)\n            elif self.num_classes == &quot;continuous&quot;:\n                print(&quot;setting up linear c_adm embedding layer&quot;)\n                self.label_emb = nn.Linear(1, time_embed_dim)\n            else:\n                raise ValueError()\n \n        self.input_blocks = nn.ModuleList(\n            [\n                TimestepEmbedSequential(\n                    conv_nd(dims, in_channels, model_channels, 3, padding=1)\n                )\n            ]\n        )\n        self._feature_size = model_channels\n        input_block_chans = [model_channels]\n        ch = model_channels\n        ds = 1\n        for level, mult in enumerate(channel_mult):\n            for nr in range(self.num_res_blocks[level]):\n                layers = [\n                    ResBlockDual(\n                        ch,\n                        time_embed_dim,\n                        dropout,\n                        semb_channels=semb_channels,\n                        out_channels=mult * model_channels,\n                        dims=dims,\n                        use_checkpoint=use_checkpoint,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                    )\n                ]\n                ch = mult * model_channels\n                if ds in attention_resolutions:\n                    if num_head_channels == -1:\n                        dim_head = ch // num_heads\n                    else:\n                        num_heads = ch // num_head_channels\n                        dim_head = num_head_channels\n                    if legacy:\n                        #num_heads = 1\n                        dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n                    if exists(disable_self_attentions):\n                        disabled_sa = disable_self_attentions[level]\n                    else:\n                        disabled_sa = False\n \n                    if not exists(num_attention_blocks) or nr &lt; num_attention_blocks[level]:\n                        layers.append(\n                            AttentionBlock(\n                                ch,\n                                use_checkpoint=use_checkpoint,\n                                num_heads=num_heads,\n                                num_head_channels=dim_head,\n                                use_new_attention_order=use_new_attention_order,\n                            ) if not use_spatial_transformer else SpatialTransformerV2(\n                                ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim,\n                                disable_self_attn=disabled_sa, use_linear=use_linear_in_transformer,\n                                use_checkpoint=use_checkpoint\n                            )\n                        )\n                self.input_blocks.append(TimestepEmbedSequential(*layers))\n                self._feature_size += ch\n                input_block_chans.append(ch)\n            if level != len(channel_mult) - 1:\n                out_ch = ch\n                self.input_blocks.append(\n                    TimestepEmbedSequential(\n                        ResBlockDual(\n                            ch,\n                            time_embed_dim,\n                            dropout,\n                            semb_channels=semb_channels,\n                            out_channels=out_ch,\n                            dims=dims,\n                            use_checkpoint=use_checkpoint,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            down=True,\n                        )\n                        if resblock_updown\n                        else Downsample(\n                            ch, conv_resample, dims=dims, out_channels=out_ch\n                        )\n                    )\n                )\n                ch = out_ch\n                input_block_chans.append(ch)\n                ds *= 2\n                self._feature_size += ch\n\t\t# [[#attention_head|attention_head]] \n        if num_head_channels == -1:\n            dim_head = ch // num_heads\n        else:\n            num_heads = ch // num_head_channels\n            dim_head = num_head_channels\n        if legacy:\n            #num_heads = 1\n            dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n        self.middle_block = TimestepEmbedSequential(\n            ResBlockDual(\n                ch,\n                time_embed_dim,\n                dropout,\n                semb_channels=semb_channels,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n            ),\n            AttentionBlock(\n                ch,\n                use_checkpoint=use_checkpoint,\n                num_heads=num_heads,\n                num_head_channels=dim_head,\n                use_new_attention_order=use_new_attention_order,\n            ) if not use_spatial_transformer else SpatialTransformerV2(  # always uses a self-attn\n                            ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim,\n                            disable_self_attn=disable_middle_self_attn, use_linear=use_linear_in_transformer,\n                            use_checkpoint=use_checkpoint\n                        ),\n            ResBlockDual(\n                ch,\n                time_embed_dim,\n                dropout,\n                semb_channels=semb_channels,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n            ),\n        )\n        self._feature_size += ch\n \n        self.output_blocks = nn.ModuleList([])\n        for level, mult in list(enumerate(channel_mult))[::-1]:\n            for i in range(self.num_res_blocks[level] + 1):\n                ich = input_block_chans.pop()\n                layers = [\n                    ResBlockDual(\n                        ch + ich,\n                        time_embed_dim,\n                        dropout,\n                        semb_channels=semb_channels,\n                        out_channels=model_channels * mult,\n                        dims=dims,\n                        use_checkpoint=use_checkpoint,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                    )\n                ]\n                ch = model_channels * mult\n                if ds in attention_resolutions:\n                    if num_head_channels == -1:\n                        dim_head = ch // num_heads\n                    else:\n                        num_heads = ch // num_head_channels\n                        dim_head = num_head_channels\n                    if legacy:\n                        #num_heads = 1\n                        dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n                    if exists(disable_self_attentions):\n                        disabled_sa = disable_self_attentions[level]\n                    else:\n                        disabled_sa = False\n \n                    if not exists(num_attention_blocks) or i &lt; num_attention_blocks[level]:\n                        layers.append(\n                            AttentionBlock(\n                                ch,\n                                use_checkpoint=use_checkpoint,\n                                num_heads=num_heads_upsample,\n                                num_head_channels=dim_head,\n                                use_new_attention_order=use_new_attention_order,\n                            ) if not use_spatial_transformer else SpatialTransformerV2(\n                                ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim,\n                                disable_self_attn=disabled_sa, use_linear=use_linear_in_transformer,\n                                use_checkpoint=use_checkpoint\n                            )\n                        )\n                if level and i == self.num_res_blocks[level]:\n                    out_ch = ch\n                    layers.append(\n                        ResBlockDual(\n                            ch,\n                            time_embed_dim,\n                            dropout,\n                            semb_channels=semb_channels,\n                            out_channels=out_ch,\n                            dims=dims,\n                            use_checkpoint=use_checkpoint,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            up=True,\n                        )\n                        if resblock_updown\n                        else Upsample(ch, conv_resample, dims=dims, out_channels=out_ch)\n                    )\n                    ds //= 2\n                self.output_blocks.append(TimestepEmbedSequential(*layers))\n                self._feature_size += ch\n \n        self.out = nn.Sequential(\n            normalization(ch),\n            nn.SiLU(),\n            zero_module(conv_nd(dims, model_channels, out_channels, 3, padding=1)),\n        )\n        if self.predict_codebook_ids:\n            self.id_predictor = nn.Sequential(\n            normalization(ch),\n            conv_nd(dims, model_channels, n_embed, 1),\n            #nn.LogSoftmax(dim=1)  # change to cross_entropy and produce non-normalized logits\n        )\nattention_head\n在使用多头注意力模块（如 AttentionBlock 或 SpatialTransformerV2）时，需要指定下列两个参数之一：\n\nnum_heads：注意力头的数量（例如 8 表示使用 8 个并行注意力分支）\nnum_head_channels：每个注意力头的通道宽度（例如 64 表示每个头维度为 64）\n\n二者的关系\n二者满足如下关系：\n\\mathrm{num\\_heads} \\times \\mathrm{num\\_head\\_channels} \\leq \\mathrm{total\\_channels}\n你只需要显式指定一个，另一个可以自动推导。\n参数检查逻辑\n源代码中的校验逻辑如下：\nif num_heads == -1:\n    assert num_head_channels != -1, &#039;Either num_heads or num_head_channels has to be set&#039;\n \nif num_head_channels == -1:\n    assert num_heads != -1, &#039;Either num_heads or num_head_channels has to be set&#039;\n也就是说，必须至少指定一个参数，否则将抛出错误。\n️ 注意事项\n\n若 total_channels 无法整除指定参数，可能导致计算出错或维度不一致；\n两者都指定时要确保一致性，即：num_heads * num_head_channels == total_channels；\n推荐做法是：设置你想控制的那一个，留另一个自动计算。\n\n示例\n假设当前层通道数为 320：\n\n若设置 num_heads=8，则 num_head_channels=320 // 8 = 40\n若设置 num_head_channels=64，则 num_heads=320 // 64 = 5\n\n# 推荐示例\nattention_block = AttentionBlock(\n    channels=320,\n    num_heads=8,\n    num_head_channels=-1,  # 自动计算为 40\n)\n \n# 或者\nattention_block = AttentionBlock(\n    channels=320,\n    num_heads=-1,\n    num_head_channels=64,  # 自动计算为 5 heads\n)\nresidual block\n这段代码用于配置UNet的各层中残差块（residual block）的数量。\n        if isinstance(num_res_blocks, int):\n            self.num_res_blocks = len(channel_mult) * [num_res_blocks]\n如果传入的num_res_blocks参数是单一整数，那么每一层将都使用这个数量的残差块。\n        else:\n            if len(num_res_blocks) != len(channel_mult):\n                raise ValueError(&quot;provide num_res_blocks either as an int (globally constant) or &quot;\n                                 &quot;as a list/tuple (per-level) with the same length as channel_mult&quot;)\n            self.num_res_blocks = num_res_blocks\n如果传入的num_res_blocks参数不是整数，那么期望为列表或其他包含整数序列的有序容器，且该容器的长度应和channel_mult的长度（也即UNet的单测深度）相同。\nattention block\n这段代码配置了attention block（自注意力机制）部分的启用情况。\n注意：在StableSR中，自注意力和交叉注意力都是通过统一的transformer模块实现的，其自注意力机制并不是SR3那样传统的dot-product attention。StableSR中，该transformer可以选择是否开启自注意力部分，但默认必须开启交叉注意力以为UNet提供必要的生成条件。\n        if disable_self_attentions is not None:\n            # should be a list of booleans, indicating whether to disable self-attention in TransformerBlocks or not\n            assert len(disable_self_attentions) == len(channel_mult)\ndisable_self_attentions参数期望是None或者与channel_mult长度相同的布尔数组，用于控制UNet内各层是否开启自注意力模块。None表示全部开启，布尔数组则按照其真值控制。\n        if num_attention_blocks is not None:\n            assert len(num_attention_blocks) == len(self.num_res_blocks)\n            assert all(map(lambda i: self.num_res_blocks[i] &gt;= num_attention_blocks[i], range(len(num_attention_blocks))))\n            print(f&quot;Constructor of UNetModel received num_attention_blocks={num_attention_blocks}. &quot;\n                  f&quot;This option has LESS priority than attention_resolutions {attention_resolutions}, &quot;\n                  f&quot;i.e., in cases where num_attention_blocks[i] &gt; 0 but 2**i not in attention_resolutions, &quot;\n                  f&quot;attention will still not be set.&quot;)\n\n\n控制 每一层中最多能插入多少个 attention 模块；\n\n\n不是层级数，而是实际 ResBlock 数量对 attention 的数量限制；\n\n\n必须匹配结构深度，即 num_attention_blocks[i] ≤ num_res_blocks[i]。\n\n\n打印语句更加清晰地表明了配置参数的优先级关系\n\n\n最终是否插入 attention block 的判定，首先看 attention_resolutions，然后才看 num_attention_blocks。\n\n\n也就是说：\n\n\n你可以设置 num_attention_blocks[i] = 1 表示“最多插入 1 个”；\n\n\n但如果 2**i 不在 attention_resolutions（例如 16, 32）中，那 attention 就不会插入；\n\n\n换句话说：你只表达了“允许插入”，插不插要由 attention_resolutions 决定。\n\n\n\n\nUNet 关键参数配置\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n参数说明attention_resolutions控制在哪些分辨率下插入 attention（如 16、32）dropoutdropout 概率，用于 regularizationchannel_mult每一层通道数是基通道数（model_channels）的几倍conv_resample是否使用卷积实现下/上采样（True 为可学习）num_classes是否使用 class-conditional 条件（如标签）use_checkpoint是否使用 gradient checkpointing 减少显存dtype网络中使用的精度（float16 或 float32）num_heads / num_head_channelsAttention 中的 head 设置num_heads_upsample上采样阶段的 head 数predict_codebook_ids是否输出 codebook token（即是否为 VQGAN decoder）\ntime_embed\n注意：这是将时间信息引入UNet的核心途径。\n        time_embed_dim = model_channels * 4\n        self.time_embed = nn.Sequential(\n            linear(model_channels, time_embed_dim),\n            nn.SiLU(),\n            linear(time_embed_dim, time_embed_dim),\n        )\n这段代码构造了一个简单的 MLP（多层感知机），将原始的 timestep embedding 映射成供 ResBlock 使用的时间条件向量。\n\n输入维度：model_channels（例如 320）\n输出维度：time_embed_dim = 4 × model_channels（例如 1280）\n激活函数：SiLU（即 Swish 激活，效果比 ReLU 更平滑）\n\nTODO:time_embed只是time aware encoder的一个模块，在进入他之前会有一个原始的time embeding（可能是正余弦位置编码），将编码结果输入这个time_embed的MLP来进一步添加可学习性，其输出用于调节残差块的工作模式。以上内容应该在后续代码和forward函数中有更多的体现。\n类别条件（Class-Conditional）机制在 UNet 中的实现\n一、什么是类别条件？\n在扩散模型（如 DDPM、StableSR、LDM）中，类别条件是一种用于引导图像生成方向的辅助信息。通过在每一步扩散中注入类别标签，模型能够学习：\n\n如何在第 t 步生成属于类别 y 的图像特征。\n\n该机制适用于分类引导图像生成，例如：\n\n生成一张“猫”而不是“狗”的图像\n合成特定类型的建筑、车辆或自然场景图像\n条件控制任务，如 super-resolution with category hints\n\n\n二、实现方式\n在 UNetModelDualcondV2 中，类别条件通过以下结构实现：\nif isinstance(self.num_classes, int):\n    self.label_emb = nn.Embedding(num_classes, time_embed_dim)\nelif self.num_classes == &quot;continuous&quot;:\n    self.label_emb = nn.Linear(1, time_embed_dim)\n支持两种类别条件格式：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n类型含义模块输入形式离散类别明确类别标签，如 0~9nn.Embeddingy ∈ {0, ..., num_classes}连续变量属性值、实数条件nn.Lineary ∈ ℝ（形如 [B, 1]）\n嵌入后将与时间嵌入相加，用于调节每个 ResBlock：\nemb = time_embed(t) + label_emb(y)\n\n三、类别条件的来源\n类别条件 不是自动生成的，而是 由用户显式提供：\n\n对于分类任务：标签 y 来自数据集；\n对于连续条件任务：如模糊程度、温度等数值，由用户设定；\n不使用类别条件时，设置 num_classes = None 即可关闭。\n\n\n四、关闭类别条件的方法\n只需在初始化模型时设定：\nnum_classes = None\n即可完全关闭类别条件路径：\n\n不构建 label_emb\n时间嵌入 emb = time_embed(t) 不再包含类别信息\n模型仅依赖时间步与其他条件（如结构条件）\n\n\n五、技术意义\n类别条件扩展了扩散模型的能力，使其支持：\n\n分类控制生成（class-conditional generation）\n多模态控制结构（如 time + class + structure）\n通用控制器设计（支持标签、模态、风格等注入）\n\n这一机制也为后续加入 cross-attention 等结构提供了条件输入的通道。"},"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/attribute":{"slug":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/attribute","filePath":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class UNetModelDualcondV2/attribute.md","title":"attribute","links":[],"tags":[],"content":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n参数作用与说明image_size输入图像的大小，用于构建 U-Net 结构的层级，一般用于输入前的尺寸校验或推断网络深度。in_channels输入图像的通道数，例如 RGB 为 3，灰度图为 1。model_channels模型的基础通道数，决定了第一层特征图的宽度。U-Net 的通道数通常是 model_channels * mult 形式增长。out_channels输出图像的通道数，通常与 in_channels 相同，例如 3 通道图像。也可以是 codebook 大小（用于 VQ）。num_res_blocks每个 downsample/upsample 层使用的 ResBlock 个数。可为 int（每层相同），也可为 list（不同层不同数量）。attention_resolutions指定在哪些 resolution 下插入 attention 模块。例如包含 4 表示在 1/4 尺度特征图上加入 self-attn 或 cross-attn。dropoutDropout 概率，控制网络的随机失活程度。用于提升泛化能力。channel_multU-Net 每个层级的通道扩展倍率。例如 (1, 2, 4, 8) 表示第4层为基通道的8倍。conv_resample是否使用卷积方式进行上下采样（如 PixelShuffle、ConvTranspose），否则使用简单插值。dims输入数据维度，2 表示 2D 图像，1 或 3 则用于序列或体数据。num_classes分类条件数量。如果设置为整数，则模型是 class-conditional；支持 int 或 &quot;continuous&quot;。use_checkpoint是否开启 gradient checkpointing（反向传播时节省内存）。会牺牲速度换空间。use_fp16是否使用 float16 精度推理，主要用于减少显存。num_heads注意力机制中 head 的数量，如果为 -1，则根据 num_head_channels 自动计算。num_head_channels每个 attention head 的维度，优先级高于 num_heads。二者需至少设置一个。num_heads_upsample用于上采样阶段的 attention head 数，默认与 num_heads 一致。use_scale_shift_norm是否在 ResBlock 中使用 FiLM 风格的 scale-shift 归一化（用于条件建模）。resblock_updown是否使用残差块进行上/下采样，否则使用 Downsample/Upsample 模块。use_new_attention_order控制 attention 顺序的实验性设置。默认为 False。use_spatial_transformer是否使用 Transformer 替代原生 AttentionBlock。开启后需设置 context_dim。transformer_depthTransformer 模块的堆叠深度（层数）。context_dimCross-Attention 中 context（如文本、图像编码）的维度，必须与 transformer 配套。n_embed如果不为 None，表示该模型用于预测 codebook 的离散 token（如 VQ-VAE / VQGAN 场景）。legacy控制是否使用 legacy 的 attention 维度设置逻辑。True 表示保持旧实现兼容性。disable_self_attentions是否禁用某些层中的 self-attn。为一个布尔列表，长度等于 channel_mult。num_attention_blocks控制每个层的 attention block 数量。优先级低于 attention_resolutions。disable_middle_self_attn是否禁用 U-Net 最底部（中间层）的 self-attn。use_linear_in_transformer是否在 transformer 中使用 Linear 形式的投影层。semb_channels结构条件通道数（例如结构图、小波子带等），会传入 ResBlockDual 进行时间/结构融合。\n与 Time-Aware Encoder 的关系\n作为 Time-Aware Encoder，这些参数中以下几项起关键作用：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n参数Time-Aware Encoder 中的意义t_emb（隐含于 time_embed）为每个时刻 t 生成时间嵌入，与结构一起参与生成（类似时间条件扩散）semb_channels支持结构条件的通道输入（如小波子带、深度图、边缘图等），通过 ResBlockDual 融合进入主干网络context_dim + use_spatial_transformer实现 Cross-Attention，用于将文本/图像上下文信息融合进特征流use_checkpoint + use_fp16控制训练时的计算效率，适合大规模时间序列训练"},"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/convert_to_fp16":{"slug":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/convert_to_fp16","filePath":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class UNetModelDualcondV2/convert_to_fp16.md","title":"convert_to_fp16","links":[],"tags":[],"content":""},"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/convert_to_fp32":{"slug":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/convert_to_fp32","filePath":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class UNetModelDualcondV2/convert_to_fp32.md","title":"convert_to_fp32","links":[],"tags":[],"content":""},"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/forward":{"slug":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/forward","filePath":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class UNetModelDualcondV2/forward.md","title":"forward","links":[],"tags":[],"content":""},"StableSR_doc/ldm/utils/Module_info":{"slug":"StableSR_doc/ldm/utils/Module_info","filePath":"StableSR_doc/ldm/utils/Module_info.md","title":"Module_info","links":["StableSR_doc/ldm/utils/get_obj_from_str","StableSR_doc/ldm/utils/instantiate_from_config"],"tags":[],"content":"Functions\n\nget_obj_from_str\ninstantiate_from_config\n"},"StableSR_doc/ldm/utils/get_obj_from_str":{"slug":"StableSR_doc/ldm/utils/get_obj_from_str","filePath":"StableSR_doc/ldm/utils/get_obj_from_str.md","title":"get_obj_from_str","links":[],"tags":[],"content":"def get_obj_from_str(string, reload=False):\n    module, cls = string.rsplit(&quot;.&quot;, 1)\n    if reload:\n        module_imp = importlib.import_module(module)\n        importlib.reload(module_imp)\n    return getattr(importlib.import_module(module, package=None), cls)"},"StableSR_doc/ldm/utils/instantiate_from_config":{"slug":"StableSR_doc/ldm/utils/instantiate_from_config","filePath":"StableSR_doc/ldm/utils/instantiate_from_config.md","title":"instantiate_from_config","links":["StableSR_doc/ldm/utils/get_obj_from_str"],"tags":[],"content":"def instantiate_from_config(config):\n    if not &quot;target&quot; in config:\n        if config == &#039;__is_first_stage__&#039;:\n            return None\n        elif config == &quot;__is_unconditional__&quot;:\n            return None\n        raise KeyError(&quot;Expected key `target` to instantiate.&quot;)\n    return get_obj_from_str(config[&quot;target&quot;])(**config.get(&quot;params&quot;, dict()))\n \n作用\n读取config配置文件,按照其指定规则将target指定的类实例化.同时在第2行if块中给出了不进行实例化的判断逻辑,可以利用它来控制开关某些模块(如注意力机制和结构条件).\nconfig 配置要求\n应该使用yaml文件,包含target项用于指定类名,params项及其子项用于指定实例化时的模型参数.\n具体的实例化逻辑和过程\nget_obj_from_str 返回一个类对象,在get_obj_from_str(…)后面的 (**config.get)(&quot;params&quot;,dict())相当于将params作为参数传入到前面的类对象的__init__中,最终完成实例化."},"StableSR_doc/pytorch-lightning":{"slug":"StableSR_doc/pytorch-lightning","filePath":"StableSR_doc/pytorch lightning.md","title":"pytorch lightning","links":[],"tags":[],"content":"pytorch lighting模块简介\npl.LightningMoudule\npl.LightningModule 是对 torch.nn.Module 的高级封装\n只需要实现下面几个关键方法，它就能帮你自动完成训练流程、GPU 分发、日志记录、checkpoint保存等复杂操作：\n\n\n\n__init__(self): 初始化模型结构、损失函数、超参数等。\n\n\n\n\nforward(self, x): 定义前向传播逻辑（注意：仅在调用 model(x) 时使用，训练逻辑用 training_step）。\n\n\n\n\ntraining_step(self, batch, batch_idx):定义一个训练步骤的行为，返回 loss。\n\n\n通常的步骤是\n\n从 batch 取出数据；\n前向传播；\n计算损失；\n使用 self.log(...) 自动记录日志（如 loss）。\n\n\n\n\n\n\nvalidation_step(...) / test_step(...):验证和测试阶段的行为，结构与 training_step 类似。\n\n\n\n\nconfigure_optimizers(self):返回优化器和（可选的）学习率调度器。\n\n\n\npytorch_Lightning Callback 机制\nPyTorch Lightning 的 Callback（回调机制）是训练过程中的事件钩子系统，允许用户在训练 / 验证 / 测试 / 保存等阶段插入自定义逻辑，类似于钩子（hook）或监听器。\n\n✅ 常见用途\n\n自动保存最佳模型（如根据 val/loss 最小）\nEarly stopping（提前停止训练）\n日志记录 / 学习率可视化\n自定义日志、评估、样本可视化等行为\n\n\n🧩 核心概念：Callback 是一个类\nfrom pytorch_lightning.callbacks import Callback\n \nclass MyCallback(Callback):\n    def on_train_start(self, trainer, pl_module):\n        print(&quot;训练开始！&quot;)\n    \n    def on_validation_end(self, trainer, pl_module):\n        print(&quot;验证阶段结束。&quot;)\n \n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n        print(f&quot;训练第 {batch_idx} 个 batch 完成&quot;)\n\n🧪 使用 Callback 的方式\nfrom pytorch_lightning import Trainer\n \ntrainer = Trainer(callbacks=[MyCallback()])\n可以一次性注册多个 callback：\ntrainer = Trainer(callbacks=[\n    MyCallback(),\n    ModelCheckpoint(...),\n    EarlyStopping(...)\n])\n\n📦 官方内置常用回调\n1. ModelCheckpoint: 保存最优模型\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n \ncheckpoint_callback = ModelCheckpoint(\n    monitor=&quot;val/loss&quot;,     # 监控哪个指标\n    save_top_k=1,           # 只保留 top1\n    mode=&quot;min&quot;,             # 目标是最小化 loss\n    filename=&quot;best-checkpoint&quot;,\n    save_last=True\n)\n2. EarlyStopping: 提前停止\nfrom pytorch_lightning.callbacks import EarlyStopping\n \nearly_stop_callback = EarlyStopping(\n    monitor=&quot;val/loss&quot;,\n    patience=5,     # 若 5 个 epoch 没有提升则停止\n    mode=&quot;min&quot;\n)\n\n📚 常用 Callback 钩子方法一览\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n方法名调用时机on_fit_startfit() 开始时on_train_start训练阶段开始on_train_end训练阶段结束on_train_batch_start每个 batch 开始前on_train_batch_end每个 batch 结束后on_validation_end每轮验证结束后on_save_checkpoint保存 checkpoint 时on_load_checkpoint加载 checkpoint 时\n\n🎯 实例：在验证后记录当前模型状态\nclass LogModelNorm(Callback):\n    def on_validation_end(self, trainer, pl_module):\n        total_norm = 0\n        for p in pl_module.parameters():\n            if p.grad is not None:\n                param_norm = p.grad.data.norm(2)\n                total_norm += param_norm.item() ** 2\n        total_norm = total_norm ** 0.5\n        print(f&quot;当前梯度范数：{total_norm:.4f}&quot;)\n\n✅ 小结\n\nCallback 是一种轻量级的插件系统；\n所有模型相关回调都可以集中管理，不污染核心训练逻辑；\n非常适合记录日志、保存中间结果、动态修改训练行为等。\n"},"index":{"slug":"index","filePath":"index.md","title":"首页","links":[],"tags":[],"content":"欢迎来到我的 Quartz Blogs\nWeclome!\n这是我的数字花园首页,你可以从左边的目录或搜索框进入笔记。"},"learn_c/data-type":{"slug":"learn_c/data-type","filePath":"learn_c/data type.md","title":"data type","links":[],"tags":[],"content":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数据类型 (关键字)常见声明方式占位符 (printf/scanf)说明charchar c = &#039;A&#039;;%c字符类型（实际上存储整数，对应 ASCII 码）。通常 1 字节 (8 位)。signed charsigned char sc = -10;%c明确带符号的字符，范围一般是 -128 ~ 127。unsigned charunsigned char uc = 250;%c 或 %hhu无符号字符，范围 0 ~ 255。intint x = 42;%d 或 %i整数，通常 4 字节，范围约 -2,147,483,648 ~ 2,147,483,647。short / short intshort s = 100;%hd短整型，一般 2 字节。unsigned shortunsigned short us = 60000;%hu无符号短整型，范围约 0 ~ 65535。long / long intlong l = 123456;%ld长整型，一般 4 或 8 字节（取决于系统）。unsigned longunsigned long ul = 4000000000UL;%lu无符号长整型。long long / long long intlong long ll = 123456789LL;%lld更大的整数，一般 8 字节。unsigned long longunsigned long long ull = 18446744073709551615ULL;%llu最大的标准无符号整数类型。floatfloat f = 3.14f;%f单精度浮点数，约 6~7 位有效数字。doubledouble d = 3.141592653;%lf双精度浮点数，约 15~16 位有效数字。long doublelong double ld = 3.141592653589793L;%Lf扩展精度浮点，精度比 double 高（平台依赖）。_Bool (C99)_Bool flag = 1;%d只能是 0 或 1。需要 &lt;stdbool.h&gt; 头文件时可写作 bool。voidvoid func(void);无表示“无类型”，用于函数返回类型或指针 (void*)。"},"分布式/Welcome":{"slug":"分布式/Welcome","filePath":"分布式/Welcome.md","title":"Welcome","links":["create-a-link"],"tags":[],"content":"This is your new vault.\nMake a note of something, create a link, or try the Importer!\nWhen you’re ready, delete this note and make the vault your own."},"数理统计2/回归/一元线性回归/1.基本假设":{"slug":"数理统计2/回归/一元线性回归/1.基本假设","filePath":"数理统计2/回归/一元线性回归/1.基本假设.md","title":"1.基本假设","links":[],"tags":[],"content":"解释变量x_,\\cdots,x_p非随机变量，样本\\{(y_i,X_i) \\mid 1\\leq i\\leq n\\}观测值x_{i1},\\cdots ,x_{ip}为常数\n误差项随机变量\\epsilon 满足\nE[\\epsilon_i]=0,i=1\\cdots ,n \n\\begin{cases}\n\\text{Var}(\\epsilon_i) = \\sigma^2, &amp; i = 1, \\cdots, n \\\\\n\\text{Cov}(\\epsilon_i, \\epsilon_j) = 0, &amp; i \\neq j\n\\end{cases}\n即各样本点上的误差项均值为零、方差相同且相互独立。"},"数理统计2/回归/一元线性回归/2.最小二乘估计OLSE":{"slug":"数理统计2/回归/一元线性回归/2.最小二乘估计OLSE","filePath":"数理统计2/回归/一元线性回归/2.最小二乘估计OLSE.md","title":"2.最小二乘估计OLSE","links":[],"tags":[],"content":"求解一元线性回归模型，就是要给出真实回归模型y=\\beta_1x+\\beta_0中\\beta_0,\\beta_1的估计\\hat{\\beta_0},\\hat{\\beta_1}.但由于残差项\\epsilon的分布类型是未知的，无法推理出y的分布类型，进而估计，于是采用最小二乘法估计\\beta_0,\\beta_1.\n最小二乘估计求出的回归方程，成为一元线性经验回归方程。\n\\hat{y}=\\hat{\\beta_1}x+\\hat{\\beta_0}\n算法\n目的为最小化总拟合误差\\left|y_i-\\hat{\\beta_1}x-\\hat{\\beta_0}\\right|.\n构造目标函数\nQ(\\gamma_0,\\gamma_1)=\\Sigma_{i=1}^n(y_i-\\gamma_0-\\gamma_1x)^2\nQ是凸函数，\n\\left\\{\n\\begin{aligned}\n\\frac{\\partial{Q}}{\\partial{\\gamma_0}} &amp;= 0 \\\\\n\\frac{\\partial{Q}}{\\partial{\\gamma_1}} &amp;= 0 \n\\end{aligned}\n\\right.\n满足上述方程组的\\gamma_0,\\gamma_1即为最小二乘估计\\hat{\\beta_0},\\hat{\\beta_1}\n\\left\\{\n\\begin{aligned}\n\\hat{\\beta_1} &amp;= \\frac{L_{xy}}{L_{xx}} \\\\\n\\hat{\\beta_0} &amp;= \\bar{y}-\\hat{\\beta_1}x\n\\end{aligned}\n\\right.\n其中\n\\left\\{\n\\begin{aligned}\nL_{xx}&amp;=\\Sigma(x_i-\\bar{x})^2 \\\\\nL_{xy}&amp;=\\Sigma(x_i-\\bar{x})(y_i-\\bar{y})\n\\end{aligned}\n\\right."},"数理统计2/回归/一元线性回归/3.回归系数的性质":{"slug":"数理统计2/回归/一元线性回归/3.回归系数的性质","filePath":"数理统计2/回归/一元线性回归/3.回归系数的性质.md","title":"3.回归系数的性质","links":[],"tags":[],"content":"线性性\n\\hat{\\beta_1},\\hat{\\beta_0}是 随机变量y_i的线性函数\n无偏性\n\\begin{aligned}\nE\\hat{\\beta_1} &amp;=E[\\frac{\\Sigma_{i=1}^n (x_i-\\bar{x})y_i}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}] \\\\\n\t\t\t   \n\t\t\t   &amp;=\\frac{\\Sigma_{i=1}^n (x_i-\\bar{x})E[y_i]}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2} \\\\\n\t\t\t   \n\t\t\t   &amp;=\\frac{\\Sigma_{i=1}^n (x_i-\\bar{x})(\\beta_1x_i+\\beta_0)}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2} \\\\\n\t\t\t   \n\t\t\t   &amp;=\\frac{\\Sigma_{i=1}^n (x_i-\\bar{x})x_i}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}\\beta_1 \\\\\n\t\t\t   \n\\end{aligned}\n其中\\frac{\\Sigma_{i=1}^n (x_i-\\bar{x})x_i}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}=1 \n故E\\hat{\\beta_1}=\\beta_1,同理可证E\\hat{\\beta_0}=\\beta_0\n方差\n由于y_1,\\cdots,y_n独立，var(y_i)=\\sigma^2\n故$$\n\\begin{aligned}\nVar(\\hat{\\beta_1}) &amp;= \\Sigma_{i=1}^{n} [\\frac{x_i - \\bar{x}}{\\Sigma_{i=1}^n (x_i-\\bar{x})^2}]^2 Var(y_i)\\\n&amp;= \\frac{\\sigma^2}{L_{xx}}\n\\end{aligned}\n\nVar(\\hat{\\beta_0})=[\\frac{1}{n}+\\frac{\\bar{x}^2}{L_{xx}}]\\sigma\n"},"数理统计2/回归/一元线性回归/4.误差随机变量方差的估计":{"slug":"数理统计2/回归/一元线性回归/4.误差随机变量方差的估计","filePath":"数理统计2/回归/一元线性回归/4.误差随机变量方差的估计.md","title":"4.误差随机变量方差的估计","links":[],"tags":[],"content":"\\hat{\\sigma}^2 = \\frac{1}{n-2} \\sum (y_i - \\hat{y}_i)^2\n其中\n\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i"},"数理统计2/回归/一元线性回归/5.显著性检验":{"slug":"数理统计2/回归/一元线性回归/5.显著性检验","filePath":"数理统计2/回归/一元线性回归/5.显著性检验.md","title":"5.显著性检验","links":[],"tags":[],"content":"H_0: \\beta_1 = 0 \\quad\\text{vs.}\\quad H_1: \\beta_1 \\neq 0\nt检验\n在假定误差项满足独立同分布，且 \\varepsilon_i \\sim N(0, \\sigma^2) 的条件下：\n回归系数估计的分布\n假设\\epsilon_i \\sim N(0,\\sigma^2） 则\\hat{\\beta}_1 \\sim N\\left(\\beta_1, \\dfrac{\\sigma^2}{L_{xx}}\\right)  ，其中  L_{xx} = \\sum (x_i - \\bar{x})^2\n在 H_0 下，有\nt = \\frac{\\hat{\\beta}_1 - 0}{\\sqrt{\\hat{\\sigma}^2 / L_{xx}}} \\sim t_{n-2}\nF检验：直接从回归效果检验显著性\n平方和分解式\n\\sum_{i=1}^{n} (y_i - \\bar{y})^2 = \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2 + \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n\n称 \\sum_{i=1}^{n} (y_i - \\bar{y})^2 为总离差平方和 (SST)，描述观测值 y 本身的方差\n称 \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2 为回归平方和 (SSR)\n称 \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 为残差平方和 (SSE)\n\nSST = SSR + SSE\n证明:\n即证\n\\sum_{i=1}^{n} (y_i - \\hat{y}_i + \\hat{y}_i - \\bar{y})^2 = \\sum_{i=1}^{n} (y_i - \\bar{y})^2 + \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n即证\n\\sum_{i=1}^{n} (y_i - \\hat{y}_i)(\\hat{y}_i - \\bar{y}) = 0\n记残差 e_i = y_i - \\hat{y}_i\n\\frac{\\partial Q}{\\partial \\beta_1} = 2 \\sum x_i (y_i - \\hat{\\beta_1}x_i-\\hat{\\beta_0}) = 0, \\quad \\frac{\\partial Q}{\\partial \\beta_0} = 2 \\sum (y_i - \\beta_0 - \\beta_1 x_i) = 0\n\\Rightarrow e_i = y_i - \\hat{y}_i = y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i \n\n\\Rightarrow \\sum x_i e_i = \\sum e_i = 0.\n\\begin{aligned}\n&amp;\\sum_{i=1}^{n} (y_i - \\hat{y}_i)(\\hat{y}_i - \\bar{y}) \\\\ \n=&amp;\\sum e_i (\\hat{y}_i - \\bar{y}) =\\sum e_i\\hat{y_i}+\\bar{y}\\sum e_i \\\\\n=&amp;\\sum e_i (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) \\\\\n=&amp; \\hat{\\beta}_0 \\sum e_i + \\hat{\\beta}_1 \\sum (e_i x_i) \\\\\n=&amp; 0\n\\end{aligned}\n证毕\n构造F分布进行检验\nSSR 越大，SSE 越小说明回归越好\nH_0: \\beta_1 = 0 \\leftrightarrow H_1: \\beta_1 \\neq 0\n在 H_0 下，F = \\frac{SSR/1}{SSE/(n-2)}，\\frac{SSE}{n-2} \\sim \\chi^2(n-2)，SSR \\sim \\chi^2(1)\n\\Rightarrow F \\xrightarrow{H_0} F(1, n-2)\n皮尔逊相关系数检验\nr = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}} = \\frac{l_{xy}}{\\sqrt{l_{xx} l_{yy}}}\nr越接近 1 拟合越好"},"数理统计2/回归/一元线性回归/6.残差分析":{"slug":"数理统计2/回归/一元线性回归/6.残差分析","filePath":"数理统计2/回归/一元线性回归/6.残差分析.md","title":"6.残差分析","links":[],"tags":[],"content":"残差分析：判断真实模型是否为线性模型\n若为线性模型则必满足 Ee_i = Ex_i \\cdot e_i = 0\n残差应在 0 附近随机波动\n残差 e_i 的方差\n\\begin{aligned}\nVar(e_i) &amp;= Var(y_i - \\hat{y}_i) \\\\\n&amp;= Var(y_i) + Var(\\hat{y}_i) - 2Cov(y_i, \\hat{y}_i)\n\\end{aligned}\n(1) Var(y_i) = \\sigma^2\n(2)\n\\begin{aligned}\nVar(\\hat{y}_i) &amp;= Var(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) \\\\\n&amp;= Var(\\hat{\\beta}_0) + Var(\\hat{\\beta}_1 x_i) + 2Cov(\\hat{\\beta}_0, \\hat{\\beta}_1 x_i) \\\\\n\\end{aligned}\n其中\n\\begin{aligned}\nVar(\\hat{\\beta}_0) &amp;= \\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{L_{xx}}\\right)\\sigma^2 \\\\\n\\\\\nVar(\\hat{\\beta}_1 x_i) &amp;= x_i^2 \\frac{\\sigma^2}{L_{xx}}\\\\\n\\\\\nCov(\\hat{\\beta}_0, \\hat{\\beta}_1) &amp;= Cov(\\bar{y} - \\hat{\\beta}_1 \\bar{x}, \\hat{\\beta}_1) \\\\\n&amp;= Cov(-\\hat{\\beta}_1 \\bar{x}, \\hat{\\beta}_1) + Cov(\\bar{y}, \\hat{\\beta}_1) \\\\\n&amp;= -\\bar{x} Var(\\hat{\\beta}_1) + \\frac{Var(y_i)}{n \\sum (x_i - \\bar{x})^2} \\cdot \\sum (x_i - \\bar{x}) \\\\\n&amp;= -\\bar{x} \\frac{\\sigma^2}{L_{xx}} + 0 \\\\\n\\end{aligned}\n故Cov(\\hat{\\beta}_0, \\hat{\\beta}_1 x_i) = -\\bar{x} x_i \\frac{\\sigma^2}{L_{xx}}\n故\n\\begin{aligned}\nVar(\\hat{y}_i) &amp;= \\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{L_{xx}}\\right)\\sigma^2 + x_i^2 \\frac{\\sigma^2}{L_{xx}} - 2\\bar{x} x_i \\frac{\\sigma^2}{L_{xx}} \\\\\n&amp;= \\left(\\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{L_{xx}}\\right)\\sigma^2\n\\end{aligned}\n(3)\n\\begin{aligned}\nCov(y_i, \\hat{y}_i) &amp;= Cov(y_i, \\hat{\\beta}_1 x_i + \\bar{y} - \\hat{\\beta}_1 \\bar{x})\\\\\n&amp;= Cov(y_i, \\hat{\\beta}_1 x_i) + (x_i - \\bar{x}) Cov(y_i, \\hat{\\beta}_1) \\\\\n&amp;= \\frac{1}{n} Var(y_i) + (x_i - \\bar{x}) Cov(y_i, \\frac{\\sum (x_i - \\bar{x}) y_i}{L_{xx}}) \\\\\n&amp;= \\frac{1}{n} Var(y_i) + \\frac{(x_i - \\bar{x})^2}{L_{xx}} Var(y_i) \\\\\n&amp;= \\left(\\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{L_{xx}}\\right)\\sigma^2\n\\end{aligned}\n综上：Var(e_i) = \\sigma^2 + \\left(\\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{L_{xx}}\\right)\\sigma^2 - 2\\left(\\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{L_{xx}}\\right)\\sigma^2 = (1 - \\frac{1}{n} - \\frac{(x_i - \\bar{x})^2}{L_{xx}})\\sigma^2 = (1 - \\hat{h}_i)\\sigma^2"},"数理统计2/回归/回归模型":{"slug":"数理统计2/回归/回归模型","filePath":"数理统计2/回归/回归模型.md","title":"回归模型","links":[],"tags":[],"content":"变量x_1,\\cdots,x_p与随机变量y存在相关关系，即确定x_1,\\cdots,x_p的取值后可以确定y的分布，y与x_1,\\cdots,x_p之间的概率模型为y=f(x_i,\\cdots,x_p)+\\epsilon当f为线性函数时，称模型为线性回归模型。"},"数理统计2/概率论与数理统计基础/常用分布":{"slug":"数理统计2/概率论与数理统计基础/常用分布","filePath":"数理统计2/概率论与数理统计基础/常用分布.md","title":"常用分布","links":[],"tags":[],"content":"t分布\n若\\epsilon \\sim N(0,1),\\eta服从自由度为n的\\chi^2分布\\eta \\sim \\chi^2(n),则称随机变量T=\\frac{\\epsilon}{\\sqrt{\\frac{\\eta}{n}}}服从自由度为n的t分布，T \\sim t(n).\nF 分布\n设\\epsilon,\\eta是自由度为m,n的独立的\\chi^2随机变量，则称随机变量F=\\frac{\\epsilon/m}{\\eta/n}所服从的分布为F分布，自由度为(m,n),记作F \\sim F(m,n)."},"数理统计2/概率论与数理统计基础/方差、协方差的性质":{"slug":"数理统计2/概率论与数理统计基础/方差、协方差的性质","filePath":"数理统计2/概率论与数理统计基础/方差、协方差的性质.md","title":"方差、协方差的性质","links":[],"tags":[],"content":"引理\nX,Y独立则对任何函数h,g成立\n E[g(X)g(Y)]=E[g(X)] \\cdot E[h(Y)]\n方差与协方差常用公式\n\n协方差的定义Cov(X,Y)=E[(X-EX)(Y-EY)]=E[XY]-EX \\cdot EY\n交换性：Cov(X,Y)=Cov(Y,X)\n线性性:Coc(aX,Y)=aCov(X,Y) Cov(\\Sigma_{i=1}^nX_i,\\Sigma_{j=1}^mY_j)=\\Sigma_{i=1}^n\\Sigma_{j=1}^m Cov(X_i,Y_j)\n方差展开式：Var(\\Sigma_{i=1}^nX_i)=\\Sigma_{i=1}^nVar(X_i)+2\\Sigma_{i&lt;j}Cov(X_i,X_j)\n"}}