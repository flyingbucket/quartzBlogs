{"StableSR_doc/README":{"slug":"StableSR_doc/README","filePath":"StableSR_doc/README.md","title":"README","links":[],"tags":[],"content":"StableSR_doc"},"StableSR_doc/ldm/models/autoencoder/AutoencoderKL":{"slug":"StableSR_doc/ldm/models/autoencoder/AutoencoderKL","filePath":"StableSR_doc/ldm/models/autoencoder/AutoencoderKL.md","title":"AutoencoderKL","links":[],"tags":[],"content":"AutoencoderKL\nSource Code\nclass AutoencoderKL(pl.LightningModule):\n    def __init__(self,\n                 ddconfig,\n                 lossconfig,\n                 embed_dim,\n                 ckpt_path=None,\n                 ignore_keys=[],\n                 image_key=&quot;image&quot;,\n                 colorize_nlabels=None,\n                 monitor=None,\n                 ):\n        super().__init__()\n        self.image_key = image_key\n        self.encoder = Encoder(**ddconfig)\n        self.decoder = Decoder(**ddconfig)\n        self.loss = instantiate_from_config(lossconfig)\n        assert ddconfig[&quot;double_z&quot;]\n        self.quant_conv = torch.nn.Conv2d(2*ddconfig[&quot;z_channels&quot;], 2*embed_dim, 1)\n        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[&quot;z_channels&quot;], 1)\n        self.embed_dim = embed_dim\n        if colorize_nlabels is not None:\n            assert type(colorize_nlabels)==int\n            self.register_buffer(&quot;colorize&quot;, torch.randn(3, colorize_nlabels, 1, 1))\n        if monitor is not None:\n            self.monitor = monitor\n        if ckpt_path is not None:\n            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys)\n \n    def init_from_ckpt(self, path, ignore_keys=list(), only_model=False):\n        sd = torch.load(path, map_location=&quot;cpu&quot;)\n        if &quot;state_dict&quot; in list(sd.keys()):\n            sd = sd[&quot;state_dict&quot;]\n        keys = list(sd.keys())\n        for k in keys:\n            if &#039;first_stage_model&#039; in k:\n                sd[k[18:]] = sd[k]\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(&quot;Deleting key {} from state_dict.&quot;.format(k))\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f&quot;Encoder Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys&quot;)\n        if len(missing) &gt; 0:\n            print(f&quot;Missing Keys: {missing}&quot;)\n        # if len(unexpected) &gt; 0:\n        #     print(f&quot;Unexpected Keys: {unexpected}&quot;)\n \n    def encode(self, x, return_encfea=False):\n        h = self.encoder(x)\n        moments = self.quant_conv(h)\n        posterior = DiagonalGaussianDistribution(moments)\n        if return_encfea:\n            return posterior, moments\n        return posterior\n \n    def encode_gt(self, x, new_encoder):\n        h = new_encoder(x)\n        moments = self.quant_conv(h)\n        posterior = DiagonalGaussianDistribution(moments)\n        return posterior, moments\n \n    def decode(self, z):\n        z = self.post_quant_conv(z)\n        dec = self.decoder(z)\n        return dec\n \n    def forward(self, input, sample_posterior=True):\n        posterior = self.encode(input)\n        if sample_posterior:\n            z = posterior.sample()\n        else:\n            z = posterior.mode()\n        dec = self.decode(z)\n        return dec, posterior\n \n    def get_input(self, batch, k):\n        x = batch[k]\n        if len(x.shape) == 3:\n            x = x[..., None]\n        # x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n        x = x.to(memory_format=torch.contiguous_format).float()\n        # x = x*2.0-1.0\n        return x\n \n    def training_step(self, batch, batch_idx, optimizer_idx):\n        inputs = self.get_input(batch, self.image_key)\n        reconstructions, posterior = self(inputs)\n \n        if optimizer_idx == 0:\n            # train encoder+decoder+logvar\n            aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=&quot;train&quot;)\n            self.log(&quot;aeloss&quot;, aeloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n            return aeloss\n \n        if optimizer_idx == 1:\n            # train the discriminator\n            discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step,\n                                                last_layer=self.get_last_layer(), split=&quot;train&quot;)\n \n            self.log(&quot;discloss&quot;, discloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n            return discloss\n \n    def validation_step(self, batch, batch_idx):\n        inputs = self.get_input(batch, self.image_key)\n        reconstructions, posterior = self(inputs)\n        aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, 0, self.global_step,\n                                        last_layer=self.get_last_layer(), split=&quot;val&quot;)\n \n        discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, 1, self.global_step,\n                                            last_layer=self.get_last_layer(), split=&quot;val&quot;)\n \n        self.log(&quot;val/rec_loss&quot;, log_dict_ae[&quot;val/rec_loss&quot;])\n        self.log_dict(log_dict_ae)\n        self.log_dict(log_dict_disc)\n        return self.log_dict\n \n    def configure_optimizers(self):\n        lr = self.learning_rate\n        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n                                  list(self.decoder.parameters())+\n                                  list(self.quant_conv.parameters())+\n                                  list(self.post_quant_conv.parameters()),\n                                  lr=lr, betas=(0.5, 0.9))\n        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n                                    lr=lr, betas=(0.5, 0.9))\n        return [opt_ae, opt_disc], []\n \n    def get_last_layer(self):\n        return self.decoder.conv_out.weight\n \n    @torch.no_grad()\n    def log_images(self, batch, only_inputs=False, **kwargs):\n        log = dict()\n        x = self.get_input(batch, self.image_key)\n        x = x.to(self.device)\n        if not only_inputs:\n            xrec, posterior = self(x)\n            if x.shape[1] &gt; 3:\n                # colorize with random projection\n                assert xrec.shape[1] &gt; 3\n                x = self.to_rgb(x)\n                xrec = self.to_rgb(xrec)\n            # log[&quot;samples&quot;] = self.decode(torch.randn_like(posterior.sample()))\n            log[&quot;reconstructions&quot;] = xrec\n        log[&quot;inputs&quot;] = x\n        return log\n \n    def to_rgb(self, x):\n        assert self.image_key == &quot;segmentation&quot;\n        if not hasattr(self, &quot;colorize&quot;):\n            self.register_buffer(&quot;colorize&quot;, torch.randn(3, x.shape[1], 1, 1).to(x))\n        x = F.conv2d(x, weight=self.colorize)\n        x = 2.*(x-x.min())/(x.max()-x.min()) - 1.\n        return x"},"StableSR_doc/ldm/models/autoencoder/AutoencoderKLResi":{"slug":"StableSR_doc/ldm/models/autoencoder/AutoencoderKLResi","filePath":"StableSR_doc/ldm/models/autoencoder/AutoencoderKLResi.md","title":"AutoencoderKLResi","links":[],"tags":[],"content":"AutoencoderKLResi\nSource Code\nclass AutoencoderKLResi(pl.LightningModule):\n    def __init__(self,\n                 ddconfig,\n                 lossconfig,\n                 embed_dim,\n                 ckpt_path=None,\n                 ignore_keys=[],\n                 image_key=&quot;image&quot;,\n                 colorize_nlabels=None,\n                 monitor=None,\n                 fusion_w=1.0,\n                 freeze_dec=True,\n                 synthesis_data=False,\n                 use_usm=False,\n                 test_gt=False,\n                 ):\n        super().__init__()\n        self.image_key = image_key\n        self.encoder = Encoder(**ddconfig)\n        self.decoder = Decoder_Mix(**ddconfig)\n        self.decoder.fusion_w = fusion_w\n        self.loss = instantiate_from_config(lossconfig)\n        self.quant_conv = torch.nn.Conv2d(2*ddconfig[&quot;z_channels&quot;], 2*embed_dim, 1)\n        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[&quot;z_channels&quot;], 1)\n        self.embed_dim = embed_dim\n        if colorize_nlabels is not None:\n            assert type(colorize_nlabels)==int\n            self.register_buffer(&quot;colorize&quot;, torch.randn(3, colorize_nlabels, 1, 1))\n        if monitor is not None:\n            self.monitor = monitor\n        if ckpt_path is not None:\n            missing_list = self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys)\n        else:\n            missing_list = []\n \n        print(&#039;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;missing&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&#039;)\n        print(missing_list)\n        self.synthesis_data = synthesis_data\n        self.use_usm = use_usm\n        self.test_gt = test_gt\n \n        if freeze_dec:\n            for name, param in self.named_parameters():\n                if &#039;fusion_layer&#039; in name:\n                    param.requires_grad = True\n                # elif &#039;encoder&#039; in name:\n                #     param.requires_grad = True\n                # elif &#039;quant_conv&#039; in name and &#039;post_quant_conv&#039; not in name:\n                #     param.requires_grad = True\n                elif &#039;loss.discriminator&#039; in name:\n                    param.requires_grad = True\n                else:\n                    param.requires_grad = False\n \n        print(&#039;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;trainable_list&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&#039;)\n        trainable_list = []\n        for name, params in self.named_parameters():\n            if params.requires_grad:\n                trainable_list.append(name)\n        print(trainable_list)\n \n        print(&#039;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;Untrainable_list&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&#039;)\n        untrainable_list = []\n        for name, params in self.named_parameters():\n            if not params.requires_grad:\n                untrainable_list.append(name)\n        print(untrainable_list)\n        # untrainable_list = list(set(trainable_list).difference(set(missing_list)))\n        # print(&#039;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;untrainable_list&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&#039;)\n        # print(untrainable_list)\n \n    # def init_from_ckpt(self, path, ignore_keys=list()):\n    #     sd = torch.load(path, map_location=&quot;cpu&quot;)[&quot;state_dict&quot;]\n    #     keys = list(sd.keys())\n    #     for k in keys:\n    #         for ik in ignore_keys:\n    #             if k.startswith(ik):\n    #                 print(&quot;Deleting key {} from state_dict.&quot;.format(k))\n    #                 del sd[k]\n    #     self.load_state_dict(sd, strict=False)\n    #     print(f&quot;Restored from {path}&quot;)\n \n    def init_from_ckpt(self, path, ignore_keys=list(), only_model=False):\n        sd = torch.load(path, map_location=&quot;cpu&quot;)\n        if &quot;state_dict&quot; in list(sd.keys()):\n            sd = sd[&quot;state_dict&quot;]\n        keys = list(sd.keys())\n        for k in keys:\n            if &#039;first_stage_model&#039; in k:\n                sd[k[18:]] = sd[k]\n                del sd[k]\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(&quot;Deleting key {} from state_dict.&quot;.format(k))\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f&quot;Encoder Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys&quot;)\n        if len(missing) &gt; 0:\n            print(f&quot;Missing Keys: {missing}&quot;)\n        if len(unexpected) &gt; 0:\n            print(f&quot;Unexpected Keys: {unexpected}&quot;)\n        return missing\n \n    def encode(self, x):\n        h, enc_fea = self.encoder(x, return_fea=True)\n        moments = self.quant_conv(h)\n        posterior = DiagonalGaussianDistribution(moments)\n        # posterior = h\n        return posterior, enc_fea\n \n    def encode_gt(self, x, new_encoder):\n        h = new_encoder(x)\n        moments = self.quant_conv(h)\n        posterior = DiagonalGaussianDistribution(moments)\n        return posterior, moments\n \n    def decode(self, z, enc_fea):\n        z = self.post_quant_conv(z)\n        dec = self.decoder(z, enc_fea)\n        return dec\n \n    def forward(self, input, latent, sample_posterior=True):\n        posterior, enc_fea_lq = self.encode(input)\n        dec = self.decode(latent, enc_fea_lq)\n        return dec, posterior\n \n    @torch.no_grad()\n    def _dequeue_and_enqueue(self):\n        &quot;&quot;&quot;It is the training pair pool for increasing the diversity in a batch.\n \n        Batch processing limits the diversity of synthetic degradations in a batch. For example, samples in a\n        batch could not have different resize scaling factors. Therefore, we employ this training pair pool\n        to increase the degradation diversity in a batch.\n        &quot;&quot;&quot;\n        # initialize\n        b, c, h, w = self.lq.size()\n        _, c_, h_, w_ = self.latent.size()\n        if b == self.configs.data.params.batch_size:\n            if not hasattr(self, &#039;queue_size&#039;):\n                self.queue_size = self.configs.data.params.train.params.get(&#039;queue_size&#039;, b*50)\n            if not hasattr(self, &#039;queue_lr&#039;):\n                assert self.queue_size % b == 0, f&#039;queue size {self.queue_size} should be divisible by batch size {b}&#039;\n                self.queue_lr = torch.zeros(self.queue_size, c, h, w).cuda()\n                _, c, h, w = self.gt.size()\n                self.queue_gt = torch.zeros(self.queue_size, c, h, w).cuda()\n                self.queue_sample = torch.zeros(self.queue_size, c, h, w).cuda()\n                self.queue_latent = torch.zeros(self.queue_size, c_, h_, w_).cuda()\n                self.queue_ptr = 0\n            if self.queue_ptr == self.queue_size:  # the pool is full\n                # do dequeue and enqueue\n                # shuffle\n                idx = torch.randperm(self.queue_size)\n                self.queue_lr = self.queue_lr[idx]\n                self.queue_gt = self.queue_gt[idx]\n                self.queue_sample = self.queue_sample[idx]\n                self.queue_latent = self.queue_latent[idx]\n                # get first b samples\n                lq_dequeue = self.queue_lr[0:b, :, :, :].clone()\n                gt_dequeue = self.queue_gt[0:b, :, :, :].clone()\n                sample_dequeue = self.queue_sample[0:b, :, :, :].clone()\n                latent_dequeue = self.queue_latent[0:b, :, :, :].clone()\n                # update the queue\n                self.queue_lr[0:b, :, :, :] = self.lq.clone()\n                self.queue_gt[0:b, :, :, :] = self.gt.clone()\n                self.queue_sample[0:b, :, :, :] = self.sample.clone()\n                self.queue_latent[0:b, :, :, :] = self.latent.clone()\n \n                self.lq = lq_dequeue\n                self.gt = gt_dequeue\n                self.sample = sample_dequeue\n                self.latent = latent_dequeue\n            else:\n                # only do enqueue\n                self.queue_lr[self.queue_ptr:self.queue_ptr + b, :, :, :] = self.lq.clone()\n                self.queue_gt[self.queue_ptr:self.queue_ptr + b, :, :, :] = self.gt.clone()\n                self.queue_sample[self.queue_ptr:self.queue_ptr + b, :, :, :] = self.sample.clone()\n                self.queue_latent[self.queue_ptr:self.queue_ptr + b, :, :, :] = self.latent.clone()\n                self.queue_ptr = self.queue_ptr + b\n \n    def get_input(self, batch):\n        input = batch[&#039;lq&#039;]\n        gt = batch[&#039;gt&#039;]\n        latent = batch[&#039;latent&#039;]\n        sample = batch[&#039;sample&#039;]\n \n        assert not torch.isnan(latent).any()\n \n        input = input.to(memory_format=torch.contiguous_format).float()\n        gt = gt.to(memory_format=torch.contiguous_format).float()\n        latent = latent.to(memory_format=torch.contiguous_format).float() / 0.18215\n \n        gt = gt * 2.0 - 1.0\n        input = input * 2.0 - 1.0\n        sample = sample * 2.0 -1.0\n \n        return input, gt, latent, sample\n \n    @torch.no_grad()\n    def get_input_synthesis(self, batch, val=False, test_gt=False):\n \n        jpeger = DiffJPEG(differentiable=False).cuda()  # simulate JPEG compression artifacts\n        im_gt = batch[&#039;gt&#039;].cuda()\n        if self.use_usm:\n            usm_sharpener = USMSharp().cuda()  # do usm sharpening\n            im_gt = usm_sharpener(im_gt)\n        im_gt = im_gt.to(memory_format=torch.contiguous_format).float()\n        kernel1 = batch[&#039;kernel1&#039;].cuda()\n        kernel2 = batch[&#039;kernel2&#039;].cuda()\n        sinc_kernel = batch[&#039;sinc_kernel&#039;].cuda()\n \n        ori_h, ori_w = im_gt.size()[2:4]\n \n        # ----------------------- The first degradation process ----------------------- #\n        # blur\n        out = filter2D(im_gt, kernel1)\n        # random resize\n        updown_type = random.choices(\n                [&#039;up&#039;, &#039;down&#039;, &#039;keep&#039;],\n                self.configs.degradation[&#039;resize_prob&#039;],\n                )[0]\n        if updown_type == &#039;up&#039;:\n            scale = random.uniform(1, self.configs.degradation[&#039;resize_range&#039;][1])\n        elif updown_type == &#039;down&#039;:\n            scale = random.uniform(self.configs.degradation[&#039;resize_range&#039;][0], 1)\n        else:\n            scale = 1\n        mode = random.choice([&#039;area&#039;, &#039;bilinear&#039;, &#039;bicubic&#039;])\n        out = F.interpolate(out, scale_factor=scale, mode=mode)\n        # add noise\n        gray_noise_prob = self.configs.degradation[&#039;gray_noise_prob&#039;]\n        if random.random() &lt; self.configs.degradation[&#039;gaussian_noise_prob&#039;]:\n            out = random_add_gaussian_noise_pt(\n                out,\n                sigma_range=self.configs.degradation[&#039;noise_range&#039;],\n                clip=True,\n                rounds=False,\n                gray_prob=gray_noise_prob,\n                )\n        else:\n            out = random_add_poisson_noise_pt(\n                out,\n                scale_range=self.configs.degradation[&#039;poisson_scale_range&#039;],\n                gray_prob=gray_noise_prob,\n                clip=True,\n                rounds=False)\n        # JPEG compression\n        jpeg_p = out.new_zeros(out.size(0)).uniform_(*self.configs.degradation[&#039;jpeg_range&#039;])\n        out = torch.clamp(out, 0, 1)  # clamp to [0, 1], otherwise JPEGer will result in unpleasant artifacts\n        out = jpeger(out, quality=jpeg_p)\n \n        # ----------------------- The second degradation process ----------------------- #\n        # blur\n        if random.random() &lt; self.configs.degradation[&#039;second_blur_prob&#039;]:\n            out = filter2D(out, kernel2)\n        # random resize\n        updown_type = random.choices(\n                [&#039;up&#039;, &#039;down&#039;, &#039;keep&#039;],\n                self.configs.degradation[&#039;resize_prob2&#039;],\n                )[0]\n        if updown_type == &#039;up&#039;:\n            scale = random.uniform(1, self.configs.degradation[&#039;resize_range2&#039;][1])\n        elif updown_type == &#039;down&#039;:\n            scale = random.uniform(self.configs.degradation[&#039;resize_range2&#039;][0], 1)\n        else:\n            scale = 1\n        mode = random.choice([&#039;area&#039;, &#039;bilinear&#039;, &#039;bicubic&#039;])\n        out = F.interpolate(\n                out,\n                size=(int(ori_h / self.configs.sf * scale),\n                      int(ori_w / self.configs.sf * scale)),\n                mode=mode,\n                )\n        # add noise\n        gray_noise_prob = self.configs.degradation[&#039;gray_noise_prob2&#039;]\n        if random.random() &lt; self.configs.degradation[&#039;gaussian_noise_prob2&#039;]:\n            out = random_add_gaussian_noise_pt(\n                out,\n                sigma_range=self.configs.degradation[&#039;noise_range2&#039;],\n                clip=True,\n                rounds=False,\n                gray_prob=gray_noise_prob,\n                )\n        else:\n            out = random_add_poisson_noise_pt(\n                out,\n                scale_range=self.configs.degradation[&#039;poisson_scale_range2&#039;],\n                gray_prob=gray_noise_prob,\n                clip=True,\n                rounds=False,\n                )\n \n        # JPEG compression + the final sinc filter\n        # We also need to resize images to desired sizes. We group [resize back + sinc filter] together\n        # as one operation.\n        # We consider two orders:\n        #   1. [resize back + sinc filter] + JPEG compression\n        #   2. JPEG compression + [resize back + sinc filter]\n        # Empirically, we find other combinations (sinc + JPEG + Resize) will introduce twisted lines.\n        if random.random() &lt; 0.5:\n            # resize back + the final sinc filter\n            mode = random.choice([&#039;area&#039;, &#039;bilinear&#039;, &#039;bicubic&#039;])\n            out = F.interpolate(\n                    out,\n                    size=(ori_h // self.configs.sf,\n                          ori_w // self.configs.sf),\n                    mode=mode,\n                    )\n            out = filter2D(out, sinc_kernel)\n            # JPEG compression\n            jpeg_p = out.new_zeros(out.size(0)).uniform_(*self.configs.degradation[&#039;jpeg_range2&#039;])\n            out = torch.clamp(out, 0, 1)\n            out = jpeger(out, quality=jpeg_p)\n        else:\n            # JPEG compression\n            jpeg_p = out.new_zeros(out.size(0)).uniform_(*self.configs.degradation[&#039;jpeg_range2&#039;])\n            out = torch.clamp(out, 0, 1)\n            out = jpeger(out, quality=jpeg_p)\n            # resize back + the final sinc filter\n            mode = random.choice([&#039;area&#039;, &#039;bilinear&#039;, &#039;bicubic&#039;])\n            out = F.interpolate(\n                    out,\n                    size=(ori_h // self.configs.sf,\n                          ori_w // self.configs.sf),\n                    mode=mode,\n                    )\n            out = filter2D(out, sinc_kernel)\n \n        # clamp and round\n        im_lq = torch.clamp(out, 0, 1.0)\n \n        # random crop\n        gt_size = self.configs.degradation[&#039;gt_size&#039;]\n        im_gt, im_lq = paired_random_crop(im_gt, im_lq, gt_size, self.configs.sf)\n        self.lq, self.gt = im_lq, im_gt\n \n        self.lq = F.interpolate(\n                self.lq,\n                size=(self.gt.size(-2),\n                      self.gt.size(-1)),\n                mode=&#039;bicubic&#039;,\n                )\n \n        self.latent = batch[&#039;latent&#039;] / 0.18215\n        self.sample = batch[&#039;sample&#039;] * 2 - 1.0\n        # training pair pool\n        if not val:\n            self._dequeue_and_enqueue()\n        # sharpen self.gt again, as we have changed the self.gt with self._dequeue_and_enqueue\n        self.lq = self.lq.contiguous()  # for the warning: grad and param do not obey the gradient layout contract\n        self.lq = self.lq*2 - 1.0\n        self.gt = self.gt*2 - 1.0\n \n        self.lq = torch.clamp(self.lq, -1.0, 1.0)\n \n        x = self.lq\n        y = self.gt\n        x = x.to(self.device)\n        y = y.to(self.device)\n \n        if self.test_gt:\n            return y, y, self.latent.to(self.device), self.sample.to(self.device)\n        else:\n            return x, y, self.latent.to(self.device), self.sample.to(self.device)\n \n    def training_step(self, batch, batch_idx, optimizer_idx):\n        if self.synthesis_data:\n            inputs, gts, latents, _ = self.get_input_synthesis(batch, val=False)\n        else:\n            inputs, gts, latents, _ = self.get_input(batch)\n        reconstructions, posterior = self(inputs, latents)\n \n        if optimizer_idx == 0:\n            # train encoder+decoder+logvar\n            aeloss, log_dict_ae = self.loss(gts, reconstructions, posterior, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=&quot;train&quot;)\n            self.log(&quot;aeloss&quot;, aeloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n            return aeloss\n \n        if optimizer_idx == 1:\n            # train the discriminator\n            discloss, log_dict_disc = self.loss(gts, reconstructions, posterior, optimizer_idx, self.global_step,\n                                                last_layer=self.get_last_layer(), split=&quot;train&quot;)\n \n            self.log(&quot;discloss&quot;, discloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n            return discloss\n \n    def validation_step(self, batch, batch_idx):\n        inputs, gts, latents, _ = self.get_input(batch)\n \n        reconstructions, posterior = self(inputs, latents)\n        aeloss, log_dict_ae = self.loss(gts, reconstructions, posterior, 0, self.global_step,\n                                        last_layer=self.get_last_layer(), split=&quot;val&quot;)\n \n        discloss, log_dict_disc = self.loss(gts, reconstructions, posterior, 1, self.global_step,\n                                            last_layer=self.get_last_layer(), split=&quot;val&quot;)\n \n        self.log(&quot;val/rec_loss&quot;, log_dict_ae[&quot;val/rec_loss&quot;])\n        self.log_dict(log_dict_ae)\n        self.log_dict(log_dict_disc)\n        return self.log_dict\n \n    def configure_optimizers(self):\n        lr = self.learning_rate\n        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n                                  list(self.decoder.parameters())+\n                                  # list(self.quant_conv.parameters())+\n                                  list(self.post_quant_conv.parameters()),\n                                  lr=lr, betas=(0.5, 0.9))\n        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n                                    lr=lr, betas=(0.5, 0.9))\n        return [opt_ae, opt_disc], []\n \n    def get_last_layer(self):\n        return self.decoder.conv_out.weight\n \n    @torch.no_grad()\n    def log_images(self, batch, only_inputs=False, **kwargs):\n        log = dict()\n        if self.synthesis_data:\n            x, gts, latents, samples = self.get_input_synthesis(batch, val=False)\n        else:\n            x, gts, latents, samples = self.get_input(batch)\n        x = x.to(self.device)\n        latents = latents.to(self.device)\n        samples = samples.to(self.device)\n        if not only_inputs:\n            xrec, posterior = self(x, latents)\n            if x.shape[1] &gt; 3:\n                # colorize with random projection\n                assert xrec.shape[1] &gt; 3\n                x = self.to_rgb(x)\n                gts = self.to_rgb(gts)\n                samples = self.to_rgb(samples)\n                xrec = self.to_rgb(xrec)\n            # log[&quot;samples&quot;] = self.decode(torch.randn_like(posterior.sample()))\n            log[&quot;reconstructions&quot;] = xrec\n        log[&quot;inputs&quot;] = x\n        log[&quot;gts&quot;] = gts\n        log[&quot;samples&quot;] = samples\n        return log\n \n    def to_rgb(self, x):\n        assert self.image_key == &quot;segmentation&quot;\n        if not hasattr(self, &quot;colorize&quot;):\n            self.register_buffer(&quot;colorize&quot;, torch.randn(3, x.shape[1], 1, 1).to(x))\n        x = F.conv2d(x, weight=self.colorize)\n        x = 2.*(x-x.min())/(x.max()-x.min()) - 1.\n        return x"},"StableSR_doc/ldm/models/autoencoder/IdentityFirstStage":{"slug":"StableSR_doc/ldm/models/autoencoder/IdentityFirstStage","filePath":"StableSR_doc/ldm/models/autoencoder/IdentityFirstStage.md","title":"IdentityFirstStage","links":[],"tags":[],"content":"IdentityFirstStage\nSource Code\nclass IdentityFirstStage(torch.nn.Module):\n    def __init__(self, *args, vq_interface=False, **kwargs):\n        self.vq_interface = vq_interface  # TODO: Should be true by default but check to not break older stuff\n        super().__init__()\n \n    def encode(self, x, *args, **kwargs):\n        return x\n \n    def decode(self, x, *args, **kwargs):\n        return x\n \n    def quantize(self, x, *args, **kwargs):\n        if self.vq_interface:\n            return x, None, [None, None, None]\n        return x\n \n    def forward(self, x, *args, **kwargs):\n        return x"},"StableSR_doc/ldm/models/autoencoder/Module_info":{"slug":"StableSR_doc/ldm/models/autoencoder/Module_info","filePath":"StableSR_doc/ldm/models/autoencoder/Module_info.md","title":"Module_info","links":["StableSR_doc/ldm/models/autoencoder/VQModel","StableSR_doc/ldm/models/autoencoder/VQModelInterface","StableSR_doc/ldm/models/autoencoder/AutoencoderKL","StableSR_doc/ldm/models/autoencoder/IdentityFirstStage","StableSR_doc/ldm/models/autoencoder/AutoencoderKLResi"],"tags":[],"content":"models/autoencoder\nFunctions\nClasses\n\nVQModel\nVQModelInterface\nAutoencoderKL\nIdentityFirstStage\nAutoencoderKLResi\n\n\n\n                  \n                  UML图解 \n                  \n                \n\n\n\n\n\n"},"StableSR_doc/ldm/models/autoencoder/VQModel":{"slug":"StableSR_doc/ldm/models/autoencoder/VQModel","filePath":"StableSR_doc/ldm/models/autoencoder/VQModel.md","title":"VQModel","links":[],"tags":[],"content":"VQModel\nSource Code\nclass VQModel(pl.LightningModule):\n    def __init__(self,\n                 ddconfig,\n                 lossconfig,\n                 n_embed,\n                 embed_dim,\n                 ckpt_path=None,\n                 ignore_keys=[],\n                 image_key=&quot;image&quot;,\n                 colorize_nlabels=None,\n                 monitor=None,\n                 batch_resize_range=None,\n                 scheduler_config=None,\n                 lr_g_factor=1.0,\n                 remap=None,\n                 sane_index_shape=False, # tell vector quantizer to return indices as bhw\n                 use_ema=False\n                 ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.n_embed = n_embed\n        self.image_key = image_key\n        self.encoder = Encoder(**ddconfig)\n        self.decoder = Decoder(**ddconfig)\n        self.loss = instantiate_from_config(lossconfig)\n        self.quantize = VectorQuantizer(n_embed, embed_dim, beta=0.25,\n                                        remap=remap,\n                                        sane_index_shape=sane_index_shape)\n        self.quant_conv = torch.nn.Conv2d(ddconfig[&quot;z_channels&quot;], embed_dim, 1)\n        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[&quot;z_channels&quot;], 1)\n        if colorize_nlabels is not None:\n            assert type(colorize_nlabels)==int\n            self.register_buffer(&quot;colorize&quot;, torch.randn(3, colorize_nlabels, 1, 1))\n        if monitor is not None:\n            self.monitor = monitor\n        self.batch_resize_range = batch_resize_range\n        if self.batch_resize_range is not None:\n            print(f&quot;{self.__class__.__name__}: Using per-batch resizing in range {batch_resize_range}.&quot;)\n \n        self.use_ema = use_ema\n        if self.use_ema:\n            self.model_ema = LitEma(self)\n            print(f&quot;Keeping EMAs of {len(list(self.model_ema.buffers()))}.&quot;)\n \n        if ckpt_path is not None:\n            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys)\n        self.scheduler_config = scheduler_config\n        self.lr_g_factor = lr_g_factor\n \n    @contextmanager\n    def ema_scope(self, context=None):\n        if self.use_ema:\n            self.model_ema.store(self.parameters())\n            self.model_ema.copy_to(self)\n            if context is not None:\n                print(f&quot;{context}: Switched to EMA weights&quot;)\n        try:\n            yield None\n        finally:\n            if self.use_ema:\n                self.model_ema.restore(self.parameters())\n                if context is not None:\n                    print(f&quot;{context}: Restored training weights&quot;)\n \n    def init_from_ckpt(self, path, ignore_keys=list()):\n        sd = torch.load(path, map_location=&quot;cpu&quot;)[&quot;state_dict&quot;]\n        keys = list(sd.keys())\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(&quot;Deleting key {} from state_dict.&quot;.format(k))\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False)\n        print(f&quot;Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys&quot;)\n        if len(missing) &gt; 0:\n            print(f&quot;Missing Keys: {missing}&quot;)\n            print(f&quot;Unexpected Keys: {unexpected}&quot;)\n \n    def on_train_batch_end(self, *args, **kwargs):\n        if self.use_ema:\n            self.model_ema(self)\n \n    def encode(self, x):\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n        quant, emb_loss, info = self.quantize(h)\n        return quant, emb_loss, info\n \n    def encode_to_prequant(self, x):\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n        return h\n \n    def decode(self, quant):\n        quant = self.post_quant_conv(quant)\n        dec = self.decoder(quant)\n        return dec\n \n    def decode_code(self, code_b):\n        quant_b = self.quantize.embed_code(code_b)\n        dec = self.decode(quant_b)\n        return dec\n \n    def forward(self, input, return_pred_indices=False):\n        quant, diff, (_,_,ind) = self.encode(input)\n        dec = self.decode(quant)\n        if return_pred_indices:\n            return dec, diff, ind\n        return dec, diff\n \n    def get_input(self, batch, k):\n        x = batch[k]\n        if len(x.shape) == 3:\n            x = x[..., None]\n        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n        if self.batch_resize_range is not None:\n            lower_size = self.batch_resize_range[0]\n            upper_size = self.batch_resize_range[1]\n            if self.global_step &lt;= 4:\n                # do the first few batches with max size to avoid later oom\n                new_resize = upper_size\n            else:\n                new_resize = np.random.choice(np.arange(lower_size, upper_size+16, 16))\n            if new_resize != x.shape[2]:\n                x = F.interpolate(x, size=new_resize, mode=&quot;bicubic&quot;)\n            x = x.detach()\n        return x\n \n    def training_step(self, batch, batch_idx, optimizer_idx):\n        # github.com/pytorch/pytorch/issues/37142\n        # try not to fool the heuristics\n        x = self.get_input(batch, self.image_key)\n        xrec, qloss, ind = self(x, return_pred_indices=True)\n \n        if optimizer_idx == 0:\n            # autoencode\n            aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=&quot;train&quot;,\n                                            predicted_indices=ind)\n \n            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n            return aeloss\n \n        if optimizer_idx == 1:\n            # discriminator\n            discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=&quot;train&quot;)\n            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n            return discloss\n \n    def validation_step(self, batch, batch_idx):\n        log_dict = self._validation_step(batch, batch_idx)\n        with self.ema_scope():\n            log_dict_ema = self._validation_step(batch, batch_idx, suffix=&quot;_ema&quot;)\n        return log_dict\n \n    def _validation_step(self, batch, batch_idx, suffix=&quot;&quot;):\n        x = self.get_input(batch, self.image_key)\n        xrec, qloss, ind = self(x, return_pred_indices=True)\n        aeloss, log_dict_ae = self.loss(qloss, x, xrec, 0,\n                                        self.global_step,\n                                        last_layer=self.get_last_layer(),\n                                        split=&quot;val&quot;+suffix,\n                                        predicted_indices=ind\n                                        )\n \n        discloss, log_dict_disc = self.loss(qloss, x, xrec, 1,\n                                            self.global_step,\n                                            last_layer=self.get_last_layer(),\n                                            split=&quot;val&quot;+suffix,\n                                            predicted_indices=ind\n                                            )\n        rec_loss = log_dict_ae[f&quot;val{suffix}/rec_loss&quot;]\n        self.log(f&quot;val{suffix}/rec_loss&quot;, rec_loss,\n                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n        self.log(f&quot;val{suffix}/aeloss&quot;, aeloss,\n                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n        if version.parse(pl.__version__) &gt;= version.parse(&#039;1.4.0&#039;):\n            del log_dict_ae[f&quot;val{suffix}/rec_loss&quot;]\n        self.log_dict(log_dict_ae)\n        self.log_dict(log_dict_disc)\n        return self.log_dict\n \n    def configure_optimizers(self):\n        lr_d = self.learning_rate\n        lr_g = self.lr_g_factor*self.learning_rate\n        print(&quot;lr_d&quot;, lr_d)\n        print(&quot;lr_g&quot;, lr_g)\n        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n                                  list(self.decoder.parameters())+\n                                  list(self.quantize.parameters())+\n                                  list(self.quant_conv.parameters())+\n                                  list(self.post_quant_conv.parameters()),\n                                  lr=lr_g, betas=(0.5, 0.9))\n        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n                                    lr=lr_d, betas=(0.5, 0.9))\n \n        if self.scheduler_config is not None:\n            scheduler = instantiate_from_config(self.scheduler_config)\n \n            print(&quot;Setting up LambdaLR scheduler...&quot;)\n            scheduler = [\n                {\n                    &#039;scheduler&#039;: LambdaLR(opt_ae, lr_lambda=scheduler.schedule),\n                    &#039;interval&#039;: &#039;step&#039;,\n                    &#039;frequency&#039;: 1\n                },\n                {\n                    &#039;scheduler&#039;: LambdaLR(opt_disc, lr_lambda=scheduler.schedule),\n                    &#039;interval&#039;: &#039;step&#039;,\n                    &#039;frequency&#039;: 1\n                },\n            ]\n            return [opt_ae, opt_disc], scheduler\n        return [opt_ae, opt_disc], []\n \n    def get_last_layer(self):\n        return self.decoder.conv_out.weight\n \n    def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs):\n        log = dict()\n        x = self.get_input(batch, self.image_key)\n        x = x.to(self.device)\n        if only_inputs:\n            log[&quot;inputs&quot;] = x\n            return log\n        xrec, _ = self(x)\n        if x.shape[1] &gt; 3:\n            # colorize with random projection\n            assert xrec.shape[1] &gt; 3\n            x = self.to_rgb(x)\n            xrec = self.to_rgb(xrec)\n        log[&quot;inputs&quot;] = x\n        log[&quot;reconstructions&quot;] = xrec\n        if plot_ema:\n            with self.ema_scope():\n                xrec_ema, _ = self(x)\n                if x.shape[1] &gt; 3: xrec_ema = self.to_rgb(xrec_ema)\n                log[&quot;reconstructions_ema&quot;] = xrec_ema\n        return log\n \n    def to_rgb(self, x):\n        assert self.image_key == &quot;segmentation&quot;\n        if not hasattr(self, &quot;colorize&quot;):\n            self.register_buffer(&quot;colorize&quot;, torch.randn(3, x.shape[1], 1, 1).to(x))\n        x = F.conv2d(x, weight=self.colorize)\n        x = 2.*(x-x.min())/(x.max()-x.min()) - 1.\n        return x"},"StableSR_doc/ldm/models/autoencoder/VQModelInterface":{"slug":"StableSR_doc/ldm/models/autoencoder/VQModelInterface","filePath":"StableSR_doc/ldm/models/autoencoder/VQModelInterface.md","title":"VQModelInterface","links":[],"tags":[],"content":"VQModelInterface\nSource Code\nclass VQModelInterface(VQModel):\n    def __init__(self, embed_dim, *args, **kwargs):\n        super().__init__(embed_dim=embed_dim, *args, **kwargs)\n        self.embed_dim = embed_dim\n \n    def encode(self, x):\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n        return h\n \n    def decode(self, h, force_not_quantize=False):\n        # also go through quantization layer\n        if not force_not_quantize:\n            quant, emb_loss, info = self.quantize(h)\n        else:\n            quant = h\n        quant = self.post_quant_conv(quant)\n        dec = self.decoder(quant)\n        return dec"},"StableSR_doc/ldm/models/diffusion/ddpm/DiffusionWrapper(pl.LightningModule)":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/DiffusionWrapper(pl.LightningModule)","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/DiffusionWrapper(pl.LightningModule).md","title":"DiffusionWrapper(pl.LightningModule)","links":[],"tags":[],"content":"class DiffusionWrapper(pl.LightningModule):\n    def __init__(self, diff_model_config, conditioning_key):\n        super().__init__()\n        self.diffusion_model = instantiate_from_config(diff_model_config) # [[instantiate_from_config]]\n        self.conditioning_key = conditioning_key\n        assert self.conditioning_key in [None, &#039;concat&#039;, &#039;crossattn&#039;, &#039;hybrid&#039;, &#039;adm&#039;]\n \n    def forward(self, x, t, c_concat: list = None, c_crossattn: list = None, struct_cond=None, seg_cond=None):\n        if self.conditioning_key is None: # [[#none无条件扩散unconditional|None：无条件扩散（Unconditional）]]\n            out = self.diffusion_model(x, t)\n        elif self.conditioning_key == &#039;concat&#039;: # [[#concat通道拼接channel-concat|concat：通道拼接（Channel Concat）]]\n            xc = torch.cat([x] + c_concat, dim=1) \n            out = self.diffusion_model(xc, t)\n        elif self.conditioning_key == &#039;crossattn&#039;: # [[#crossattn交叉注意力cross-attention|crossattn：交叉注意力（Cross-Attention）]]\n            cc = torch.cat(c_crossattn, 1)\n            if seg_cond is None:\n                out = self.diffusion_model(x, t, context=cc, struct_cond=struct_cond)\n            else:\n                out = self.diffusion_model(x, t, context=cc, struct_cond=struct_cond, seg_cond=seg_cond)\n        elif self.conditioning_key == &#039;hybrid&#039;: # [[#hybrid通道拼接--交叉注意力concat--cross-attn|hybrid：通道拼接 + 交叉注意力（Concat + Cross-Attn）]]\n            xc = torch.cat([x] + c_concat, dim=1)\n            cc = torch.cat(c_crossattn, 1)\n            out = self.diffusion_model(xc, t, context=cc)\n        elif self.conditioning_key == &#039;adm&#039;: # [[#adm标签注入classifier-free-guidance|adm：标签注入（Classifier-free Guidance）]]\n            cc = c_crossattn[0]\n            out = self.diffusion_model(x, t, y=cc)\n        else:\n            raise NotImplementedError()\n \n        return out\n \nDiffusionWrapper.forward() 中 conditioning_key 的五种模式解析\n在 StableSR 或 Latent Diffusion 中，conditioning_key 决定了条件信息如何注入到扩散模型（通常是 UNet，如 UNetModelDualConv2d）中。该参数影响的是 DiffusionWrapper 在 forward 阶段如何组织输入，并通过哪些通路将条件信息传递到扩散网络。\n\nNone：无条件扩散（Unconditional）\nout = self.diffusion_model(x, t)\n\n不使用任何条件信息；\n输入仅为带噪图像 x 和时间步 t；\n通常用于纯图像建模、初期训练或 unconditional generation。\n\n\nconcat：通道拼接（Channel Concat）\nxc = torch.cat([x] + c_concat, dim=1)\nout = self.diffusion_model(xc, t)\n\n条件以图像形式提供（如低清图、边缘图、小波子带），直接拼接到 x 上；\n通道维度变为 C + C_cond；\n是 SR3 使用的典型条件注入方法；\n是传统sr3的方法,简单高效，但灵活性较差。\n\n\ncrossattn：交叉注意力（Cross-Attention）\ncc = torch.cat(c_crossattn, 1)\nout = self.diffusion_model(x, t, context=cc, struct_cond=..., seg_cond=...)\n\n条件信息（如结构图、小波图、文本）通过 cond_stage_model 编码为 latent 向量；\n这些 latent 向量作为 context，传入 UNet 内部的 CrossAttention 模块；\n可选地支持结构条件 struct_cond 和语义条件 seg_cond；\n是现代条件扩散（如 Stable Diffusion）最常用策略，灵活且表达力强。\n\n\nhybrid：通道拼接 + 交叉注意力（Concat + Cross-Attn）\nxc = torch.cat([x] + c_concat, dim=1)\ncc = torch.cat(c_crossattn, 1)\nout = self.diffusion_model(xc, t, context=cc)\n\n同时使用 concat 和 cross-attention 两种通路注入条件；\n通道拼接传递低级信息（细节、边缘）；\ncross-attn 传递高级语义（结构 latent、小波 latent）；\n在 StableSR 中，通常两路条件信息都来源于同一个结构图，只是经过不同路径处理；\n平衡引导性与灵活性，是推荐模式之一。\n\n\nadm：标签注入（Classifier-free Guidance）\ncc = c_crossattn[0]\nout = self.diffusion_model(x, t, y=cc)\n\n用于类别或 token 作为标签条件（如 ImageNet class label）；\ny=cc 表示将条件作为类标签注入；\n需要扩散模型支持 y 输入（如通过 ConditionalBatchNorm、embedding 等）；\n常见于 ADM/DDPMv2 等分类条件生成场景。\n\n\n总结\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconditioning_key条件注入方式典型用途None无条件纯图像建模&#039;concat&#039;通道拼接SR3、结构图引导&#039;crossattn&#039;交叉注意力Stable Diffusion、结构/文本引导&#039;hybrid&#039;拼接 + 注意力StableSR、小波 + 结构联合引导&#039;adm&#039;标签输入分类条件生成（如 class-conditional DDPM）\n实际应用中，可根据条件类型和任务目标灵活选择或组合这些模式。"},"StableSR_doc/ldm/models/diffusion/ddpm/Module_info":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/Module_info","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/Module_info.md","title":"Module_info","links":["StableSR_doc/ldm/models/diffusion/ddpm/torch2img","StableSR_doc/ldm/models/diffusion/ddpm/cal_pca_components","StableSR_doc/ldm/models/diffusion/ddpm/visualize_fea","StableSR_doc/ldm/models/diffusion/ddpm/calc_mean_std","StableSR_doc/ldm/models/diffusion/ddpm/adaptive_instance_normalization","StableSR_doc/ldm/models/diffusion/ddpm/space_timesteps","StableSR_doc/ldm/models/diffusion/ddpm/disabled_train","StableSR_doc/ldm/models/diffusion/ddpm/uniform_on_device","StableSR_doc/ldm/models/diffusion/ddpm/class-DDPM/DDPM(pl.LightningModule)","LatentDiffusion(DDPM)","LatentDiffusionSRTextWT(DDPM)","LatentDiffusionSRTextWTFFHQ(LatentDiffusionSRTextWT)","StableSR_doc/ldm/models/diffusion/ddpm/DiffusionWrapper(pl.LightningModule)","Layout2ImgDiffusion(LatentDiffusion)"],"tags":[],"content":"\n\n                  \n                  UML 图解：ddpm.py文件结构 \n                  \n                \n\n\n\n‘ldm.models.diffusion.ddpm’ 中各个类的关系。\n\n\n\nFunctions\ntorch2img\ncal_pca_components\nvisualize_fea\ncalc_mean_std\nadaptive_instance_normalization\nspace_timesteps\ndisabled_train\nuniform_on_device\nClasses\nDDPM(pl.LightningModule)\nLatentDiffusion(DDPM)\nLatentDiffusionSRTextWT(DDPM)\nLatentDiffusionSRTextWTFFHQ(LatentDiffusionSRTextWT)\nDiffusionWrapper(pl.LightningModule)\nLayout2ImgDiffusion(LatentDiffusion)"},"StableSR_doc/ldm/models/diffusion/ddpm/adaptive_instance_normalization":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/adaptive_instance_normalization","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/adaptive_instance_normalization.md","title":"adaptive_instance_normalization","links":[],"tags":[],"content":""},"StableSR_doc/ldm/models/diffusion/ddpm/cal_pca_components":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/cal_pca_components","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/cal_pca_components.md","title":"cal_pca_components","links":[],"tags":[],"content":""},"StableSR_doc/ldm/models/diffusion/ddpm/calc_mean_std":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/calc_mean_std","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/calc_mean_std.md","title":"calc_mean_std","links":[],"tags":[],"content":""},"StableSR_doc/ldm/models/diffusion/ddpm/class-DDPM/DDPM(pl.LightningModule)":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/class-DDPM/DDPM(pl.LightningModule)","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/class DDPM/DDPM(pl.LightningModule).md","title":"DDPM(pl.LightningModule)","links":["register_schedule","DDPM","_get_rows_from_list","configure_optimizers","forward","get_input","get_loss","get_v","StableSR_doc/ldm/models/diffusion/ddpm/class-DDPM/init_from_ckpt","on_train_batch_end","p_losses","p_mean_variance","predict_start_from_noise","predict_start_from_z_and_v","q_mean_variance","q_posterior","q_sample","q_sample_respace","shared_step","training_step"],"tags":[],"content":"Class: DDPM\nInheritance Tree (MRO):\n\nDDPM\nLightningModule\nABC\nDeviceDtypeModuleMixin\nHyperparametersMixin\nGradInformation\nModelIO\nModelHooks\nDataHooks\nCheckpointHooks\nModule\nobject\n\nInstance Attributes (from self.xxx assignments):\n\nparameterization (defined in: [[init]])\ncond_stage_model (defined in: [[init]])\nclip_denoised (defined in: [[init]])\nlog_every_t (defined in: [[init]])\nfirst_stage_key (defined in: [[init]])\nimage_size (defined in: [[init]])\nchannels (defined in: [[init]])\nuse_positional_encodings (defined in: [[init]])\nmodel (defined in: [[init]])\nuse_ema (defined in: [[init]])\nuse_scheduler (defined in: [[init]])\nv_posterior (defined in: [[init]])\noriginal_elbo_weight (defined in: [[init]])\nl_simple_weight (defined in: [[init]])\nloss_type (defined in: [[init]])\nlearn_logvar (defined in: [[init]])\nlogvar (defined in: [[init, init]])\nmodel_ema (defined in: [[init]])\nscheduler_config (defined in: [[init]])\nmonitor (defined in: [[init]])\nnum_timesteps (defined in: register_schedule)\nlinear_start (defined in: register_schedule)\nlinear_end (defined in: register_schedule)\n\nProject-defined Methods:\n\n[[init]]  ←  DDPM\n_get_rows_from_list  ←  DDPM\nconfigure_optimizers  ←  DDPM\nforward  ←  DDPM\nget_input  ←  DDPM\nget_loss  ←  DDPM\nget_v  ←  DDPM\ninit_from_ckpt  ←  DDPM\non_train_batch_end  ←  DDPM\np_losses  ←  DDPM\np_mean_variance  ←  DDPM\npredict_start_from_noise  ←  DDPM\npredict_start_from_z_and_v  ←  DDPM\nq_mean_variance  ←  DDPM\nq_posterior  ←  DDPM\nq_sample  ←  DDPM\nq_sample_respace  ←  DDPM\nregister_schedule  ←  DDPM\nshared_step  ←  DDPM\ntraining_step  ←  DDPM\n\nProject-defined Attributes:"},"StableSR_doc/ldm/models/diffusion/ddpm/class-DDPM/__init__":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/class-DDPM/__init__","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/class DDPM/__init__.md","title":"__init__","links":["StableSR_doc/pytorch-lightning"],"tags":[],"content":"init 函数介绍\nclass DDPM(pl.LightningModule): # [[pytorch lightning#pllightningmoudule|pl.LightningMoudule]]\n    # classic DDPM with Gaussian diffusion, in image space\n    def __init__(self,\n                 unet_config,\n                 timesteps=1000,\n                 beta_schedule=&quot;linear&quot;,\n                 loss_type=&quot;l2&quot;,\n                 ckpt_path=None,\n                 ignore_keys=[],\n                 load_only_unet=False,\n                 monitor=&quot;val/loss&quot;,\n                 use_ema=True,\n                 first_stage_key=&quot;image&quot;,\n                 image_size=256,\n                 channels=3,\n                 log_every_t=100,\n                 clip_denoised=True,\n                 linear_start=1e-4,\n                 linear_end=2e-2,\n                 cosine_s=8e-3,\n                 given_betas=None,\n                 original_elbo_weight=0.,\n                 v_posterior=0.,  # weight for choosing posterior variance as sigma = (1-v) * beta_tilde + v * beta\n                 l_simple_weight=1.,\n                 conditioning_key=None,\n                 parameterization=&quot;eps&quot;,  # all assuming fixed variance schedules\n                 scheduler_config=None,\n                 use_positional_encodings=False,\n                 learn_logvar=False,\n                 logvar_init=0.,\n                 ):\n        super().__init__() \n        # [[#parameterization|parameterization]]\n        assert parameterization in [&quot;eps&quot;, &quot;x0&quot;, &quot;v&quot;], &#039;currently only supporting &quot;eps&quot; and &quot;x0&quot; and &quot;v&quot;&#039;\n        self.parameterization = parameterization\n        print(f&quot;{self.__class__.__name__}: Running in {self.parameterization}-prediction mode&quot;)\n        self.cond_stage_model = None\n        self.clip_denoised = clip_denoised\n        self.log_every_t = log_every_t\n        self.first_stage_key = first_stage_key\n        self.image_size = image_size  # try conv?\n        self.channels = channels\n        self.use_positional_encodings = use_positional_encodings\n        self.model = DiffusionWrapper(unet_config, conditioning_key) # [[DiffusionWrapper(pl.LightningModule)]]\n        count_params(self.model, verbose=True) # [[#line-45-to-57--ddpm__init__-中模型管理调度器与损失权重部分解析|line 45 to 57 🔧 `DDPM.__init__` 中模型管理、调度器与损失权重部分解析]]\n        self.use_ema = use_ema\n        if self.use_ema:\n            self.model_ema = LitEma(self.model)\n            print(f&quot;Keeping EMAs of {len(list(self.model_ema.buffers()))}.&quot;)\n \n        self.use_scheduler = scheduler_config is not None\n        if self.use_scheduler:\n            self.scheduler_config = scheduler_config\n \n        self.v_posterior = v_posterior\n        self.original_elbo_weight = original_elbo_weight\n        self.l_simple_weight = l_simple_weight\n \n        if monitor is not None: # [[#-pytorch-lightning-中的-monitor-参数简析|📈 PyTorch Lightning 中的 `monitor` 参数简析]]\n            self.monitor = monitor \n        if ckpt_path is not None: # 加载预训练模型\n            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys, only_model=load_only_unet) # [[init_from_ckpt]]\n\t\t# [[#line-64-to-end|line 64 to end]]\n\t\t# [[#1-注册调度表beta-schedule|1. 注册调度表（beta schedule）]]\n        self.register_schedule(given_betas=given_betas, beta_schedule=beta_schedule, timesteps=timesteps,\n                               linear_start=linear_start, linear_end=linear_end, cosine_s=cosine_s)\n\t\t# [[#2-设置损失函数类型|2. 设置损失函数类型]]\n        self.loss_type = loss_type\n\t\t# [[#logvar-在-ddpm-扩散模型中的作用与实现|logvar 在 DDPM 扩散模型中的作用与实现]]\n        self.learn_logvar = learn_logvar\n        self.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))\n        if self.learn_logvar:\n            self.logvar = nn.Parameter(self.logvar, requires_grad=True)\n \nparameterization\n\nparameterization: 扩散模型的预测目标，有三种：\n\n&quot;eps&quot;: 噪声预测（最常用）\n&quot;x0&quot;: 预测原始图像\n&quot;v&quot;: v-pred 方案，平衡两个极端\n\n\n断言限制只支持上述三种模式\n\nline 45 to 57 🔧 DDPM.__init__ 中模型管理、调度器与损失权重部分解析\n这部分代码负责扩散模型训练过程中的几个重要功能组件的配置，包括参数统计、EMA 平滑、调度器设置，以及损失项的加权策略。\n\n🔢 模型参数统计与打印\ncount_params(self.model, verbose=True)\n\n调用 count_params 打印模型参数数量；\nself.model 是之前创建的 DiffusionWrapper（包含 UNet 和条件控制）；\nverbose=True 表示输出详细层级参数统计，有助于模型调试与规模评估。\n\n\n🧮 EMA 模型配置（Exponential Moving Average）\nself.use_ema = use_ema\nif self.use_ema:\n    self.model_ema = LitEma(self.model)\n    print(f&quot;Keeping EMAs of {len(list(self.model_ema.buffers()))}.&quot;)\n\nuse_ema: 控制是否启用 EMA；\n如果启用，将创建 model_ema，用于在训练中对模型参数进行滑动平均；\nEMA 可在推理时提供更稳定的结果（尤其训练后期）；\nLitEma 是一个内部实现的 EMA 工具类（模仿 PyTorch EMA 实现）；\nbuffers() 提供了所有被 EMA 追踪的张量（通常是模型权重）。\n\nPS. EMA简介\n指数移动平均（Exponential Moving Average）也叫权重移动平均（Weighted Moving Average），是一种给予近期数据更高权重的平均方法。\n📈 训练调度器配置（如学习率调度）\nself.use_scheduler = scheduler_config is not None\nif self.use_scheduler:\n    self.scheduler_config = scheduler_config\n\n如果提供了 scheduler_config，则将其保存；\n后续在 configure_optimizers() 方法中会用到该配置；\n可用于定义如余弦退火（cosine annealing）、线性 warmup 等学习率调度策略。\n\n\n⚖️ 损失项权重设置\nself.v_posterior = v_posterior\nself.original_elbo_weight = original_elbo_weight\nself.l_simple_weight = l_simple_weight\n这三项参数控制扩散损失的组成：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n参数名含义v_posterior后验方差的加权控制项。用于设置预测方差的策略。具体计算形式为：σ² = (1 - v) * beta_tilde + v * beta，其中 v 就是这个参数。original_elbo_weight是否加入原始论文中的 ELBO loss 项（常为 0，代表不启用）l_simple_weightL2 或 L1 损失的主权重，用于稳定训练\n\n这部分最终将作用于 get_loss() 或 p_losses() 中的损失函数组合；\n对于不同任务（如图像重建 vs 生成），可以通过调整这些参数来平衡生成质量和保真度。\n\n\n✅ 小结\n这一部分为训练过程提供了必要的配置管理：\n\n模型参数统计有助于可视化规模；\nEMA 管理可提升训练稳定性；\n调度器设置为优化器行为提供了灵活性；\n损失项加权控制生成模型优化目标的侧重点。\n\n📈 PyTorch Lightning 中的 monitor 参数简析\nmonitor 是 PyTorch Lightning 中回调（Callback）机制的一部分，用于指定训练过程中要监控的指标名称，供如 ModelCheckpoint、EarlyStopping 等回调依据该指标执行相应逻辑（如保存模型、提前停止等）。\n详见pytorch_Lightning Callback 机制\n\n✅ 关键用途\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n组件用途ModelCheckpoint保存性能最好的模型EarlyStopping在验证指标停止提升时中止训练\n\n🧩 monitor 的工作流程\n\n\n模型内部记录指标：\nself.log(&quot;val/loss&quot;, val_loss, prog_bar=True)\n\n\n指定 monitor 的回调监听这个指标：\nModelCheckpoint(monitor=&quot;val/loss&quot;, mode=&quot;min&quot;)\n\n\nLightning 自动比较并触发保存 / 停止逻辑。\n\n\n\n⚙️ monitor 示例配置\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n \ncheckpoint = ModelCheckpoint(\n    monitor=&quot;val/psnr&quot;,   # 监听 PSNR 指标\n    mode=&quot;max&quot;,           # 指标越大越好\n    save_top_k=1,\n    filename=&quot;best-psnr&quot;\n)\nfrom pytorch_lightning.callbacks import EarlyStopping\n \nearly_stop = EarlyStopping(\n    monitor=&quot;val/loss&quot;,\n    mode=&quot;min&quot;,\n    patience=5\n)\n\n🧠 说明\n\nmonitor 是一个字符串，必须和 .log(...) 中记录的名字一致；\n不会自动创建指标值，只是引用已有指标；\n和 mode 一起决定何时触发操作（min → 越小越好，max → 越大越好）；\n在模型类中可用 self.monitor 传递给 callback 实现动态配置。\n\n\n✅ 总结\n\nmonitor 是 回调系统监听的指标名称；\n配合 .log(...) 使用；\n决定是否保存模型 / 提前停止；\n本身不计算指标，仅作为引用字段使用。\n\nline 64 to end\n这一部分主要完成了噪声调度与损失配置,具体来说,\n本部分代码完成了扩散过程中的beta调度表构建(即噪声调度)、损失函数类型设定和对数方差的初始化，是 DDPM 建模核心参数的关键配置步骤。\n\n1. 注册调度表（beta schedule）\nself.register_schedule(\n    given_betas=given_betas,\n    beta_schedule=beta_schedule,\n    timesteps=timesteps,\n    linear_start=linear_start,\n    linear_end=linear_end,\n    cosine_s=cosine_s\n)\n\n该函数根据 beta_schedule 的类型（如 “linear” 或 “cosine”）构建扩散过程中的时间步噪声系数表；\n通常会生成如下参数：\n\nbetas：每一步的噪声幅度；\nalphas, alphas_cumprod, sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod 等；\n\n\n这些系数会在后续 q_sample, q_posterior, predict_start_from_noise 等函数中使用；\n参数说明：\n\nlinear_start, linear_end: 用于线性 beta 调度起止值；\ncosine_s: 用于调整余弦调度的形状；\ngiven_betas: 若提供，优先使用用户自定义 beta 表。\n\n\n\n\n2. 设置损失函数类型\nself.loss_type = loss_type\n\n控制训练时使用哪种损失：\n\n&quot;l2&quot;（默认）：预测的噪声与真实噪声之间的均方误差；\n&quot;l1&quot;：预测残差的绝对值损失；\n也可能支持其他自定义损失类型，如 perceptual loss、hybrid loss 等；\n\n\n实际使用在 get_loss() 或 p_losses() 中处理。\n\n\n3. 初始化对数方差（log-variance）\nself.learn_logvar = learn_logvar\nself.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))\nif self.learn_logvar:\n    self.logvar = nn.Parameter(self.logvar, requires_grad=True)\n\nlogvar 是一个长度为 num_timesteps 的张量，表示每一扩散时间步的对数方差；\n如果启用 learn_logvar=True，则将其设为可学习参数（nn.Parameter），允许模型自动优化每一时间步的不确定性；\n如果不启用，logvar 就是一个固定的常量值张量；\n在 get_loss() 中会参与 KL 项或 likelihood 的权重调整。\n\nlogvar 在 DDPM 扩散模型中的作用与实现\nlogvar（对数方差）是扩散模型（如 DDPM）中用于控制训练损失权重和不确定性建模的一个重要变量。在 StableSR 和 DDPM 实现中，它是 diffusion 模型的一部分，而非 encoder 或 decoder 的组成部分。\n\n1. 背景：为何需要 logvar\n扩散模型训练时通常以预测噪声为目标，基本损失形式为(以L2损失为例)：\nL_t = \\left\\| \\varepsilon_{\\text{pred}} - \\varepsilon_{\\text{true}} \\right\\|^2\n为了增强灵活性、稳定性或逼近对数似然，一些变体引入了对数方差 logvar，使得损失函数变为：\nL_t = \\frac{1}{2} \\cdot \\exp(-\\text{logvar}_t) \\cdot \\left\\| \\varepsilon_{\\text{pred}} - \\varepsilon_{\\text{true}} \\right\\|^2 + \\frac{1}{2} \\cdot \\text{logvar}_t\n这相当于使用一个可变的时间步权重项，用于：\n\n控制每一时间步损失的相对重要性；\n模拟高斯似然中的分布不确定性；\n使得模型对某些时间步预测更加稳健。\n\n\n2. 初始化方式\nlogvar 通常被初始化为常数张量：\nself.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))\n\nlogvar_init: 对数方差的初始值（常为 0）；\nnum_timesteps: 扩散总步数（如 1000）；\n得到形状为 (T,) 的 logvar 张量，其中 T 是时间步数。\n\n若启用可学习方差：\nif self.learn_logvar:\n    self.logvar = nn.Parameter(self.logvar, requires_grad=True)\n则该张量在训练中会自动更新，每一时间步都有不同的可学习不确定性。\n\n3. 使用方式（训练时）\nlogvar 通常参与损失函数定义，在 get_loss() 或 p_losses() 中用作动态权重：\nloss = weighted_mse / torch.exp(self.logvar[t]) + self.logvar[t]\n或更复杂的：\nloss = loss_weight * loss_raw + offset * logvar[t]\n\n4. logvar总结\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n项目内容名称logvar（对数方差）类型Tensor / nn.Parameter维度(num_timesteps,)用途控制不同时间步的损失权重与不确定性建模初始化方法torch.full(size, fill_value)是否可训练由 learn_logvar 参数控制\nlogvar 是一个扩散过程中的权重调节器，尤其在加入 ELBO、VLB 等目标时尤为关键。\n\n总结\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n项目功能说明register_schedule()构造扩散过程中每一步的 beta 参数，用于控制加噪过程loss_type决定训练时的损失函数类型，如 L2 或 L1learn_logvar 和 logvar控制是否学习每个时间步的对数方差，以适配不同的不确定性建模策略\n这些设置构成了扩散模型训练阶段的核心数学基础。"},"StableSR_doc/ldm/models/diffusion/ddpm/class-DDPM/forward":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/class-DDPM/forward","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/class DDPM/forward.md","title":"forward","links":[],"tags":[],"content":""},"StableSR_doc/ldm/models/diffusion/ddpm/class-DDPM/init_from_ckpt":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/class-DDPM/init_from_ckpt","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/class DDPM/init_from_ckpt.md","title":"init_from_ckpt","links":[],"tags":[],"content":"    def init_from_ckpt(self, path, ignore_keys=list(), only_model=False):\n        sd = torch.load(path, map_location=&quot;cpu&quot;)\n        if &quot;state_dict&quot; in list(sd.keys()):\n            sd = sd[&quot;state_dict&quot;]\n        keys = list(sd.keys())\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(&quot;Deleting key {} from state_dict.&quot;.format(k))\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(&#039;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&#039;)\n        print(f&quot;Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys&quot;)\n        if len(missing) &gt; 0:\n            print(f&quot;Missing Keys: {missing}&quot;)\n        if len(unexpected) &gt; 0:\n            print(f&quot;Unexpected Keys: {unexpected}&quot;)\ninit_from_ckpt(path, ignore_keys=list(), only_model=False) 函数解析\n该函数用于从 checkpoint 文件中加载模型参数，可选择只加载 UNet（self.model），或加载整个 DDPM 模型本身。支持忽略部分参数键名，适用于微调、迁移学习等场景。\n\n函数签名\ndef init_from_ckpt(self, path, ignore_keys=list(), only_model=False):\n\npath: checkpoint 文件路径（通常为 .ckpt 或 .pth）\nignore_keys: 字符串列表，指定哪些 key 应从 state_dict 中排除（常用于忽略不兼容的模块）\nonly_model: 若为 True，则只加载 self.model 的参数，不加载整个 DDPM 类结构（适用于仅更新 UNet 时）\n\n\n1. 加载 checkpoint 字典\nsd = torch.load(path, map_location=&quot;cpu&quot;)\nif &quot;state_dict&quot; in list(sd.keys()):\n    sd = sd[&quot;state_dict&quot;]\n\n使用 torch.load 加载权重；\n有些 checkpoint 是通过 PyTorch Lightning 保存的，外层是个字典，内部的模型权重位于 state_dict 键下；\n这一步兼容这两种结构。\n\n\n2. 根据 ignore_keys 删除不需要的参数\nkeys = list(sd.keys())\nfor k in keys:\n    for ik in ignore_keys:\n        if k.startswith(ik):\n            print(&quot;Deleting key {} from state_dict.&quot;.format(k))\n            del sd[k]\n\n遍历所有参数名（key），如果以 ik 中任一字符串开头，则删除该 key；\n典型用途：跳过 cond_stage_model, model_ema, scheduler 等与当前任务无关的部分。\n\n\n3. 加载权重到模型中\nmissing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(sd, strict=False)\n\nstrict=False：允许 checkpoint 和当前模型结构不完全一致（否则会报错）；\nmissing：当前模型中存在而 checkpoint 中没有的 key；\nunexpected：checkpoint 中存在但当前模型没有的 key；\n根据 only_model 决定加载整个 DDPM 模型或仅加载其 self.model（通常是 UNet）。\n\n\n4. 打印加载结果\nprint(&#039;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&#039;)\nprint(f&quot;Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys&quot;)\nif len(missing) &gt; 0:\n    print(f&quot;Missing Keys: {missing}&quot;)\nif len(unexpected) &gt; 0:\n    print(f&quot;Unexpected Keys: {unexpected}&quot;)\n\n清晰地汇报恢复情况；\n若 missing/unexpected 过多，可能说明模型结构不匹配，需要调整配置或 ignore_keys。\n\n\n总结表格\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n参数作用path指定加载的 checkpoint 文件路径ignore_keys忽略掉具有特定前缀的参数键（如 model_ema）only_model是否只加载 self.model（通常为 UNet）strict=False宽松加载，不要求结构完全一致missing / unexpected分别记录缺失和多余的参数 key 名\n此函数广泛用于 StableSR/LDMS/LatentDiffusion 的预训练模型加载与微调流程中，推荐配合 YAML 配置与 callback 一起使用。"},"StableSR_doc/ldm/models/diffusion/ddpm/class-LatentDiffusionSRTextWT/LatentDiffusionSRTextWT":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/class-LatentDiffusionSRTextWT/LatentDiffusionSRTextWT","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/class LatentDiffusionSRTextWT/LatentDiffusionSRTextWT.md","title":"LatentDiffusionSRTextWT","links":["StableSR_doc/ldm/models/diffusion/ddpm/class-LatentDiffusionSRTextWT/LatentDiffusionSRTextWT","_gaussian_weights","_get_denoise_row_from_list","_get_rows_from_list","DDPM","_predict_eps_from_xstart","_prior_bpd","_rescale_annotations","apply_model","configure_optimizers","delta_border","differentiable_decode_first_stage","forward","get_first_stage_encoding","get_fold_unfold","get_learned_conditioning","get_loss","get_v","get_weighting","StableSR_doc/ldm/models/diffusion/ddpm/class-DDPM/init_from_ckpt","instantiate_cond_stage","instantiate_first_stage","instantiate_structcond_stage","make_cond_schedule","meshgrid","on_train_batch_end","p_losses","p_mean_variance","p_mean_variance_canvas","predict_start_from_noise","predict_start_from_z_and_v","q_mean_variance","q_posterior","q_sample","q_sample_respace","randn_cropinput","register_schedule","shared_step","training_step"],"tags":[],"content":"Class: LatentDiffusionSRTextWT\nInheritance Tree (MRO):\n\nLatentDiffusionSRTextWT\nDDPM\nLightningModule\nABC\nDeviceDtypeModuleMixin\nHyperparametersMixin\nGradInformation\nModelIO\nModelHooks\nDataHooks\nCheckpointHooks\nModule\nobject\n\nInstance Attributes (from self.xxx assignments):\n\nnum_timesteps_cond (defined in: init)\nscale_by_std (defined in: init)\nunfrozen_diff (defined in: init)\nrandom_size (defined in: init)\ntest_gt (defined in: init)\ntime_replace (defined in: init, init)\nuse_usm (defined in: init)\nmix_ratio (defined in: init)\nconcat_mode (defined in: init)\ncond_stage_trainable (defined in: init)\ncond_stage_key (defined in: init)\ncond_stage_forward (defined in: init)\nclip_denoised (defined in: init)\nbbox_tokenizer (defined in: init)\nrestarted_from_ckpt (defined in: init, init)\nori_timesteps (defined in: init)\nnum_downs (defined in: init, init)\nscale_factor (defined in: init)\np2_gamma (defined in: init)\np2_k (defined in: init)\nsnr (defined in: init, init, p_losses)\ncond_ids (defined in: make_cond_schedule)\nshorten_cond_schedule (defined in: register_schedule)\nfirst_stage_model (defined in: instantiate_first_stage)\ncond_stage_model (defined in: instantiate_cond_stage, instantiate_cond_stage, instantiate_cond_stage, instantiate_cond_stage)\nstructcond_stage_model (defined in: instantiate_structcond_stage)\nqueue_size (defined in: _dequeue_and_enqueue)\nqueue_lr (defined in: _dequeue_and_enqueue, _dequeue_and_enqueue)\nqueue_gt (defined in: _dequeue_and_enqueue, _dequeue_and_enqueue)\nqueue_ptr (defined in: _dequeue_and_enqueue, _dequeue_and_enqueue)\nlq (defined in: _dequeue_and_enqueue, get_input, get_input, get_input, get_input, get_input)\ngt (defined in: _dequeue_and_enqueue, get_input)\ncolorize (defined in: to_rgb)\n\nProject-defined Methods:\n\n[[init]]  ←  LatentDiffusionSRTextWT\n_gaussian_weights  ←  LatentDiffusionSRTextWT\n_get_denoise_row_from_list  ←  LatentDiffusionSRTextWT\n_get_rows_from_list  ←  DDPM\n_predict_eps_from_xstart  ←  LatentDiffusionSRTextWT\n_prior_bpd  ←  LatentDiffusionSRTextWT\n_rescale_annotations  ←  LatentDiffusionSRTextWT\napply_model  ←  LatentDiffusionSRTextWT\nconfigure_optimizers  ←  LatentDiffusionSRTextWT\ndelta_border  ←  LatentDiffusionSRTextWT\ndifferentiable_decode_first_stage  ←  LatentDiffusionSRTextWT\nforward  ←  LatentDiffusionSRTextWT\nget_first_stage_encoding  ←  LatentDiffusionSRTextWT\nget_fold_unfold  ←  LatentDiffusionSRTextWT\nget_learned_conditioning  ←  LatentDiffusionSRTextWT\nget_loss  ←  DDPM\nget_v  ←  DDPM\nget_weighting  ←  LatentDiffusionSRTextWT\ninit_from_ckpt  ←  DDPM\ninstantiate_cond_stage  ←  LatentDiffusionSRTextWT\ninstantiate_first_stage  ←  LatentDiffusionSRTextWT\ninstantiate_structcond_stage  ←  LatentDiffusionSRTextWT\nmake_cond_schedule  ←  LatentDiffusionSRTextWT\nmeshgrid  ←  LatentDiffusionSRTextWT\non_train_batch_end  ←  DDPM\np_losses  ←  LatentDiffusionSRTextWT\np_mean_variance  ←  LatentDiffusionSRTextWT\np_mean_variance_canvas  ←  LatentDiffusionSRTextWT\npredict_start_from_noise  ←  DDPM\npredict_start_from_z_and_v  ←  DDPM\nq_mean_variance  ←  DDPM\nq_posterior  ←  DDPM\nq_sample  ←  DDPM\nq_sample_respace  ←  DDPM\nrandn_cropinput  ←  LatentDiffusionSRTextWT\nregister_schedule  ←  LatentDiffusionSRTextWT\nshared_step  ←  LatentDiffusionSRTextWT\ntraining_step  ←  DDPM\n\nProject-defined Class Attributes:"},"StableSR_doc/ldm/models/diffusion/ddpm/disabled_train":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/disabled_train","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/disabled_train.md","title":"disabled_train","links":[],"tags":[],"content":""},"StableSR_doc/ldm/models/diffusion/ddpm/space_timesteps":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/space_timesteps","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/space_timesteps.md","title":"space_timesteps","links":[],"tags":[],"content":""},"StableSR_doc/ldm/models/diffusion/ddpm/torch2img":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/torch2img","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/torch2img.md","title":"torch2img","links":[],"tags":[],"content":"def torch2img(input):\n    input_ = input[0]\n    input_ = input_.permute(1,2,0)\n    input_ = input_.data.cpu().numpy()\n    input_ = (input_ + 1.0) / 2\n    cv2.imwrite(&#039;./test.png&#039;, input_[:,:,::-1]*255.0)"},"StableSR_doc/ldm/models/diffusion/ddpm/uniform_on_device":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/uniform_on_device","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/uniform_on_device.md","title":"uniform_on_device","links":[],"tags":[],"content":""},"StableSR_doc/ldm/models/diffusion/ddpm/visualize_fea":{"slug":"StableSR_doc/ldm/models/diffusion/ddpm/visualize_fea","filePath":"StableSR_doc/ldm/models/diffusion/ddpm/visualize_fea.md","title":"visualize_fea","links":[],"tags":[],"content":""},"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/AttentionBlock":{"slug":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/AttentionBlock","filePath":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/AttentionBlock.md","title":"AttentionBlock","links":[],"tags":[],"content":""},"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/Module_info":{"slug":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/Module_info","filePath":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/Module_info.md","title":"Module_info","links":["convert_module_to_f16","convert_module_to_f32","exists","cal_fea_cossim","count_flops_attn","AttentionPool2d","TimestepBlock","TimestepBlockDual","TimestepBlock3cond","TimestepEmbedSequential","Upsample","TransposedUpsample","Downsample","ResBlock","StableSR_doc/ldm/modules/diffusionmodules/openaimodel/ResBlockDual","StableSR_doc/ldm/modules/diffusionmodules/openaimodel/AttentionBlock","QKVAttentionLegacy","QKVAttention","UNetModel","StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/UNetModelDualcondV2","EncoderUNetModelWT"],"tags":[],"content":"ldm/modules/diffusionmodules/openaimodel.py\n\n\n                  \n                  UML图解：openaimodels.py \n                  \n                \n\n\n\n\n\n\nFunctions\n\nconvert_module_to_f16\nconvert_module_to_f32\nexists\ncal_fea_cossim\ncount_flops_attn\n\nClasses\n\nAttentionPool2d\nTimestepBlock\nTimestepBlockDual\nTimestepBlock3cond\nTimestepEmbedSequential\nUpsample\nTransposedUpsample\nDownsample\nResBlock\nResBlockDual\nAttentionBlock\nQKVAttentionLegacy\nQKVAttention\nUNetModel\nUNetModelDualcondV2\nEncoderUNetModelWT\n"},"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/ResBlockDual":{"slug":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/ResBlockDual","filePath":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/ResBlockDual.md","title":"ResBlockDual","links":[],"tags":[],"content":""},"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-TimestepEmbedSequential":{"slug":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-TimestepEmbedSequential","filePath":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class TimestepEmbedSequential.md","title":"class TimestepEmbedSequential","links":[],"tags":[],"content":"class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n    &quot;&quot;&quot;\n    A sequential module that passes timestep embeddings to the children that\n    support it as an extra input.\n    &quot;&quot;&quot;\n \n    def forward(self, x, emb, context=None, struct_cond=None, seg_cond=None):\n        for layer in self:\n            if isinstance(layer, TimestepBlock):\n                x = layer(x, emb)\n            elif isinstance(layer, SpatialTransformer) or isinstance(layer, SpatialTransformerV2):\n                assert context is not None\n                x = layer(x, context)\n            elif isinstance(layer, TimestepBlockDual):\n                assert struct_cond is not None\n                x = layer(x, emb, struct_cond)\n            elif isinstance(layer, TimestepBlock3cond):\n                assert seg_cond is not None\n                x = layer(x, emb, struct_cond, seg_cond)\n            else:\n                x = layer(x)\n        return x\n类定义\nclass TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n    &quot;&quot;&quot;\n    A sequential module that passes timestep embeddings to the children that\n    support it as an extra input.\n    &quot;&quot;&quot;\n该类是一个扩展的 nn.Sequential 模块容器，支持将 timestep embedding 以及其他条件信息（如结构条件、语义条件、cross-attention）自动传递给其内部子模块。\n\n功能与用途\nTimestepEmbedSequential 用于构建带有条件调制能力的网络层序列，特别适用于扩散模型中的 UNet 结构。其关键特性包括：\n\n支持对时间嵌入向量 emb 的自动传递；\n根据子模块类型，自动传递结构条件（struct_cond）、语义条件（seg_cond）或 cross-attention 条件（context）；\n兼容 ResBlock、Attention、Transformer 等多种子模块。\n\n\nforward 方法逻辑\ndef forward(self, x, emb, context=None, struct_cond=None, seg_cond=None):\n    for layer in self:\n        if isinstance(layer, TimestepBlock):\n            x = layer(x, emb)\n        elif isinstance(layer, SpatialTransformer) or isinstance(layer, SpatialTransformerV2):\n            assert context is not None\n            x = layer(x, context)\n        elif isinstance(layer, TimestepBlockDual):\n            assert struct_cond is not None\n            x = layer(x, emb, struct_cond)\n        elif isinstance(layer, TimestepBlock3cond):\n            assert seg_cond is not None\n            x = layer(x, emb, struct_cond, seg_cond)\n        else:\n            x = layer(x)\n    return x\n函数意义：\n该函数依次将输入 x 传入每个子模块，并根据子模块类型自动传入所需的调制条件（如 timestep embedding、cross-attention 条件、结构条件、语义条件）。\n\ncase 1: TimestepBlock\nif isinstance(layer, TimestepBlock):\n    x = layer(x, emb)\n\n代表模块：ResBlock, ResBlockDual（它们一般都继承了 TimestepBlock）\n输入条件：时间嵌入 emb\n作用：将时间信息作为调制向量注入网络中，控制每一时刻的生成行为\n\n\n🧩 case 2: SpatialTransformer / SpatialTransformerV2\nelif isinstance(layer, SpatialTransformer) or isinstance(layer, SpatialTransformerV2):\n    assert context is not None\n    x = layer(x, context)\n\n代表模块：Transformer 模块，带 cross-attention\n输入条件：context，如文本、图像结构、标签等\n作用：使用 Cross-Attention 引导注意力从上下文中提取关联特征（不使用时间嵌入 emb）\n\n\n🧩 case 3: TimestepBlockDual\nelif isinstance(layer, TimestepBlockDual):\n    assert struct_cond is not None\n    x = layer(x, emb, struct_cond)\n\n代表模块：ResBlockDual 等\n输入条件：时间嵌入 emb + 结构条件 struct_cond\n作用：用于融合时序 + 图结构（如小波子带、边缘图等）条件进行调制\n\n\n🧩 case 4: TimestepBlock3cond\nelif isinstance(layer, TimestepBlock3cond):\n    assert seg_cond is not None\n    x = layer(x, emb, struct_cond, seg_cond)\n\n代表模块：三条件版本的 ResBlock（例如 StableSR 的结构 + 语义 + 时间）\n输入条件：时间嵌入 emb + 结构条件 + 语义分割条件\n作用：融合多源引导信息进行条件控制建模\n\n\n🧩 default fallback（普通模块）\nelse:\n    x = layer(x)\n\n代表模块：普通的卷积、归一化、激活函数、上下采样模块等\n输入条件：无，直接传入 x\n作用：保持原始特征流程，不参与调制\n\n\n✅ 总结\n该 forward 函数实现了：\n\n统一入口处理多类型条件模块；\n保证每类模块只接收它需要的调制信号；\n使得 UNet 模块可以灵活堆叠普通层、ResBlock、Transformer 结构等，并自动分发对应条件；\n是 StableSR/LatentDiffusion 模型多条件融合结构的关键设计之一。\n\n\n四、模块类型支持一览\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n子模块类型条件输入描述TimestepBlock(x, emb)标准时间调制模块（如 ResBlock）SpatialTransformer(x, context)cross-attn 条件，如文本、结构图TimestepBlockDual(x, emb, struct_cond)时间 + 结构条件模块TimestepBlock3cond(x, emb, struct_cond, seg_cond)时间 + 结构 + 语义条件模块其他普通模块x无条件，仅传递输入张量\n\n五、使用示例\nblock = TimestepEmbedSequential(\n    ResBlock(...),                # 支持时间嵌入\n    SpatialTransformer(...),     # 支持 cross-attention\n    Downsample(...),             # 普通模块\n)\nout = block(x, emb, context=ctx, struct_cond=sc)\n\n六、用途总结\n该类是 StableSR / LDM / StableDiffusion 等扩散模型中构建条件感知网络的标准模式，提供了灵活且自动化的方式将多种调制条件传入网络层，极大增强了 UNet 的适配能力。"},"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/UNetModelDualcondV2":{"slug":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/UNetModelDualcondV2","filePath":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class UNetModelDualcondV2/UNetModelDualcondV2.md","title":"UNetModelDualcondV2","links":["StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/attribute","StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/UNetModelDualcondV2","StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/convert_to_fp16","StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/convert_to_fp32","forward"],"tags":[],"content":"Class: UNetModelDualcondV2\nInheritance Tree (MRO):\n\nUNetModelDualcondV2\nnn.Module\nobject\n\nInstance Attributes (from self.xxx assignments):\n\nimage_size (defined in: init)\nin_channels (defined in: init)\nmodel_channels (defined in: init)\nout_channels (defined in: init)\nattention_resolutions (defined in: init)\ndropout (defined in: init)\nchannel_mult (defined in: init)\nconv_resample (defined in: init)\nnum_classes (defined in: init)\nuse_checkpoint (defined in: init)\ndtype (defined in: init)\nnum_heads (defined in: init)\nnum_head_channels (defined in: init)\nnum_heads_upsample (defined in: init)\npredict_codebook_ids (defined in: init)\ntime_embed (defined in: init)\ninput_blocks (defined in: init)\n_feature_size (defined in: init)\nmiddle_block (defined in: init)\noutput_blocks (defined in: init)\nout (defined in: init)\nnum_res_blocks (defined in: init, init)\nid_predictor (defined in: init)\nlabel_emb (defined in: init, init)\nattribute\n\nProject-defined Methods:\n\n[[init]]  ←  UNetModelDualcondV2\nconvert_to_fp16  ←  UNetModelDualcondV2\nconvert_to_fp32  ←  UNetModelDualcondV2\nforward  ←  UNetModelDualcondV2\n\nProject-defined Class Attributes:"},"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/__init__":{"slug":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/__init__","filePath":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class UNetModelDualcondV2/__init__.md","title":"__init__","links":[],"tags":[],"content":"init\n    def __init__(\n        self,\n        image_size,\n        in_channels,\n        model_channels,\n        out_channels,\n        num_res_blocks,\n        attention_resolutions,\n        dropout=0,\n        channel_mult=(1, 2, 4, 8),\n        conv_resample=True,\n        dims=2,\n        num_classes=None,\n        use_checkpoint=False,\n        use_fp16=False,\n        num_heads=-1,\n        num_head_channels=-1,\n        num_heads_upsample=-1,\n        use_scale_shift_norm=False,\n        resblock_updown=False,\n        use_new_attention_order=False,\n        use_spatial_transformer=False,    # custom transformer support\n        transformer_depth=1,              # custom transformer support\n        context_dim=None,                 # custom transformer support\n        n_embed=None,                     # custom support for prediction of discrete ids into codebook of first stage vq model\n        legacy=True,\n        disable_self_attentions=None,\n        num_attention_blocks=None,\n        disable_middle_self_attn=False,\n        use_linear_in_transformer=False,\n        semb_channels=None\n    ):\n        super().__init__()\n        if use_spatial_transformer:\n            assert context_dim is not None, &#039;Fool!! You forgot to include the dimension of your cross-attention conditioning...&#039;\n \n        if context_dim is not None:\n            assert use_spatial_transformer, &#039;Fool!! You forgot to use the spatial transformer for your cross-attention conditioning...&#039;\n            from omegaconf.listconfig import ListConfig\n            if type(context_dim) == ListConfig:\n                context_dim = list(context_dim)\n \n        if num_heads_upsample == -1:\n            num_heads_upsample = num_heads\n \n        if num_heads == -1:\n            assert num_head_channels != -1, &#039;Either num_heads or num_head_channels has to be set&#039;\n \n        if num_head_channels == -1:\n            assert num_heads != -1, &#039;Either num_heads or num_head_channels has to be set&#039;\n \n        self.image_size = image_size\n        self.in_channels = in_channels\n        self.model_channels = model_channels\n        self.out_channels = out_channels\n        if isinstance(num_res_blocks, int): # [[#residual-block|residual block]]\n            self.num_res_blocks = len(channel_mult) * [num_res_blocks]\n        else:\n            if len(num_res_blocks) != len(channel_mult):\n                raise ValueError(&quot;provide num_res_blocks either as an int (globally constant) or &quot;\n                                 &quot;as a list/tuple (per-level) with the same length as channel_mult&quot;)\n            self.num_res_blocks = num_res_blocks\n        if disable_self_attentions is not None: # [[#attention-block|attention block]]\n            # should be a list of booleans, indicating whether to disable self-attention in TransformerBlocks or not\n            assert len(disable_self_attentions) == len(channel_mult)\n        if num_attention_blocks is not None:\n            assert len(num_attention_blocks) == len(self.num_res_blocks)\n            assert all(map(lambda i: self.num_res_blocks[i] &gt;= num_attention_blocks[i], range(len(num_attention_blocks))))\n            print(f&quot;Constructor of UNetModel received num_attention_blocks={num_attention_blocks}. &quot;\n                  f&quot;This option has LESS priority than attention_resolutions {attention_resolutions}, &quot;\n                  f&quot;i.e., in cases where num_attention_blocks[i] &gt; 0 but 2**i not in attention_resolutions, &quot;\n                  f&quot;attention will still not be set.&quot;)\n\t\t# [[#unet-关键参数配置|UNet 关键参数配置]] \n        self.attention_resolutions = attention_resolutions\n        self.dropout = dropout\n        self.channel_mult = channel_mult\n        self.conv_resample = conv_resample\n        self.num_classes = num_classes\n        self.use_checkpoint = use_checkpoint\n        self.dtype = th.float16 if use_fp16 else th.float32\n        self.num_heads = num_heads\n        self.num_head_channels = num_head_channels\n        self.num_heads_upsample = num_heads_upsample\n        self.predict_codebook_ids = n_embed is not None\n\t\t# [[#time_embed|time_embed]]\n        time_embed_dim = model_channels * 4\n        self.time_embed = nn.Sequential(\n            linear(model_channels, time_embed_dim),\n            nn.SiLU(),\n            linear(time_embed_dim, time_embed_dim),\n        )\n\t\t# [[#类别条件class-conditional机制在-unet-中的实现|类别条件（Class-Conditional）机制在 UNet 中的实现]]\n        if self.num_classes is not None:\n            if isinstance(self.num_classes, int):\n                self.label_emb = nn.Embedding(num_classes, time_embed_dim)\n            elif self.num_classes == &quot;continuous&quot;:\n                print(&quot;setting up linear c_adm embedding layer&quot;)\n                self.label_emb = nn.Linear(1, time_embed_dim)\n            else:\n                raise ValueError()\n\t\t# [[#input_block|input_block]]\n        self.input_blocks = nn.ModuleList(\n            [\n                TimestepEmbedSequential(\n                    conv_nd(dims, in_channels, model_channels, 3, padding=1)\n                )\n            ]\n        )\n        self._feature_size = model_channels\n        input_block_chans = [model_channels]\n        ch = model_channels\n        ds = 1\n        for level, mult in enumerate(channel_mult):\n            for nr in range(self.num_res_blocks[level]):\n                layers = [\n                    ResBlockDual(\n                        ch,\n                        time_embed_dim,\n                        dropout,\n                        semb_channels=semb_channels,\n                        out_channels=mult * model_channels,\n                        dims=dims,\n                        use_checkpoint=use_checkpoint,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                    )\n                ]\n                ch = mult * model_channels\n                if ds in attention_resolutions:\n                    if num_head_channels == -1:\n                        dim_head = ch // num_heads\n                    else:\n                        num_heads = ch // num_head_channels\n                        dim_head = num_head_channels\n                    if legacy:\n                        #num_heads = 1\n                        dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n                    if exists(disable_self_attentions):\n                        disabled_sa = disable_self_attentions[level]\n                    else:\n                        disabled_sa = False\n \n                    if not exists(num_attention_blocks) or nr &lt; num_attention_blocks[level]:\n                        layers.append(\n                            AttentionBlock(\n                                ch,\n                                use_checkpoint=use_checkpoint,\n                                num_heads=num_heads,\n                                num_head_channels=dim_head,\n                                use_new_attention_order=use_new_attention_order,\n                            ) if not use_spatial_transformer else SpatialTransformerV2(\n                                ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim,\n                                disable_self_attn=disabled_sa, use_linear=use_linear_in_transformer,\n                                use_checkpoint=use_checkpoint\n                            )\n                        )\n                self.input_blocks.append(TimestepEmbedSequential(*layers))\n                self._feature_size += ch\n                input_block_chans.append(ch)\n            if level != len(channel_mult) - 1:\n                out_ch = ch\n                self.input_blocks.append(\n                    TimestepEmbedSequential(\n                        ResBlockDual(\n                            ch,\n                            time_embed_dim,\n                            dropout,\n                            semb_channels=semb_channels,\n                            out_channels=out_ch,\n                            dims=dims,\n                            use_checkpoint=use_checkpoint,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            down=True,\n                        )\n                        if resblock_updown\n                        else Downsample(\n                            ch, conv_resample, dims=dims, out_channels=out_ch\n                        )\n                    )\n                )\n                ch = out_ch\n                input_block_chans.append(ch)\n                ds *= 2\n                self._feature_size += ch\n\t\t# [[#attention_head|attention_head]] \n        if num_head_channels == -1:\n            dim_head = ch // num_heads\n        else:\n            num_heads = ch // num_head_channels\n            dim_head = num_head_channels\n        if legacy:\n            #num_heads = 1\n            dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n        self.middle_block = TimestepEmbedSequential(\n            ResBlockDual(\n                ch,\n                time_embed_dim,\n                dropout,\n                semb_channels=semb_channels,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n            ),\n            AttentionBlock(\n                ch,\n                use_checkpoint=use_checkpoint,\n                num_heads=num_heads,\n                num_head_channels=dim_head,\n                use_new_attention_order=use_new_attention_order,\n            ) if not use_spatial_transformer else SpatialTransformerV2(  # always uses a self-attn\n                            ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim,\n                            disable_self_attn=disable_middle_self_attn, use_linear=use_linear_in_transformer,\n                            use_checkpoint=use_checkpoint\n                        ),\n            ResBlockDual(\n                ch,\n                time_embed_dim,\n                dropout,\n                semb_channels=semb_channels,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n            ),\n        )\n        self._feature_size += ch\n \n        self.output_blocks = nn.ModuleList([])\n        for level, mult in list(enumerate(channel_mult))[::-1]:\n            for i in range(self.num_res_blocks[level] + 1):\n                ich = input_block_chans.pop()\n                layers = [\n                    ResBlockDual(\n                        ch + ich,\n                        time_embed_dim,\n                        dropout,\n                        semb_channels=semb_channels,\n                        out_channels=model_channels * mult,\n                        dims=dims,\n                        use_checkpoint=use_checkpoint,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                    )\n                ]\n                ch = model_channels * mult\n                if ds in attention_resolutions:\n                    if num_head_channels == -1:\n                        dim_head = ch // num_heads\n                    else:\n                        num_heads = ch // num_head_channels\n                        dim_head = num_head_channels\n                    if legacy:\n                        #num_heads = 1\n                        dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n                    if exists(disable_self_attentions):\n                        disabled_sa = disable_self_attentions[level]\n                    else:\n                        disabled_sa = False\n \n                    if not exists(num_attention_blocks) or i &lt; num_attention_blocks[level]:\n                        layers.append(\n                            AttentionBlock(\n                                ch,\n                                use_checkpoint=use_checkpoint,\n                                num_heads=num_heads_upsample,\n                                num_head_channels=dim_head,\n                                use_new_attention_order=use_new_attention_order,\n                            ) if not use_spatial_transformer else SpatialTransformerV2(\n                                ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim,\n                                disable_self_attn=disabled_sa, use_linear=use_linear_in_transformer,\n                                use_checkpoint=use_checkpoint\n                            )\n                        )\n                if level and i == self.num_res_blocks[level]:\n                    out_ch = ch\n                    layers.append(\n                        ResBlockDual(\n                            ch,\n                            time_embed_dim,\n                            dropout,\n                            semb_channels=semb_channels,\n                            out_channels=out_ch,\n                            dims=dims,\n                            use_checkpoint=use_checkpoint,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            up=True,\n                        )\n                        if resblock_updown\n                        else Upsample(ch, conv_resample, dims=dims, out_channels=out_ch)\n                    )\n                    ds //= 2\n                self.output_blocks.append(TimestepEmbedSequential(*layers))\n                self._feature_size += ch\n \n        self.out = nn.Sequential(\n            normalization(ch),\n            nn.SiLU(),\n            zero_module(conv_nd(dims, model_channels, out_channels, 3, padding=1)),\n        )\n        if self.predict_codebook_ids:\n            self.id_predictor = nn.Sequential(\n            normalization(ch),\n            conv_nd(dims, model_channels, n_embed, 1),\n            #nn.LogSoftmax(dim=1)  # change to cross_entropy and produce non-normalized logits\n        )\nattention_head\n在使用多头注意力模块（如 AttentionBlock 或 SpatialTransformerV2）时，需要指定下列两个参数之一：\n\nnum_heads：注意力头的数量（例如 8 表示使用 8 个并行注意力分支）\nnum_head_channels：每个注意力头的通道宽度（例如 64 表示每个头维度为 64）\n\n二者的关系\n二者满足如下关系：\n\\mathrm{num\\_heads} \\times \\mathrm{num\\_head\\_channels} \\leq \\mathrm{total\\_channels}\n你只需要显式指定一个，另一个可以自动推导。\n参数检查逻辑\n源代码中的校验逻辑如下：\nif num_heads == -1:\n    assert num_head_channels != -1, &#039;Either num_heads or num_head_channels has to be set&#039;\n \nif num_head_channels == -1:\n    assert num_heads != -1, &#039;Either num_heads or num_head_channels has to be set&#039;\n也就是说，必须至少指定一个参数，否则将抛出错误。\n️ 注意事项\n\n若 total_channels 无法整除指定参数，可能导致计算出错或维度不一致；\n两者都指定时要确保一致性，即：num_heads * num_head_channels == total_channels；\n推荐做法是：设置你想控制的那一个，留另一个自动计算。\n\n示例\n假设当前层通道数为 320：\n\n若设置 num_heads=8，则 num_head_channels=320 // 8 = 40\n若设置 num_head_channels=64，则 num_heads=320 // 64 = 5\n\n# 推荐示例\nattention_block = AttentionBlock(\n    channels=320,\n    num_heads=8,\n    num_head_channels=-1,  # 自动计算为 40\n)\n \n# 或者\nattention_block = AttentionBlock(\n    channels=320,\n    num_heads=-1,\n    num_head_channels=64,  # 自动计算为 5 heads\n)\nresidual block\n这段代码用于配置UNet的各层中残差块（residual block）的数量。\n        if isinstance(num_res_blocks, int):\n            self.num_res_blocks = len(channel_mult) * [num_res_blocks]\n如果传入的num_res_blocks参数是单一整数，那么每一层将都使用这个数量的残差块。\n        else:\n            if len(num_res_blocks) != len(channel_mult):\n                raise ValueError(&quot;provide num_res_blocks either as an int (globally constant) or &quot;\n                                 &quot;as a list/tuple (per-level) with the same length as channel_mult&quot;)\n            self.num_res_blocks = num_res_blocks\n如果传入的num_res_blocks参数不是整数，那么期望为列表或其他包含整数序列的有序容器，且该容器的长度应和channel_mult的长度（也即UNet的单测深度）相同。\nattention block\n这段代码配置了attention block（自注意力机制）部分的启用情况。\n注意：在StableSR中，自注意力和交叉注意力都是通过统一的transformer模块实现的，其自注意力机制并不是SR3那样传统的dot-product attention。StableSR中，该transformer可以选择是否开启自注意力部分，但默认必须开启交叉注意力以为UNet提供必要的生成条件。\n        if disable_self_attentions is not None:\n            # should be a list of booleans, indicating whether to disable self-attention in TransformerBlocks or not\n            assert len(disable_self_attentions) == len(channel_mult)\ndisable_self_attentions参数期望是None或者与channel_mult长度相同的布尔数组，用于控制UNet内各层是否开启自注意力模块。None表示全部开启，布尔数组则按照其真值控制。\n        if num_attention_blocks is not None:\n            assert len(num_attention_blocks) == len(self.num_res_blocks)\n            assert all(map(lambda i: self.num_res_blocks[i] &gt;= num_attention_blocks[i], range(len(num_attention_blocks))))\n            print(f&quot;Constructor of UNetModel received num_attention_blocks={num_attention_blocks}. &quot;\n                  f&quot;This option has LESS priority than attention_resolutions {attention_resolutions}, &quot;\n                  f&quot;i.e., in cases where num_attention_blocks[i] &gt; 0 but 2**i not in attention_resolutions, &quot;\n                  f&quot;attention will still not be set.&quot;)\n\n\n控制 每一层中最多能插入多少个 attention 模块；\n\n\n不是层级数，而是实际 ResBlock 数量对 attention 的数量限制；\n\n\n必须匹配结构深度，即 num_attention_blocks[i] ≤ num_res_blocks[i]。\n\n\n打印语句更加清晰地表明了配置参数的优先级关系\n\n\n最终是否插入 attention block 的判定，首先看 attention_resolutions，然后才看 num_attention_blocks。\n\n\n也就是说：\n\n\n你可以设置 num_attention_blocks[i] = 1 表示“最多插入 1 个”；\n\n\n但如果 2**i 不在 attention_resolutions（例如 16, 32）中，那 attention 就不会插入；\n\n\n换句话说：你只表达了“允许插入”，插不插要由 attention_resolutions 决定。\n\n\n\n\nUNet 关键参数配置\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n参数说明attention_resolutions控制在哪些分辨率下插入 attention（如 16、32）dropoutdropout 概率，用于 regularizationchannel_mult每一层通道数是基通道数（model_channels）的几倍conv_resample是否使用卷积实现下/上采样（True 为可学习）num_classes是否使用 class-conditional 条件（如标签）use_checkpoint是否使用 gradient checkpointing 减少显存dtype网络中使用的精度（float16 或 float32）num_heads / num_head_channelsAttention 中的 head 设置num_heads_upsample上采样阶段的 head 数predict_codebook_ids是否输出 codebook token（即是否为 VQGAN decoder）\ntime_embed\n注意：这是将时间信息引入UNet的核心途径。\n        time_embed_dim = model_channels * 4\n        self.time_embed = nn.Sequential(\n            linear(model_channels, time_embed_dim),\n            nn.SiLU(),\n            linear(time_embed_dim, time_embed_dim),\n        )\n这段代码构造了一个简单的 MLP（多层感知机），将原始的 timestep embedding 映射成供 ResBlock 使用的时间条件向量。\n\n输入维度：model_channels（例如 320）\n输出维度：time_embed_dim = 4 × model_channels（例如 1280）\n激活函数：SiLU（即 Swish 激活，效果比 ReLU 更平滑）\n\nTODO:time_embed只是time aware encoder的一个模块，在进入他之前会有一个原始的time embeding（可能是正余弦位置编码），将编码结果输入这个time_embed的MLP来进一步添加可学习性，其输出用于调节残差块的工作模式。以上内容应该在后续代码和forward函数中有更多的体现。\n类别条件（Class-Conditional）机制在 UNet 中的实现\n一、什么是类别条件？\n在扩散模型（如 DDPM、StableSR、LDM）中，类别条件是一种用于引导图像生成方向的辅助信息。通过在每一步扩散中注入类别标签，模型能够学习：\n\n如何在第 t 步生成属于类别 y 的图像特征。\n\n该机制适用于分类引导图像生成，例如：\n\n生成一张“猫”而不是“狗”的图像\n合成特定类型的建筑、车辆或自然场景图像\n条件控制任务，如 super-resolution with category hints\n\n\n二、实现方式\n在 UNetModelDualcondV2 中，类别条件通过以下结构实现：\nif isinstance(self.num_classes, int):\n    self.label_emb = nn.Embedding(num_classes, time_embed_dim)\nelif self.num_classes == &quot;continuous&quot;:\n    self.label_emb = nn.Linear(1, time_embed_dim)\n支持两种类别条件格式：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n类型含义模块输入形式离散类别明确类别标签，如 0~9nn.Embeddingy ∈ {0, ..., num_classes}连续变量属性值、实数条件nn.Lineary ∈ ℝ（形如 [B, 1]）\n嵌入后将与时间嵌入相加，用于调节每个 ResBlock：\nemb = time_embed(t) + label_emb(y)\n\n三、类别条件的来源\n类别条件 不是自动生成的，而是 由用户显式提供：\n\n对于分类任务：标签 y 来自数据集；\n对于连续条件任务：如模糊程度、温度等数值，由用户设定；\n不使用类别条件时，设置 num_classes = None 即可关闭。\n\n\n四、关闭类别条件的方法\n只需在初始化模型时设定：\nnum_classes = None\n即可完全关闭类别条件路径：\n\n不构建 label_emb\n时间嵌入 emb = time_embed(t) 不再包含类别信息\n模型仅依赖时间步与其他条件（如结构条件）\n\n\n五、技术意义\n类别条件扩展了扩散模型的能力，使其支持：\n\n分类控制生成（class-conditional generation）\n多模态控制结构（如 time + class + structure）\n通用控制器设计（支持标签、模态、风格等注入）\n\n这一机制也为后续加入 cross-attention 等结构提供了条件输入的通道。\ninput_block\n        self.input_blocks = nn.ModuleList(\n            [\n                TimestepEmbedSequential(\n                    conv_nd(dims, in_channels, model_channels, 3, padding=1)\n                )\n            ]\n        )\nTimestepEmbedSequential类继承自nn.Sequential和TimestepBlock，自身没有实现init函数，故实例化是调用的是nn.Sequential的init函数。TimestepBlock共有三种，都是nn.Module的子类构建的abc,调用TimestepEmbedSequential实例时，根据传入的TimestepBlock类型判断forward函数的执行逻辑。\ninput_blocks的第一层是固定的二维卷积，后面还会给这个nn.ModuleList中append其他被TimestepEmbedSequential包裹的ResidualBlock,AttentionBlock"},"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/attribute":{"slug":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/attribute","filePath":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class UNetModelDualcondV2/attribute.md","title":"attribute","links":[],"tags":[],"content":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n参数作用与说明image_size输入图像的大小，用于构建 U-Net 结构的层级，一般用于输入前的尺寸校验或推断网络深度。in_channels输入图像的通道数，例如 RGB 为 3，灰度图为 1。model_channels模型的基础通道数，决定了第一层特征图的宽度。U-Net 的通道数通常是 model_channels * mult 形式增长。out_channels输出图像的通道数，通常与 in_channels 相同，例如 3 通道图像。也可以是 codebook 大小（用于 VQ）。num_res_blocks每个 downsample/upsample 层使用的 ResBlock 个数。可为 int（每层相同），也可为 list（不同层不同数量）。attention_resolutions指定在哪些 resolution 下插入 attention 模块。例如包含 4 表示在 1/4 尺度特征图上加入 self-attn 或 cross-attn。dropoutDropout 概率，控制网络的随机失活程度。用于提升泛化能力。channel_multU-Net 每个层级的通道扩展倍率。例如 (1, 2, 4, 8) 表示第4层为基通道的8倍。conv_resample是否使用卷积方式进行上下采样（如 PixelShuffle、ConvTranspose），否则使用简单插值。dims输入数据维度，2 表示 2D 图像，1 或 3 则用于序列或体数据。num_classes分类条件数量。如果设置为整数，则模型是 class-conditional；支持 int 或 &quot;continuous&quot;。use_checkpoint是否开启 gradient checkpointing（反向传播时节省内存）。会牺牲速度换空间。use_fp16是否使用 float16 精度推理，主要用于减少显存。num_heads注意力机制中 head 的数量，如果为 -1，则根据 num_head_channels 自动计算。num_head_channels每个 attention head 的维度，优先级高于 num_heads。二者需至少设置一个。num_heads_upsample用于上采样阶段的 attention head 数，默认与 num_heads 一致。use_scale_shift_norm是否在 ResBlock 中使用 FiLM 风格的 scale-shift 归一化（用于条件建模）。resblock_updown是否使用残差块进行上/下采样，否则使用 Downsample/Upsample 模块。use_new_attention_order控制 attention 顺序的实验性设置。默认为 False。use_spatial_transformer是否使用 Transformer 替代原生 AttentionBlock。开启后需设置 context_dim。transformer_depthTransformer 模块的堆叠深度（层数）。context_dimCross-Attention 中 context（如文本、图像编码）的维度，必须与 transformer 配套。n_embed如果不为 None，表示该模型用于预测 codebook 的离散 token（如 VQ-VAE / VQGAN 场景）。legacy控制是否使用 legacy 的 attention 维度设置逻辑。True 表示保持旧实现兼容性。disable_self_attentions是否禁用某些层中的 self-attn。为一个布尔列表，长度等于 channel_mult。num_attention_blocks控制每个层的 attention block 数量。优先级低于 attention_resolutions。disable_middle_self_attn是否禁用 U-Net 最底部（中间层）的 self-attn。use_linear_in_transformer是否在 transformer 中使用 Linear 形式的投影层。semb_channels结构条件通道数（例如结构图、小波子带等），会传入 ResBlockDual 进行时间/结构融合。\n与 Time-Aware Encoder 的关系\n作为 Time-Aware Encoder，这些参数中以下几项起关键作用：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n参数Time-Aware Encoder 中的意义t_emb（隐含于 time_embed）为每个时刻 t 生成时间嵌入，与结构一起参与生成（类似时间条件扩散）semb_channels支持结构条件的通道输入（如小波子带、深度图、边缘图等），通过 ResBlockDual 融合进入主干网络context_dim + use_spatial_transformer实现 Cross-Attention，用于将文本/图像上下文信息融合进特征流use_checkpoint + use_fp16控制训练时的计算效率，适合大规模时间序列训练"},"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/convert_to_fp16":{"slug":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/convert_to_fp16","filePath":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class UNetModelDualcondV2/convert_to_fp16.md","title":"convert_to_fp16","links":[],"tags":[],"content":""},"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/convert_to_fp32":{"slug":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/convert_to_fp32","filePath":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class UNetModelDualcondV2/convert_to_fp32.md","title":"convert_to_fp32","links":[],"tags":[],"content":""},"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/forward":{"slug":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class-UNetModelDualcondV2/forward","filePath":"StableSR_doc/ldm/modules/diffusionmodules/openaimodel/class UNetModelDualcondV2/forward.md","title":"forward","links":[],"tags":[],"content":""},"StableSR_doc/ldm/utils/Module_info":{"slug":"StableSR_doc/ldm/utils/Module_info","filePath":"StableSR_doc/ldm/utils/Module_info.md","title":"Module_info","links":["StableSR_doc/ldm/utils/get_obj_from_str","StableSR_doc/ldm/utils/instantiate_from_config"],"tags":[],"content":"Functions\n\nget_obj_from_str\ninstantiate_from_config\n"},"StableSR_doc/ldm/utils/get_obj_from_str":{"slug":"StableSR_doc/ldm/utils/get_obj_from_str","filePath":"StableSR_doc/ldm/utils/get_obj_from_str.md","title":"get_obj_from_str","links":[],"tags":[],"content":"def get_obj_from_str(string, reload=False):\n    module, cls = string.rsplit(&quot;.&quot;, 1)\n    if reload:\n        module_imp = importlib.import_module(module)\n        importlib.reload(module_imp)\n    return getattr(importlib.import_module(module, package=None), cls)"},"StableSR_doc/ldm/utils/instantiate_from_config":{"slug":"StableSR_doc/ldm/utils/instantiate_from_config","filePath":"StableSR_doc/ldm/utils/instantiate_from_config.md","title":"instantiate_from_config","links":["StableSR_doc/ldm/utils/get_obj_from_str"],"tags":[],"content":"def instantiate_from_config(config):\n    if not &quot;target&quot; in config:\n        if config == &#039;__is_first_stage__&#039;:\n            return None\n        elif config == &quot;__is_unconditional__&quot;:\n            return None\n        raise KeyError(&quot;Expected key `target` to instantiate.&quot;)\n    return get_obj_from_str(config[&quot;target&quot;])(**config.get(&quot;params&quot;, dict()))\n \n作用\n读取config配置文件,按照其指定规则将target指定的类实例化.同时在第2行if块中给出了不进行实例化的判断逻辑,可以利用它来控制开关某些模块(如注意力机制和结构条件).\nconfig 配置要求\n应该使用yaml文件,包含target项用于指定类名,params项及其子项用于指定实例化时的模型参数.\n具体的实例化逻辑和过程\nget_obj_from_str 返回一个类对象,在get_obj_from_str(…)后面的 (**config.get)(&quot;params&quot;,dict())相当于将params作为参数传入到前面的类对象的__init__中,最终完成实例化."},"StableSR_doc/pytorch-lightning":{"slug":"StableSR_doc/pytorch-lightning","filePath":"StableSR_doc/pytorch lightning.md","title":"pytorch lightning","links":[],"tags":[],"content":"pytorch lighting模块简介\npl.LightningMoudule\npl.LightningModule 是对 torch.nn.Module 的高级封装\n只需要实现下面几个关键方法，它就能帮你自动完成训练流程、GPU 分发、日志记录、checkpoint保存等复杂操作：\n\n\n\n__init__(self): 初始化模型结构、损失函数、超参数等。\n\n\n\n\nforward(self, x): 定义前向传播逻辑（注意：仅在调用 model(x) 时使用，训练逻辑用 training_step）。\n\n\n\n\ntraining_step(self, batch, batch_idx):定义一个训练步骤的行为，返回 loss。\n\n\n通常的步骤是\n\n从 batch 取出数据；\n前向传播；\n计算损失；\n使用 self.log(...) 自动记录日志（如 loss）。\n\n\n\n\n\n\nvalidation_step(...) / test_step(...):验证和测试阶段的行为，结构与 training_step 类似。\n\n\n\n\nconfigure_optimizers(self):返回优化器和（可选的）学习率调度器。\n\n\n\npytorch_Lightning Callback 机制\nPyTorch Lightning 的 Callback（回调机制）是训练过程中的事件钩子系统，允许用户在训练 / 验证 / 测试 / 保存等阶段插入自定义逻辑，类似于钩子（hook）或监听器。\n\n✅ 常见用途\n\n自动保存最佳模型（如根据 val/loss 最小）\nEarly stopping（提前停止训练）\n日志记录 / 学习率可视化\n自定义日志、评估、样本可视化等行为\n\n\n🧩 核心概念：Callback 是一个类\nfrom pytorch_lightning.callbacks import Callback\n \nclass MyCallback(Callback):\n    def on_train_start(self, trainer, pl_module):\n        print(&quot;训练开始！&quot;)\n    \n    def on_validation_end(self, trainer, pl_module):\n        print(&quot;验证阶段结束。&quot;)\n \n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n        print(f&quot;训练第 {batch_idx} 个 batch 完成&quot;)\n\n🧪 使用 Callback 的方式\nfrom pytorch_lightning import Trainer\n \ntrainer = Trainer(callbacks=[MyCallback()])\n可以一次性注册多个 callback：\ntrainer = Trainer(callbacks=[\n    MyCallback(),\n    ModelCheckpoint(...),\n    EarlyStopping(...)\n])\n\n📦 官方内置常用回调\n1. ModelCheckpoint: 保存最优模型\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n \ncheckpoint_callback = ModelCheckpoint(\n    monitor=&quot;val/loss&quot;,     # 监控哪个指标\n    save_top_k=1,           # 只保留 top1\n    mode=&quot;min&quot;,             # 目标是最小化 loss\n    filename=&quot;best-checkpoint&quot;,\n    save_last=True\n)\n2. EarlyStopping: 提前停止\nfrom pytorch_lightning.callbacks import EarlyStopping\n \nearly_stop_callback = EarlyStopping(\n    monitor=&quot;val/loss&quot;,\n    patience=5,     # 若 5 个 epoch 没有提升则停止\n    mode=&quot;min&quot;\n)\n\n📚 常用 Callback 钩子方法一览\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n方法名调用时机on_fit_startfit() 开始时on_train_start训练阶段开始on_train_end训练阶段结束on_train_batch_start每个 batch 开始前on_train_batch_end每个 batch 结束后on_validation_end每轮验证结束后on_save_checkpoint保存 checkpoint 时on_load_checkpoint加载 checkpoint 时\n\n🎯 实例：在验证后记录当前模型状态\nclass LogModelNorm(Callback):\n    def on_validation_end(self, trainer, pl_module):\n        total_norm = 0\n        for p in pl_module.parameters():\n            if p.grad is not None:\n                param_norm = p.grad.data.norm(2)\n                total_norm += param_norm.item() ** 2\n        total_norm = total_norm ** 0.5\n        print(f&quot;当前梯度范数：{total_norm:.4f}&quot;)\n\n✅ 小结\n\nCallback 是一种轻量级的插件系统；\n所有模型相关回调都可以集中管理，不污染核心训练逻辑；\n非常适合记录日志、保存中间结果、动态修改训练行为等。\n"},"StableSR_doc/trick-pool":{"slug":"StableSR_doc/trick-pool","filePath":"StableSR_doc/trick pool.md","title":"trick pool","links":[],"tags":[],"content":"UNet相关\nSPADE\n在UNet的残差块中引入SPADE层来替代传统的batch norm等归一化操作，保留空间结构语义信息；使用小波图像作为struct_cond引导SPADE。UNet中各层都需要引入struct_cond,于是需要构造一种方法，把小波图变换成各个层所需要的形状，同时要尽可能保持小波图的语义信息，尝试以下方法：\n\nBICUBIC下采样\n\n纯CNN\nBICUBIC下采样+CNN残差块\n\n\n\nCross Attention\nUNet中引入交叉注意力机制,利用小波图、DGConv图、膨胀卷积等各类特征图的latent表示作为Key,利用UNet当前层的latent图的对应特征图作为Query.\n\n使用小波、DGConv、膨胀卷积等特征图的latent表示作为Value,然后与UNet主干内上的原图latent表示融合，然后作为下一UNet层的输入，这样可能因为不同域信息的叠加导致语义混乱。\n\n尝试以下融合方法：\n\n残差融合\n门控融合\n卷积\n\n\n\n\n使用原图的latent表示作为Value,小波、DGConv、膨胀卷积等特征图latent表示仅起到引导作用。直接将Value的softmax组合输入到下一层UNet网络，问题在于，Value并不来自UNet,容易丢失上层UNet的信息，考虑以下方法。\n\n将交叉注意力机制插入到残差块，Value的 softmax(QK^T) 组合作为残差块中的残差层，最终仍和UNet主干网络上的latent原图做残差连接\n\n\n\n问题：\nCross Attention的主要问题是UNet图的Querry如何与小波等特征图Key对齐，\n\n在UNet接受单通道输入的情况下，UNet的任何一层都是latent图，无法直接对latent图做小波变换等特征提取，无法与Key在域的层面与Key对齐。\n输入网络之前，先对原始图像做好特征提取，将原图和小波等特征图作为一张图输入VAE和UNet，但经过的一层UNet之后这些通道就都被打散，无法仅使用原先的特征图通道的信息生成Querry；另外，多通道输入意味着UNet需要对多通道、不同域的图像超分辨，六月底在SR3架构上尝试这一方法时，发生psnr极高、损失函数震荡且生成图片质量极差的问题，可能是因为不同域的信息混合导致UNet学习失败。\n\nPS.这个“对齐”的问题可能本身就不是一个问题，在文生图等交叉注意力场景下，使用的方法就是将UNet主干图reshape为一个[B,H*W,C]的张量，token embeder也将文字token编码为一个相同形状张量，然后K和V都是这个embeder的返回值，没有做信息类别上的对齐。\n数据处理相关\n一般超分模型中，以高清图像作为标签，下采样降质后的低清图片作为超分模型的输入，其下采样方法往往是BICUBIC插值、jpeg压缩等通用方法。针对SAR我们计划定制新的符合SAR光学特性的图像退化方式：\n\n雷达PSF\n低通滤波\n乘性噪声\n\nVAE相关\n将DGConv特征图作为残差块添加到VAE中，与原图像做残差连接，然后再进行编码。"},"learn_c/基本语法/data-type":{"slug":"learn_c/基本语法/data-type","filePath":"learn_c/基本语法/data type.md","title":"data type","links":[],"tags":[],"content":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数据类型 (关键字)常见定义/声明方式占位符 (printf/scanf)说明charchar c = &#039;A&#039;;%c字符类型（实际上存储整数，对应 ASCII 码）。通常 1 字节 (8 位)。signed charsigned char sc = -10;%c明确带符号的字符，范围一般是 -128 ~ 127。unsigned charunsigned char uc = 250;%c 或 %hhu无符号字符，范围 0 ~ 255。intint x = 42;%d 或 %i整数，通常 4 字节，范围约 -2,147,483,648 ~ 2,147,483,647。short / short intshort s = 100;%hd短整型，一般 2 字节。unsigned shortunsigned short us = 60000;%hu无符号短整型，范围约 0 ~ 65535。long / long intlong l = 123456;%ld长整型，一般 4 或 8 字节（取决于系统）。unsigned longunsigned long ul = 4000000000UL;%lu无符号长整型。long long / long long intlong long ll = 123456789LL;%lld更大的整数，一般 8 字节。unsigned long longunsigned long long ull = 18446744073709551615ULL;%llu最大的标准无符号整数类型。floatfloat f = 3.14f;%f单精度浮点数，约 6~7 位有效数字。doubledouble d = 3.141592653;%lf双精度浮点数，约 15~16 位有效数字。long doublelong double ld = 3.141592653589793L;%Lf扩展精度浮点，精度比 double 高（平台依赖）。_Bool (C99)_Bool flag = 1;%d只能是 0 或 1。需要 &lt;stdbool.h&gt; 头文件时可写作 bool。voidvoid func(void);无表示“无类型”，用于函数返回类型或指针 (void*)。"},"learn_c/基本语法/宏macro":{"slug":"learn_c/基本语法/宏macro","filePath":"learn_c/基本语法/宏macro.md","title":"宏macro","links":[],"tags":[],"content":"在C语言中，宏是通过预处理器进行文本替换的一种功能。宏通常用于简化代码，增加可读性，或者在编译时执行一些条件编译操作。宏在编译过程的预处理阶段展开，它们不会在运行时消耗资源。\n宏的种类\n\n\n对象宏（Object-like Macros）\n对象宏类似于常量的替代，通常用于常量定义。它是直接用一个名称替代某个值或表达式。\n示例：\n#define PI 3.14159\n#define MAX_BUFFER_SIZE 1024\n这样在代码中，PI 会被替换成 3.14159，MAX_BUFFER_SIZE 会被替换成 1024。\n\n\n函数宏（Function-like Macros）\n函数宏类似于函数，但它在预处理阶段被直接替换成对应的代码。函数宏允许带参数，宏展开时会将参数替换到宏定义的位置。\n示例：\n#define SQUARE(x) ((x) * (x))\n#define MAX(a, b) ((a) &gt; (b) ? (a) : (b))\n这样，在代码中，SQUARE(4) 会被展开成 ((4) * (4))，MAX(3, 5) 会被展开成 ((3) &gt; (5) ? (3) : (5))。\n注意：\n\n\n在函数宏中，参数应加上括号，以确保表达式的优先级正确。\n\n\n宏展开时没有类型检查，因此可能会导致一些潜在的错误，比如宏参数中有副作用。\n\n\n\n\n条件编译宏（Conditional Compilation）\n宏也可以用来控制代码的编译条件。通过条件编译，可以根据不同的环境或配置来决定是否编译某些代码块。\n示例：\n#ifdef DEBUG\n    printf(&quot;Debugging enabled\\n&quot;);\n#else\n    printf(&quot;Debugging disabled\\n&quot;);\n#endif\n#ifdef 用来检查是否定义了 DEBUG，如果定义了 DEBUG，那么对应的代码块会被编译，否则跳过。\n\n\n#define与#undef\n\n\n#define 用于定义宏。\n\n\n#undef 用于取消已定义的宏。\n\n\n示例：\n#define DEBUG\n#undef DEBUG\n\n\n宏替换与副作用问题\n由于宏是简单的文本替换，在使用函数宏时，要小心参数中的副作用。比如：\n#define SQUARE(x) ((x) * (x))\nint a = 2;\nint result = SQUARE(a++);\n在这个例子中，a++ 会被展开成 ((a++) * (a++))，这可能会导致意外的副作用（例如 a 的值变得不符合预期）。\n\n\n宏的优缺点\n优点：\n\n\n性能提升：宏替换在编译时进行，因此在运行时没有额外的开销。\n\n\n代码简洁：可以使用宏避免重复编写相同的代码。\n\n\n条件编译：宏可以帮助在不同平台或者配置中编译不同的代码。\n\n\n缺点：\n\n\n缺乏类型安全：宏只是简单的文本替换，没有类型检查，容易产生错误。\n\n\n调试困难：调试宏展开后的代码不如函数直观，因为它们在编译时展开，调试时无法看到展开后的完整代码。\n\n\n副作用问题：宏的参数会多次计算，可能导致副作用，尤其在使用带有副作用的表达式时。\n\n\n总结来说，宏是C语言中非常强大的功能，但在使用时要小心处理副作用和调试问题。如果可能，应该尽量使用 const 或 inline 函数来替代宏，避免一些潜在的错误。"},"learn_c/基本语法/局部变量、全局变量与静态变量":{"slug":"learn_c/基本语法/局部变量、全局变量与静态变量","filePath":"learn_c/基本语法/局部变量、全局变量与静态变量.md","title":"局部变量、全局变量与静态变量","links":[],"tags":[],"content":"局部变量\n\n\n作用域：\n\n局部变量的作用域仅限于它所在的函数、if、for 或 while 等代码块内部。在代码块外部无法访问该变量。\n例如，for 循环中声明的变量只在循环内部有效。\n\n\n\n生存期：\n\n局部变量的生存期从它们被声明时开始，直到代码块执行完毕为止。它们通常保存在栈上，因此当程序离开作用域时，局部变量会被销毁。\n\n示例：\n\n\nvoid func() {\n  int x = 10;  // 局部变量 x\n  printf(&quot;%d\\n&quot;, x);\n}\n// x 在 func 函数外部无法访问\n在上面的代码中，x 仅在 func() 函数内部有效，退出函数后 x 被销毁。\n全局变量\n作用域和生存期\n\n\n作用域：\n\n\n全局变量的作用域是整个文件。如果在其他文件中需要访问该全局变量，可以使用 extern 关键字声明。\n\n\n全局变量在程序中只存在一份，可以在程序中的任何地方访问。\n\n\n\n\n生存期：\n\n\n全局变量的生存期是整个程序的生命周期，从程序开始到程序结束。它们通常保存在数据段中。\n\n\n示例：\nint x = 10;  // 全局变量 x\n \nvoid func() {\n    printf(&quot;%d\\n&quot;, x);  // 访问全局变量 x\n}\n \nint main() {\n    printf(&quot;%d\\n&quot;, x);  // 访问全局变量 x\n    func();\n    return 0;\n}\n\n\n在上述代码中，x 是全局变量，可以在 main() 和 func() 中访问，并且它的值会在程序执行期间一直存在。\n注意：如果在局部作用域中重新定义一个与全局变量同名的变量（例如，int x），该局部变量会覆盖全局变量的作用域。\nint x = 10;  // 全局变量 x\n \nvoid func() {\n\tint x = 20;  // 局部变量 x，遮蔽了全局变量 x\n\tprintf(&quot;Inside func, x = %d\\n&quot;, x);  // 输出局部变量 x 的值 20\n}\n \nint main() {\n\tprintf(&quot;Before func, x = %d\\n&quot;, x);  // 输出全局变量 x 的值 10\n\tfunc();\n\tprintf(&quot;After func, x = %d\\n&quot;, x);  // 输出全局变量 x 的值 10\n\treturn 0;\n}\n输出结果：\nBefore func, x = 10\nInside func, x = 20\nAfter func, x = 10\n\nextern关键字\n如果使用extern关键字导出全局变量，则在其他文件中也可以访问。\nfile1.c\n#include &lt;stdio.h&gt;\n \nint x = 10;  // 全局变量 x\n \nvoid print_x() {\n    printf(&quot;x = %d\\n&quot;, x);  // 访问全局变量 x\n}\n \nfile2.c\n#include &lt;stdio.h&gt;\n \nextern int x;  // 使用 extern 声明全局变量 x\n \nvoid change_x() {\n    x = 20;  // 修改全局变量 x\n}\n \nint main() {\n    printf(&quot;Before change, x = %d\\n&quot;, x);  // 访问全局变量 x\n    change_x();  // 修改全局变量 x\n    printf(&quot;After change, x = %d\\n&quot;, x);  // 访问修改后的全局变量 x\n    return 0;\n}\n \nfile1.c中定义了全局变量 x,file2.c中使用 extern int x; 声明了 x，表示它在其他文件中已经定义。然后，file2.c 可以访问和修改 x。extern可以在任意文件的任意位置声明，编译器和连接器会自动在所有参与编译的源文件中寻找其定义。\n对于上述两个源文件 file1.c 和 file2.c，可以使用以下命令进行编译和链接：\ngcc file1.c file2.c -o program\n\n运行程序时，输出将会是：\nBefore change, x = 10 \nAfter change, x = 20\n\n静态变量 (static)\n\n\n局部静态变量：\n\n\n作用域：静态局部变量的作用域依然是局部的，只能在定义它的函数或代码块内部访问。\n\n\n生存期：静态局部变量的生存期是整个程序的生命周期。即使函数退出，静态变量的值会被保留，在下一次调用该函数时，静态变量仍然保持之前的值。\n\n\n示例：\nvoid func() {\n    static int counter = 0;  // 静态局部变量 counter\n    counter++;\n    printf(&quot;%d\\n&quot;, counter);\n}\n \nint main() {\n    func();  // 输出 1\n    func();  // 输出 2\n    func();  // 输出 3\n    return 0;\n}\n在上述代码中，counter 是静态局部变量，它的值只有在第一次调用func()时初始化，在后续每次调用 func() 时会跳过初始化，保持之前的值并累加。\n\n\n全局静态变量：\n\n\n作用域：静态全局变量的作用域被限制在当前文件内，其他文件无法访问。它的作用类似于一个局部的全局变量。\n\n\n生存期：静态全局变量的生存期是整个程序的生命周期。\n\n\n示例：\nstatic int x = 10;  // 静态全局变量 x，作用域仅限于当前文件\n \nvoid func() {\n    printf(&quot;%d\\n&quot;, x);  // 访问静态全局变量 x\n}\n在该示例中，x 是静态全局变量，它的作用域仅限于当前文件，其他文件无法访问。\n\n\n总结\n\n\n局部变量：作用域是代码块内，生存期仅在代码块执行期间。\n\n\n全局变量：作用域是整个文件，生存期是程序生命周期。\n\n\n静态变量：\n\n\n静态局部变量：作用域是局部的，生存期是程序生命周期，跨函数调用保持值。\n\n\n静态全局变量：作用域是当前文件，生存期是程序生命周期。\n\n\n\n"},"分布式/hadoop/HDFS/HDFS体系结构":{"slug":"分布式/hadoop/HDFS/HDFS体系结构","filePath":"分布式/hadoop/HDFS/HDFS体系结构.md","title":"HDFS体系结构","links":[],"tags":[],"content":"命名空间管理\n通信协议\nHDFS体系结构的局限性"},"分布式/hadoop/HDFS/HDFS概述":{"slug":"分布式/hadoop/HDFS/HDFS概述","filePath":"分布式/hadoop/HDFS/HDFS概述.md","title":"HDFS概述","links":[],"tags":[],"content":"文件系统基本常识\n在文件系统中，所有的磁盘内容会以文件和目录的形式呈现给用户。不同的文件系统（如 NTFS、ext4、FAT32）有各自独特的命名空间，即文件路径的组织结构。\n物理磁盘结构\n在磁盘的物理层面，磁盘被分为扇区（Sector）。每个扇区的大小通常为512字节或4KB。多个扇区组成簇（Cluster，或称块Block），一个簇是文件系统在存储数据时的基本单位，通常一个簇的大小是4KB、8KB或更大。文件系统将文件数据按簇来存储，簇的大小会影响磁盘空间的利用率。\n文件的数据和元数据\n\n文件数据：是指文件的实际内容，存储在磁盘上用于保存用户信息的部分。\n文件元数据：包含文件的属性信息，如：\n\n文件的时间戳：记录文件的创建时间、修改时间和最后访问时间。\n文件的所有者：表示文件的拥有者，通常与文件的权限系统相关。\n文件的权限：描述谁可以读取、写入或执行文件。\n文件的物理位置：文件在磁盘上的存储位置，指示文件数据存储在哪些簇中。文件系统会根据这些信息管理文件的读写操作。\n\n\n\n此外，文件元数据通常还包括文件名、大小、文件类型等信息。文件系统利用这些元数据来管理文件的存取、删除和权限控制。\n其他补充：\n\n目录结构：文件系统通过目录来组织文件，这些目录形成一个层级结构，类似树状结构，帮助用户和程序更方便地存取文件。\n磁盘管理：文件系统还会管理磁盘的空间使用情况，如空闲空间管理、文件分配表（如FAT表）或inode结构（如在Unix类文件系统中使用）。\n\n分布式文件系统\n节点分为两类：名称节点（namenode）和数据节点（datanode）。\n名称节点负责存储元数据，数据节点负责存储块数据。\nHDFS的特性：\n\n优点\n\n流数据读写\n大型数据读写（tb pb级别）\n一次写入，多次读取\n\n\n缺点\n\n不支持随机写入\n\n\n\n块（Block）\nHDFS默认一个块128mb,HDFS块的大小远远大于普通文件系统，可以最小化寻址开销。\n\n支持大规模文件读写，文件分块存储到不同的节点，所以系统可以存储大于任一节点容量的文件。\n简化系统设计，由于块的大小是固定的，可以很方便地计算出各个节点需要存储多少个块，而且元数据和文件数据想分离，方便管理。\n适合数据冗余备份。\n不适合存储大量小文件，因为每个文件至少占一个块，存储大量小文件会严重影响寻址效率 。\n\n名称节点（NameNode）\n名称节点负责管理分布式文件系统的命名空间（Namespace），保存了两个核心数据结构：Fsimage、EditLog.\n\nFsimage用于维护文件系统树以及文件树中所有文件和文件夹的元数据。\nEditLog操作日志文件记录了所有针对文件的创建、删除、重命名等操作的日志。\n\n命名空间 (Namespace)\nHDFS 的命名空间类似于传统文件系统中的目录结构，它存储文件的路径信息、目录信息和文件的相关属性（如权限、所有者等）。命名空间管理了文件和目录之间的关系，确保文件和目录的路径唯一性。它由 NameNode 管理，NameNode 是 HDFS 中的核心组件，负责所有与文件系统元数据相关的请求。\n\n命名空间 存储着文件系统的结构和文件的所有元数据。它包含了所有的文件和目录信息，如文件路径、权限、所有者、时间戳等。\n\nfsimage\nfsimage 是一个包含整个文件系统命名空间快照的文件。它是 HDFS 元数据的持久化存储，记录了文件系统的状态。每当 HDFS 启动时，NameNode 会加载 fsimage 文件中的内容，恢复文件系统的状态。\n\nfsimage 记录了从文件系统启动以来所有元数据的最终状态，表示文件系统的持久化快照。它是一个完整的文件系统元数据的静态镜像。\n每次 HDFS 启动时，NameNode 会从磁盘加载 fsimage 文件，恢复元数据的最新状态。fsimage 文件的存在保证了即使在系统重启后，也能够恢复到最近的持久化状态。\n\neditlog\neditlog 是一个日志文件，记录了所有对 HDFS 命名空间的修改操作。每次对文件系统进行更改（例如创建文件、删除文件、修改权限等），这些更改都会被写入 editlog 文件。\n\neditlog 记录了每次对命名空间进行修改的操作，且仅记录增量修改。它用于保证在 NameNode 崩溃或重启后能够恢复到修改发生时的状态。\n与 fsimage 不同，editlog 是增量记录，存储了对文件系统的所有修改操作。editlog 会不断增长，会定期通过第二名称节点进行 Checkpointing 合并。\n\n第二名称节点 (Secondary NameNode) 与 Checkpointing\n在 HDFS 中，Secondary NameNode 并不是主 NameNode 的备份，它的主要职责是辅助主 NameNode 进行 Checkpointing，以防止 editlog 文件无限增长，保证文件系统元数据的稳定性和可恢复性。\n主要职责\n\n定期从主 NameNode 下载当前的 fsimage 文件和累积的 editlog 文件。\n将 fsimage 和 editlog 合并，生成新的、包含最新修改的 fsimage 文件。\n将新的 fsimage 文件上传回主 NameNode，并通知主 NameNode 清空已合并的 editlog，从而防止 editlog 无限增长。\n\nCheckpointing 流程\n\n\n获取文件\nSecondary NameNode 定期与主 NameNode 通信，获取当前最新的 fsimage 文件以及自上次 Checkpointing 之后积累的 editlog 文件。\n\n\n合并操作\nSecondary NameNode 在本地将 editlog 文件中的所有操作应用到 fsimage 中，生成一个新的 fsimage 文件。这一步保证了新的 fsimage 包含了最新的文件系统元数据状态。\n\n\n上传更新\n将新的 fsimage 文件上传回主 NameNode，主 NameNode 替换旧的 fsimage，同时清空已经合并到 fsimage 中的 editlog，从而减小 editlog 文件大小，保证系统稳定。\n\n\nSecondary NameNode 的特点\n\n不是备份 NameNode：它不能在主 NameNode 故障时直接替代 NameNode。\n降低恢复成本：通过定期合并 fsimage 和 editlog，减少 NameNode 崩溃时恢复操作所需应用的 editlog 数量。\n提高系统稳定性：避免 editlog 文件过大导致 NameNode 重启和恢复变慢。\n\n总结\nSecondary NameNode 的核心作用是 辅助 Checkpointing：\n\n保证 fsimage 快照与 editlog 的增量修改同步；\n防止 editlog 文件无限增长；\n降低主 NameNode 崩溃后的恢复成本。\n\n通过 Secondary NameNode 的定期 Checkpointing，HDFS 能够保持命名空间元数据的持久性和一致性，同时提高系统的健壮性和可靠性。\n概述总结\n\n命名空间 由 NameNode 管理，存储文件系统的元数据。\nfsimage 是文件系统的快照，包含命名空间的完整状态，作为文件系统元数据的持久化存储。\neditlog 记录文件系统对命名空间的增量修改，确保文件系统的最新状态能够恢复。\nCheckpointing 通过将 fsimage 和 editlog 合并来避免 editlog 文件过大，并定期生成新的 fsimage 文件。\nSecondary NameNode 定期从主 NameNode 获取 fsimage 和 editlog，进行合并操作，并将新的 fsimage 返回给主 NameNode。\n\n这样，fsimage 和 editlog 的相互作用确保了 HDFS 能够高效、持久地管理文件系统元数据，并能够在发生故障时快速恢复。"},"数理统计2/index":{"slug":"数理统计2/index","filePath":"数理统计2/index.md","title":"首页","links":[],"tags":[],"content":"欢迎来到我的 Quartz\n这是我的数字花园首页。你可以从左边的目录或搜索框进入笔记。"},"数理统计2/回归/一元线性回归/1.基本假设":{"slug":"数理统计2/回归/一元线性回归/1.基本假设","filePath":"数理统计2/回归/一元线性回归/1.基本假设.md","title":"1.基本假设","links":[],"tags":[],"content":"解释变量x_,\\cdots,x_p非随机变量，样本\\{(y_i,X_i) \\mid 1\\leq i\\leq n\\}观测值x_{i1},\\cdots ,x_{ip}为常数\n误差项随机变量\\epsilon 满足\nE[\\epsilon_i]=0,i=1\\cdots ,n \n\\begin{cases}\n\\text{Var}(\\epsilon_i) = \\sigma^2, &amp; i = 1, \\cdots, n \\\\\n\\text{Cov}(\\epsilon_i, \\epsilon_j) = 0, &amp; i \\neq j\n\\end{cases}\n即各样本点上的误差项均值为零、方差相同且相互独立。"},"数理统计2/回归/一元线性回归/2.最小二乘估计OLSE":{"slug":"数理统计2/回归/一元线性回归/2.最小二乘估计OLSE","filePath":"数理统计2/回归/一元线性回归/2.最小二乘估计OLSE.md","title":"2.最小二乘估计OLSE","links":[],"tags":[],"content":"求解一元线性回归模型，就是要给出真实回归模型y=\\beta_1x+\\beta_0中\\beta_0,\\beta_1的估计\\hat{\\beta_0},\\hat{\\beta_1}.但由于残差项\\epsilon的分布类型是未知的，无法推理出y的分布类型，进而估计，于是采用最小二乘法估计\\beta_0,\\beta_1.\n最小二乘估计求出的回归方程，成为一元线性经验回归方程。\n\\hat{y}=\\hat{\\beta_1}x+\\hat{\\beta_0}\n算法\n目的为最小化总拟合误差\\left|y_i-\\hat{\\beta_1}x-\\hat{\\beta_0}\\right|.\n构造目标函数\nQ(\\gamma_0,\\gamma_1)=\\Sigma_{i=1}^n(y_i-\\gamma_0-\\gamma_1x)^2\nQ是凸函数，\n\\left\\{\n\\begin{aligned}\n\\frac{\\partial{Q}}{\\partial{\\gamma_0}} &amp;= 0 \\\\\n\\frac{\\partial{Q}}{\\partial{\\gamma_1}} &amp;= 0 \n\\end{aligned}\n\\right.\n满足上述方程组的\\gamma_0,\\gamma_1即为最小二乘估计\\hat{\\beta_0},\\hat{\\beta_1}\n\\left\\{\n\\begin{aligned}\n\\hat{\\beta_1} &amp;= \\frac{L_{xy}}{L_{xx}} \\\\\n\\hat{\\beta_0} &amp;= \\bar{y}-\\hat{\\beta_1}x\n\\end{aligned}\n\\right.\n其中\n\\left\\{\n\\begin{aligned}\nL_{xx}&amp;=\\Sigma(x_i-\\bar{x})^2 \\\\\nL_{xy}&amp;=\\Sigma(x_i-\\bar{x})(y_i-\\bar{y})\n\\end{aligned}\n\\right."},"数理统计2/回归/一元线性回归/3.回归系数的性质":{"slug":"数理统计2/回归/一元线性回归/3.回归系数的性质","filePath":"数理统计2/回归/一元线性回归/3.回归系数的性质.md","title":"3.回归系数的性质","links":[],"tags":[],"content":"线性性\n\\hat{\\beta_1},\\hat{\\beta_0}是 随机变量y_i的线性函数\n无偏性\n\\begin{aligned}\nE\\hat{\\beta_1} &amp;=E[\\frac{\\Sigma_{i=1}^n (x_i-\\bar{x})y_i}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}] \\\\\n\t\t\t   \n\t\t\t   &amp;=\\frac{\\Sigma_{i=1}^n (x_i-\\bar{x})E[y_i]}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2} \\\\\n\t\t\t   \n\t\t\t   &amp;=\\frac{\\Sigma_{i=1}^n (x_i-\\bar{x})(\\beta_1x_i+\\beta_0)}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2} \\\\\n\t\t\t   \n\t\t\t   &amp;=\\frac{\\Sigma_{i=1}^n (x_i-\\bar{x})x_i}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}\\beta_1 \\\\\n\t\t\t   \n\\end{aligned}\n其中\\frac{\\Sigma_{i=1}^n (x_i-\\bar{x})x_i}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}=1 \n故E\\hat{\\beta_1}=\\beta_1,同理可证E\\hat{\\beta_0}=\\beta_0\n方差\n由于y_1,\\cdots,y_n独立，var(y_i)=\\sigma^2\n故$$\n\\begin{aligned}\nVar(\\hat{\\beta_1}) &amp;= \\Sigma_{i=1}^{n} [\\frac{x_i - \\bar{x}}{\\Sigma_{i=1}^n (x_i-\\bar{x})^2}]^2 Var(y_i)\\\n&amp;= \\frac{\\sigma^2}{L_{xx}}\n\\end{aligned}\n\nVar(\\hat{\\beta_0})=[\\frac{1}{n}+\\frac{\\bar{x}^2}{L_{xx}}]\\sigma\n"},"数理统计2/回归/一元线性回归/4.误差随机变量方差的估计":{"slug":"数理统计2/回归/一元线性回归/4.误差随机变量方差的估计","filePath":"数理统计2/回归/一元线性回归/4.误差随机变量方差的估计.md","title":"4.误差随机变量方差的估计","links":[],"tags":[],"content":"\\hat{\\sigma}^2 = \\frac{1}{n-2} \\sum (y_i - \\hat{y}_i)^2\n其中\n\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i"},"数理统计2/回归/一元线性回归/5.显著性检验":{"slug":"数理统计2/回归/一元线性回归/5.显著性检验","filePath":"数理统计2/回归/一元线性回归/5.显著性检验.md","title":"5.显著性检验","links":[],"tags":[],"content":"H_0: \\beta_1 = 0 \\quad\\text{vs.}\\quad H_1: \\beta_1 \\neq 0\nt检验\n在假定误差项满足独立同分布，且 \\varepsilon_i \\sim N(0, \\sigma^2) 的条件下：\n回归系数估计的分布\n假设\\epsilon_i \\sim N(0,\\sigma^2） 则\\hat{\\beta}_1 \\sim N\\left(\\beta_1, \\dfrac{\\sigma^2}{L_{xx}}\\right)  ，其中  L_{xx} = \\sum (x_i - \\bar{x})^2\n在 H_0 下，有\nt = \\frac{\\hat{\\beta}_1 - 0}{\\sqrt{\\hat{\\sigma}^2 / L_{xx}}} \\sim t_{n-2}\nF检验：直接从回归效果检验显著性\n平方和分解式\n\\sum_{i=1}^{n} (y_i - \\bar{y})^2 = \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2 + \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n\n称 \\sum_{i=1}^{n} (y_i - \\bar{y})^2 为总离差平方和 (SST)，描述观测值 y 本身的方差\n称 \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2 为回归平方和 (SSR)\n称 \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 为残差平方和 (SSE)\n\nSST = SSR + SSE\n证明:\n即证\n\\sum_{i=1}^{n} (y_i - \\hat{y}_i + \\hat{y}_i - \\bar{y})^2 = \\sum_{i=1}^{n} (y_i - \\bar{y})^2 + \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n即证\n\\sum_{i=1}^{n} (y_i - \\hat{y}_i)(\\hat{y}_i - \\bar{y}) = 0\n记残差 e_i = y_i - \\hat{y}_i\n\\frac{\\partial Q}{\\partial \\beta_1} = 2 \\sum x_i (y_i - \\hat{\\beta_1}x_i-\\hat{\\beta_0}) = 0, \\quad \\frac{\\partial Q}{\\partial \\beta_0} = 2 \\sum (y_i - \\beta_0 - \\beta_1 x_i) = 0\n\\Rightarrow e_i = y_i - \\hat{y}_i = y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i \n\n\\Rightarrow \\sum x_i e_i = \\sum e_i = 0.\n\\begin{aligned}\n&amp;\\sum_{i=1}^{n} (y_i - \\hat{y}_i)(\\hat{y}_i - \\bar{y}) \\\\ \n=&amp;\\sum e_i (\\hat{y}_i - \\bar{y}) =\\sum e_i\\hat{y_i}+\\bar{y}\\sum e_i \\\\\n=&amp;\\sum e_i (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) \\\\\n=&amp; \\hat{\\beta}_0 \\sum e_i + \\hat{\\beta}_1 \\sum (e_i x_i) \\\\\n=&amp; 0\n\\end{aligned}\n证毕\n构造F分布进行检验\nSSR 越大，SSE 越小说明回归越好\nH_0: \\beta_1 = 0 \\leftrightarrow H_1: \\beta_1 \\neq 0\n在 H_0 下，F = \\frac{SSR/1}{SSE/(n-2)}，\\frac{SSE}{n-2} \\sim \\chi^2(n-2)，SSR \\sim \\chi^2(1)\n\\Rightarrow F \\xrightarrow{H_0} F(1, n-2)\n皮尔逊相关系数检验\nr = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}} = \\frac{l_{xy}}{\\sqrt{l_{xx} l_{yy}}}\nr越接近 1 拟合越好"},"数理统计2/回归/一元线性回归/6.残差分析":{"slug":"数理统计2/回归/一元线性回归/6.残差分析","filePath":"数理统计2/回归/一元线性回归/6.残差分析.md","title":"6.残差分析","links":[],"tags":[],"content":"残差分析：判断真实模型是否为线性模型\n若为线性模型则必满足 Ee_i = Ex_i \\cdot e_i = 0\n残差应在 0 附近随机波动\n残差 e_i 的方差\n\\begin{aligned}\nVar(e_i) &amp;= Var(y_i - \\hat{y}_i) \\\\\n&amp;= Var(y_i) + Var(\\hat{y}_i) - 2Cov(y_i, \\hat{y}_i)\n\\end{aligned}\n(1) Var(y_i) = \\sigma^2\n(2)\n\\begin{aligned}\nVar(\\hat{y}_i) &amp;= Var(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) \\\\\n&amp;= Var(\\hat{\\beta}_0) + Var(\\hat{\\beta}_1 x_i) + 2Cov(\\hat{\\beta}_0, \\hat{\\beta}_1 x_i) \\\\\n\\end{aligned}\n其中\n\\begin{aligned}\nVar(\\hat{\\beta}_0) &amp;= \\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{L_{xx}}\\right)\\sigma^2 \\\\\n\\\\\nVar(\\hat{\\beta}_1 x_i) &amp;= x_i^2 \\frac{\\sigma^2}{L_{xx}}\\\\\n\\\\\nCov(\\hat{\\beta}_0, \\hat{\\beta}_1) &amp;= Cov(\\bar{y} - \\hat{\\beta}_1 \\bar{x}, \\hat{\\beta}_1) \\\\\n&amp;= Cov(-\\hat{\\beta}_1 \\bar{x}, \\hat{\\beta}_1) + Cov(\\bar{y}, \\hat{\\beta}_1) \\\\\n&amp;= -\\bar{x} Var(\\hat{\\beta}_1) + \\frac{Var(y_i)}{n \\sum (x_i - \\bar{x})^2} \\cdot \\sum (x_i - \\bar{x}) \\\\\n&amp;= -\\bar{x} \\frac{\\sigma^2}{L_{xx}} + 0 \\\\\n\\end{aligned}\n故Cov(\\hat{\\beta}_0, \\hat{\\beta}_1 x_i) = -\\bar{x} x_i \\frac{\\sigma^2}{L_{xx}}\n故\n\\begin{aligned}\nVar(\\hat{y}_i) &amp;= \\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{L_{xx}}\\right)\\sigma^2 + x_i^2 \\frac{\\sigma^2}{L_{xx}} - 2\\bar{x} x_i \\frac{\\sigma^2}{L_{xx}} \\\\\n&amp;= \\left(\\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{L_{xx}}\\right)\\sigma^2\n\\end{aligned}\n(3)\n\\begin{aligned}\nCov(y_i, \\hat{y}_i) &amp;= Cov(y_i, \\hat{\\beta}_1 x_i + \\bar{y} - \\hat{\\beta}_1 \\bar{x})\\\\\n&amp;= Cov(y_i, \\hat{\\beta}_1 x_i) + (x_i - \\bar{x}) Cov(y_i, \\hat{\\beta}_1) \\\\\n&amp;= \\frac{1}{n} Var(y_i) + (x_i - \\bar{x}) Cov(y_i, \\frac{\\sum (x_i - \\bar{x}) y_i}{L_{xx}}) \\\\\n&amp;= \\frac{1}{n} Var(y_i) + \\frac{(x_i - \\bar{x})^2}{L_{xx}} Var(y_i) \\\\\n&amp;= \\left(\\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{L_{xx}}\\right)\\sigma^2\n\\end{aligned}\n综上：Var(e_i) = \\sigma^2 + \\left(\\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{L_{xx}}\\right)\\sigma^2 - 2\\left(\\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{L_{xx}}\\right)\\sigma^2 = (1 - \\frac{1}{n} - \\frac{(x_i - \\bar{x})^2}{L_{xx}})\\sigma^2 = (1 - \\hat{h}_i)\\sigma^2"},"数理统计2/回归/回归模型":{"slug":"数理统计2/回归/回归模型","filePath":"数理统计2/回归/回归模型.md","title":"回归模型","links":[],"tags":[],"content":"变量x_1,\\cdots,x_p与随机变量y存在相关关系，即确定x_1,\\cdots,x_p的取值后可以确定y的分布，y与x_1,\\cdots,x_p之间的概率模型为y=f(x_i,\\cdots,x_p)+\\epsilon当f为线性函数时，称模型为线性回归模型。"},"数理统计2/概率论与数理统计基础/常用分布":{"slug":"数理统计2/概率论与数理统计基础/常用分布","filePath":"数理统计2/概率论与数理统计基础/常用分布.md","title":"常用分布","links":[],"tags":[],"content":"t分布\n若\\epsilon \\sim N(0,1),\\eta服从自由度为n的\\chi^2分布\\eta \\sim \\chi^2(n),则称随机变量T=\\frac{\\epsilon}{\\sqrt{\\frac{\\eta}{n}}}服从自由度为n的t分布，T \\sim t(n).\nF 分布\n设\\epsilon,\\eta是自由度为m,n的独立的\\chi^2随机变量，则称随机变量F=\\frac{\\epsilon/m}{\\eta/n}所服从的分布为F分布，自由度为(m,n),记作F \\sim F(m,n)."},"数理统计2/概率论与数理统计基础/方差、协方差的性质":{"slug":"数理统计2/概率论与数理统计基础/方差、协方差的性质","filePath":"数理统计2/概率论与数理统计基础/方差、协方差的性质.md","title":"方差、协方差的性质","links":[],"tags":[],"content":"引理\nX,Y独立则对任何函数h,g成立\n E[g(X)g(Y)]=E[g(X)] \\cdot E[h(Y)]\n方差与协方差常用公式\n\n协方差的定义Cov(X,Y)=E[(X-EX)(Y-EY)]=E[XY]-EX \\cdot EY\n交换性：Cov(X,Y)=Cov(Y,X)\n线性性:Coc(aX,Y)=aCov(X,Y) Cov(\\Sigma_{i=1}^nX_i,\\Sigma_{j=1}^mY_j)=\\Sigma_{i=1}^n\\Sigma_{j=1}^m Cov(X_i,Y_j)\n方差展开式：Var(\\Sigma_{i=1}^nX_i)=\\Sigma_{i=1}^nVar(X_i)+2\\Sigma_{i&lt;j}Cov(X_i,X_j)\n"}}